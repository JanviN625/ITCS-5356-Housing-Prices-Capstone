{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d1388c5",
   "metadata": {
    "id": "4d1388c5"
   },
   "source": [
    "\n",
    "# Multi-Layer Neural Networks for Classification\n",
    "\n",
    "Course: Introduction to Machine Learning\n",
    "\n",
    "Instructor: Xiang Zhang\n",
    "\n",
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    " \\newcommand{\\wv}{\\mathbf{w}}\n",
    " \\newcommand{\\yv}{\\mathbf{y}}\n",
    " \\newcommand{\\zv}{\\mathbf{z}}\n",
    " \\newcommand{\\uv}{\\mathbf{u}}\n",
    " \\newcommand{\\vv}{\\mathbf{v}}\n",
    " \\newcommand{\\tv}{\\mathbf{t}}\n",
    " \\newcommand{\\bv}{\\mathbf{b}}\n",
    " \\newcommand{\\av}{\\mathbf{a}}\n",
    " \\newcommand{\\Chi}{\\mathcal{X}}\n",
    " \\newcommand{\\R}{\\rm I\\!R}\n",
    " \\newcommand{\\sign}{\\text{sign}}\n",
    " \\newcommand{\\Ym}{\\mathbf{Y}}\n",
    " \\newcommand{\\Xm}{\\mathbf{X}}\n",
    " \\newcommand{\\Wm}{\\mathbf{W}}\n",
    " \\newcommand{\\Zm}{\\mathbf{Z}}\n",
    " \\newcommand{\\Im}{\\mathbf{I}}\n",
    " \\newcommand{\\Um}{\\mathbf{U}}\n",
    " \\newcommand{\\Vm}{\\mathbf{V}}\n",
    " \\newcommand{\\Am}{\\mathbf{A}}\n",
    " \\newcommand{\\muv}{\\boldsymbol\\mu}\n",
    " \\newcommand{\\Sigmav}{\\boldsymbol\\Sigma}\n",
    " \\newcommand{\\Lambdav}{\\boldsymbol\\Lambda}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce7a459",
   "metadata": {
    "id": "2ce7a459"
   },
   "source": [
    "<br/>\n",
    "<font color=\"blue\"><b>\n",
    "\n",
    "NAME: *Janvi Nandwani*\n",
    "\n",
    "</b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10675a23",
   "metadata": {
    "id": "10675a23"
   },
   "source": [
    "## Goal\n",
    "The goal of this lab will be to practice implementing the feed-forward and feedback processes for multi-layer neural networks. Further, we'll look at how we can reformulate neural networks to work for classification problems. Finally, we'll also take a look at more systematic hyper-parameter tuning using Sklearn's `GridSearchCV` class. To do so, once again, we'll work with Sign Language MNIST dataset which is a data set for classifying images of The American Sign Language of hand gestures..\n",
    "\n",
    "Your job is to read through the lab and fill in any code segments that are marked by `TODO` headers and comments. **It should be noted, that all the correct outputs are given below each code cell. It might be useful to duplicate all the `TODO` cells so you can try to match the correct output with your own code!**\n",
    "\n",
    "Use the `todo_check()`to help guide you in understanding whether your code for a given TODO is correct or incorrect. However, failing a TODO check doesn't mean you won't receive points, though it could be a good indication. If you are failing feel free to ask and we can help check what is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b61696e",
   "metadata": {
    "id": "4b61696e"
   },
   "source": [
    "## Agenda\n",
    "- Review and load the Sign Language MNIST dataset via Sklearn\n",
    "- Visualize and explore the Sign Language MNIST dataset\n",
    "- Create the data preparation pipeline where we apply data preprocessing AFTER splitting\n",
    "- Implement a simple multi-layer neural networks for multi-class classification problems\n",
    "    - Implement ReLU, and softmax activation functions.\n",
    "    - Implement the multi-layer neural network feed-forward process for making predictions\n",
    "    - Implement the  multi-layer neural network feedback process for updating the weights and biases\n",
    "- Investigate hyper-parameters tuning by using cross validation with grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5af1e5",
   "metadata": {
    "id": "eb5af1e5"
   },
   "source": [
    "## Table of notation\n",
    "\n",
    "| Symbol                     | Meaning                     | Symbol    | Meaning                                                          |\n",
    "|----------------------------|-----------------------------|-----------|------------------------------------------------------------------|\n",
    "| $\\xv$ or $\\vec{x}$         | feature/input vector        | $x_i$     | $i$th element of $\\xv$                                           |\n",
    "| $\\Xm$                      | input matrix                | $x_{i,j}$ | $i$th row and $j$th column of $\\Xm$                              |\n",
    "| $\\yv$ or $\\tv$             | labels/targets              | $n$       | number of features or columns\n",
    "| $\\wv$ or $\\mathbf{\\theta}$ | weight/parameter vector     | $m$       | number of data samples <br>(also used to refer to the slope) |samples or rows                                   |\n",
    "| $f$ or $h$                 | hypothesis function <br> (i.e., a model)        | $\\hat{\\yv}$ <br> $f(\\xv {;} \\wv)$<br>$h(\\xv {;} \\wv)$ | predictions <br> y-hat |\n",
    "| $E$              | error or sum of error (loss)  | $SSE$      | sum of squared error function                                            |\n",
    "| $MSE$                      | mean squared error| $\\nabla$  | gradient (nabla)                                       |\n",
    "| $\\partial$                 | partial derivative          | $\\alpha$  | learning rate (alpha)                                  |       \n",
    "| $J$ | general placeholder for <br>the objective function | $x^T$| transpose of a vector or matrix |\n",
    "$b$ | bias or y-intercept term | $T$ | Threshold |\n",
    "$*$| element-wise<br> multiplication | $\\cdot$ | dot product|\n",
    "| $z$<br>$\\zv$| value before applying activation function |  $X, Y$ | Random variables |\n",
    "| $K$| number/set of classes | $k$ | current class|\n",
    "| $MLE$|  maximum likelihood estimation | $ML$ |  maximum likelihood|\n",
    "| $MLL$|  maximum log likelihood | $LL$ | log likelihood |\n",
    "| $L$|  likelihood | $NLL$ | negative log likelihood |\n",
    "| $g$<br>$g'$ | activation function<br>activation function derivative | $a$/$h$ <br> $\\av/\\mathbf{h}$<br>$\\Am$/$\\mathbf{H}$ | output of activation function <br> or neuron\n",
    "$w$<br>$\\wv$<br>$\\Wm$ | weights| $z$<br>$\\zv$<br>$\\Zm$ | linear combination output|\n",
    "|$\\Wm^{[l]}$| $l$th layer weights| $\\Am^{[l]}$| $l$th layer activations\n",
    "|$\\Zm^{[l]}$| $l$th layer linear combinations| $\\bv^{[l]}$| $l$th layer bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24839c00",
   "metadata": {
    "id": "24839c00"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff6be447",
   "metadata": {
    "id": "ff6be447"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import gc\n",
    "from typing import Tuple, Union, List, Dict\n",
    "\n",
    "import sklearn\n",
    "sklearn_version = '1.0'\n",
    "# Check to make sure you have the right version of sklearn\n",
    "assert sklearn.__version__  > sklearn_version, f'sklearn version is only {sklearn.__version__} and needs to be > {sklearn_version}'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c6b4b5",
   "metadata": {
    "id": "f9c6b4b5"
   },
   "outputs": [],
   "source": [
    "# Set this to True if you DO NOT want to run the\n",
    "# garbage_collect() functions throughout the notebook\n",
    "turn_off_garbage_collect = False\n",
    "\n",
    "def garbage_collect(vars_):\n",
    "    if not turn_off_garbage_collect:\n",
    "        for v in vars_:\n",
    "            if v in globals():\n",
    "                del globals()[v]\n",
    "        collected = gc.collect()\n",
    "\n",
    "def todo_check(condi_err):\n",
    "    failed_err = \"You passed {}/{} and FAILED the following code checks:{}\"\n",
    "    failed = \"\"\n",
    "    n_failed = 0\n",
    "    for check, (condi, err) in enumerate(condi_err):\n",
    "        if not condi:\n",
    "            n_failed += 1\n",
    "            failed += f\"\\nFailed check [{check+1}]:\\n\\t Tip: {err}\"\n",
    "\n",
    "    if len(failed) != 0:\n",
    "        passed = len(condi_err) - n_failed\n",
    "        err = failed_err.format(passed, len(condi_err), failed)\n",
    "        raise AssertionError(err.format(failed))\n",
    "    print(\"Your code PASSED the code check!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58835ff",
   "metadata": {
    "id": "c58835ff"
   },
   "source": [
    "# Problem statement summary\n",
    "\n",
    "## Sign Language MNIST dataset problem summary\n",
    "\n",
    "<img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F357637%2F6237b2fd781887e7c9e09ef6b7dc610b%2Famer_sign2.png?generation=1608488019646176&alt=media\">\n",
    "\n",
    "As mentioned, the [Sign Language MNIST](https://www.kaggle.com/datamunge/sign-language-mnist) dataset is a dataset of 24 sign language hand gesture images where the goal is to classify each image as the correct letter. Read the below summary from Kaggle for a more in-depth description of the data.\n",
    "\n",
    ">The original MNIST image dataset of handwritten digits is a popular benchmark for image-based machine learning methods but researchers have renewed efforts to update it and develop drop-in replacements that are more challenging for computer vision and original for real-world applications. As noted in one recent replacement called the Fashion-MNIST dataset, the Zalando researchers quoted the startling claim that \"Most pairs of MNIST digits (784 total pixels per sample) can be distinguished pretty well by just one pixel\". To stimulate the community to develop more drop-in replacements, the Sign Language MNIST is presented here and follows the same CSV format with labels and pixel values in single rows. The American Sign Language letter database of hand gestures represent a multi-class problem with 24 classes of letters (excluding J and Z which require motion).\n",
    ">\n",
    ">The dataset format is patterned to match closely with the classic MNIST. Each training and test case represents a label (0-25) as a one-to-one map for each alphabetic letter A-Z (and no cases for 9=J or 25=Z because of gesture motions). The training data (27,455 cases) and test data (7172 cases) are approximately half the size of the standard MNIST but otherwise similar with a header row of label, pixel1,pixel2….pixel784 which represent a single 28x28 pixel image with grayscale values between 0-255. The original hand gesture image data represented multiple users repeating the gesture against different backgrounds.\n",
    ">\n",
    ">The Sign Language MNIST data came from greatly extending the small number (1704) of the color images included as not cropped around the hand region of interest. To create new data, an image pipeline was used based on ImageMagick and included cropping to hands-only, gray-scaling, resizing, and then creating at least 50+ variations to enlarge the quantity. The modification and expansion strategy was filters ('Mitchell', 'Robidoux', 'Catrom', 'Spline', 'Hermite'), along with 5% random pixelation, +/- 15% brightness/contrast, and finally 3 degrees rotation. Because of the tiny size of the images, these modifications effectively alter the resolution and class separation in interesting, controllable ways.\n",
    "\n",
    "Take note of the following important aspects about the data:\n",
    "\n",
    "1. The data is already split such that there are 27k training samples and 7K testing examples.\n",
    "1. The data features correspond to 784 pixels which means the images are 28x28. Often, it is best practice to flatten the 2D images into a single 1D array such that the width multiplied by the height determines the number of features (i.e., pixels).\n",
    "1. Pixel values are gray scale and range from 0-255.\n",
    "1. There are 24 classes, one for each letter in the alphabet excluding (J and Z).\n",
    "1. The dataset largely consists of augmented images which means different augmentations were applied to the images to create new images. This is a common practice when working with image data as it can greatly increase the number of data samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e7315",
   "metadata": {
    "id": "aa0e7315"
   },
   "source": [
    "# Data downloading and loading\n",
    "\n",
    "The first step will be downloading the data. You have two options to download the data:\n",
    "\n",
    "1. Download the data from the Canvas submission page for this lab\n",
    "2. Download the data from [Kaggle](https://www.kaggle.com/datamunge/sign-language-mnist).\n",
    "    1. To download click on the \"Download\" button right under the Sign Language MNIST banner.\n",
    "    2. In order to download the data, you'll need to sign into a Kaggle account.\n",
    "    3. Once downloaded, you'll want to extract the `sign_mnist_train.csv` and `sign_mnist_test.csv`files from the zip and put them in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cd8417",
   "metadata": {
    "id": "81cd8417"
   },
   "source": [
    "#### TODO 1\n",
    "Complete the TODO by loading the training data from the `sign_mnist_train.csv` file and the testing data from the `sign_mnist_test.csv` file.\n",
    "\n",
    "1. Load training data for the Sign Language MNIST dataset using the Pandas and the `sign_mnist_train.csv` file. Store the output into the `asl_trn_df` variable.\n",
    "\n",
    "\n",
    "2. Load testing data for the Sign Language MNIST dataset using the Pandas and the `sign_mnist_test.csv` file. Store the output into the `asl_tst_df` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29569652",
   "metadata": {
    "id": "29569652",
    "outputId": "777f698b-bf48-4c9d-8ae5-02987f629a05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"sign_mnist_train.csv\") and not os.path.exists(\"sign_mnist_test.csv\") :\n",
    "    warnings.warn(f\"One or both of the sign_mnist_*.csv was not detected in your local path \" \\\n",
    "                 f\"{os.getcwd()}\")\n",
    "# TODO 1.1\n",
    "asl_trn_df = pd.read_csv(\"sign_mnist_train.csv\")\n",
    "# TODO 1.2\n",
    "asl_tst_df = pd.read_csv(\"sign_mnist_test.csv\")\n",
    "\n",
    "todo_check([\n",
    "    (type(asl_trn_df) is pd.DataFrame, \"asl_trn_df is not a DataFrame\"),\n",
    "    (asl_trn_df.shape == (27455, 785), 'asl_trn_df does not have the shape (27455, 785)'),\n",
    "    (type(asl_tst_df) is pd.DataFrame, \"asl_tst_df is not a DataFrame\"),\n",
    "    (asl_tst_df.shape == (7172, 785), 'asl_tst_df does not have the shape (7172, 785)'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd68e12",
   "metadata": {
    "id": "cdd68e12"
   },
   "source": [
    "Below is the DataFrame for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "448b7790",
   "metadata": {
    "id": "448b7790",
    "outputId": "a114f24c-4809-475c-c5d4-6fca2e9fd5c2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>150</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>149</td>\n",
       "      <td>128</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>103</td>\n",
       "      <td>135</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>...</td>\n",
       "      <td>202</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>231</td>\n",
       "      <td>230</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>222</td>\n",
       "      <td>229</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>164</td>\n",
       "      <td>167</td>\n",
       "      <td>170</td>\n",
       "      <td>172</td>\n",
       "      <td>176</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>133</td>\n",
       "      <td>163</td>\n",
       "      <td>157</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27450</th>\n",
       "      <td>13</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>192</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>...</td>\n",
       "      <td>132</td>\n",
       "      <td>165</td>\n",
       "      <td>99</td>\n",
       "      <td>77</td>\n",
       "      <td>52</td>\n",
       "      <td>200</td>\n",
       "      <td>234</td>\n",
       "      <td>200</td>\n",
       "      <td>222</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27451</th>\n",
       "      <td>23</td>\n",
       "      <td>151</td>\n",
       "      <td>154</td>\n",
       "      <td>157</td>\n",
       "      <td>158</td>\n",
       "      <td>160</td>\n",
       "      <td>161</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>166</td>\n",
       "      <td>...</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>196</td>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27452</th>\n",
       "      <td>18</td>\n",
       "      <td>174</td>\n",
       "      <td>174</td>\n",
       "      <td>174</td>\n",
       "      <td>174</td>\n",
       "      <td>174</td>\n",
       "      <td>175</td>\n",
       "      <td>175</td>\n",
       "      <td>174</td>\n",
       "      <td>173</td>\n",
       "      <td>...</td>\n",
       "      <td>121</td>\n",
       "      <td>196</td>\n",
       "      <td>209</td>\n",
       "      <td>208</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27453</th>\n",
       "      <td>17</td>\n",
       "      <td>177</td>\n",
       "      <td>181</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>187</td>\n",
       "      <td>189</td>\n",
       "      <td>190</td>\n",
       "      <td>191</td>\n",
       "      <td>191</td>\n",
       "      <td>...</td>\n",
       "      <td>119</td>\n",
       "      <td>56</td>\n",
       "      <td>27</td>\n",
       "      <td>58</td>\n",
       "      <td>102</td>\n",
       "      <td>79</td>\n",
       "      <td>47</td>\n",
       "      <td>64</td>\n",
       "      <td>87</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27454</th>\n",
       "      <td>23</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>182</td>\n",
       "      <td>181</td>\n",
       "      <td>182</td>\n",
       "      <td>183</td>\n",
       "      <td>182</td>\n",
       "      <td>...</td>\n",
       "      <td>108</td>\n",
       "      <td>132</td>\n",
       "      <td>170</td>\n",
       "      <td>194</td>\n",
       "      <td>214</td>\n",
       "      <td>203</td>\n",
       "      <td>197</td>\n",
       "      <td>205</td>\n",
       "      <td>209</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27455 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0          3     107     118     127     134     139     143     146     150   \n",
       "1          6     155     157     156     156     156     157     156     158   \n",
       "2          2     187     188     188     187     187     186     187     188   \n",
       "3          2     211     211     212     212     211     210     211     210   \n",
       "4         13     164     167     170     172     176     179     180     184   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "27450     13     189     189     190     190     192     193     193     193   \n",
       "27451     23     151     154     157     158     160     161     163     164   \n",
       "27452     18     174     174     174     174     174     175     175     174   \n",
       "27453     17     177     181     184     185     187     189     190     191   \n",
       "27454     23     179     180     180     180     182     181     182     183   \n",
       "\n",
       "       pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0         153  ...       207       207       207       207       206   \n",
       "1         158  ...        69       149       128        87        94   \n",
       "2         187  ...       202       201       200       199       198   \n",
       "3         210  ...       235       234       233       231       230   \n",
       "4         185  ...        92       105       105       108       133   \n",
       "...       ...  ...       ...       ...       ...       ...       ...   \n",
       "27450     193  ...       132       165        99        77        52   \n",
       "27451     166  ...       198       198       198       198       198   \n",
       "27452     173  ...       121       196       209       208       206   \n",
       "27453     191  ...       119        56        27        58       102   \n",
       "27454     182  ...       108       132       170       194       214   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0           206       206       204       203       202  \n",
       "1           163       175       103       135       149  \n",
       "2           199       198       195       194       195  \n",
       "3           226       225       222       229       163  \n",
       "4           163       157       163       164       179  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "27450       200       234       200       222       225  \n",
       "27451       196       195       195       195       194  \n",
       "27452       204       203       202       200       200  \n",
       "27453        79        47        64        87        93  \n",
       "27454       203       197       205       209       215  \n",
       "\n",
       "[27455 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asl_trn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4db15e",
   "metadata": {
    "id": "8e4db15e"
   },
   "source": [
    "Below is the DataFrame for the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "521203eb",
   "metadata": {
    "id": "521203eb",
    "outputId": "0bf37127-421e-42db-ef3c-4e3386fe22ad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>150</td>\n",
       "      <td>151</td>\n",
       "      <td>...</td>\n",
       "      <td>138</td>\n",
       "      <td>148</td>\n",
       "      <td>127</td>\n",
       "      <td>89</td>\n",
       "      <td>82</td>\n",
       "      <td>96</td>\n",
       "      <td>106</td>\n",
       "      <td>112</td>\n",
       "      <td>120</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>126</td>\n",
       "      <td>128</td>\n",
       "      <td>131</td>\n",
       "      <td>132</td>\n",
       "      <td>133</td>\n",
       "      <td>134</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>136</td>\n",
       "      <td>...</td>\n",
       "      <td>47</td>\n",
       "      <td>104</td>\n",
       "      <td>194</td>\n",
       "      <td>183</td>\n",
       "      <td>186</td>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "      <td>182</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>85</td>\n",
       "      <td>88</td>\n",
       "      <td>92</td>\n",
       "      <td>96</td>\n",
       "      <td>105</td>\n",
       "      <td>123</td>\n",
       "      <td>135</td>\n",
       "      <td>143</td>\n",
       "      <td>147</td>\n",
       "      <td>...</td>\n",
       "      <td>68</td>\n",
       "      <td>166</td>\n",
       "      <td>242</td>\n",
       "      <td>227</td>\n",
       "      <td>230</td>\n",
       "      <td>227</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>224</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "      <td>205</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>207</td>\n",
       "      <td>209</td>\n",
       "      <td>210</td>\n",
       "      <td>209</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>154</td>\n",
       "      <td>248</td>\n",
       "      <td>247</td>\n",
       "      <td>248</td>\n",
       "      <td>253</td>\n",
       "      <td>236</td>\n",
       "      <td>230</td>\n",
       "      <td>240</td>\n",
       "      <td>253</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>188</td>\n",
       "      <td>191</td>\n",
       "      <td>193</td>\n",
       "      <td>195</td>\n",
       "      <td>199</td>\n",
       "      <td>201</td>\n",
       "      <td>202</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>40</td>\n",
       "      <td>64</td>\n",
       "      <td>48</td>\n",
       "      <td>29</td>\n",
       "      <td>46</td>\n",
       "      <td>49</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7167</th>\n",
       "      <td>1</td>\n",
       "      <td>135</td>\n",
       "      <td>119</td>\n",
       "      <td>108</td>\n",
       "      <td>102</td>\n",
       "      <td>105</td>\n",
       "      <td>99</td>\n",
       "      <td>61</td>\n",
       "      <td>103</td>\n",
       "      <td>121</td>\n",
       "      <td>...</td>\n",
       "      <td>108</td>\n",
       "      <td>112</td>\n",
       "      <td>116</td>\n",
       "      <td>114</td>\n",
       "      <td>118</td>\n",
       "      <td>180</td>\n",
       "      <td>184</td>\n",
       "      <td>176</td>\n",
       "      <td>167</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7168</th>\n",
       "      <td>12</td>\n",
       "      <td>157</td>\n",
       "      <td>159</td>\n",
       "      <td>161</td>\n",
       "      <td>164</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>171</td>\n",
       "      <td>174</td>\n",
       "      <td>175</td>\n",
       "      <td>...</td>\n",
       "      <td>213</td>\n",
       "      <td>213</td>\n",
       "      <td>213</td>\n",
       "      <td>214</td>\n",
       "      <td>213</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>209</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7169</th>\n",
       "      <td>2</td>\n",
       "      <td>190</td>\n",
       "      <td>191</td>\n",
       "      <td>190</td>\n",
       "      <td>191</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>191</td>\n",
       "      <td>...</td>\n",
       "      <td>216</td>\n",
       "      <td>215</td>\n",
       "      <td>213</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>213</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>209</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7170</th>\n",
       "      <td>4</td>\n",
       "      <td>201</td>\n",
       "      <td>205</td>\n",
       "      <td>208</td>\n",
       "      <td>209</td>\n",
       "      <td>214</td>\n",
       "      <td>216</td>\n",
       "      <td>218</td>\n",
       "      <td>223</td>\n",
       "      <td>226</td>\n",
       "      <td>...</td>\n",
       "      <td>112</td>\n",
       "      <td>169</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>237</td>\n",
       "      <td>113</td>\n",
       "      <td>91</td>\n",
       "      <td>67</td>\n",
       "      <td>70</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7171</th>\n",
       "      <td>2</td>\n",
       "      <td>173</td>\n",
       "      <td>174</td>\n",
       "      <td>173</td>\n",
       "      <td>174</td>\n",
       "      <td>173</td>\n",
       "      <td>173</td>\n",
       "      <td>175</td>\n",
       "      <td>175</td>\n",
       "      <td>174</td>\n",
       "      <td>...</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "      <td>197</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>197</td>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "      <td>193</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7172 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0         6     149     149     150     150     150     151     151     150   \n",
       "1         5     126     128     131     132     133     134     135     135   \n",
       "2        10      85      88      92      96     105     123     135     143   \n",
       "3         0     203     205     207     206     207     209     210     209   \n",
       "4         3     188     191     193     195     199     201     202     203   \n",
       "...     ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "7167      1     135     119     108     102     105      99      61     103   \n",
       "7168     12     157     159     161     164     166     166     171     174   \n",
       "7169      2     190     191     190     191     190     190     192     192   \n",
       "7170      4     201     205     208     209     214     216     218     223   \n",
       "7171      2     173     174     173     174     173     173     175     175   \n",
       "\n",
       "      pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0        151  ...       138       148       127        89        82        96   \n",
       "1        136  ...        47       104       194       183       186       184   \n",
       "2        147  ...        68       166       242       227       230       227   \n",
       "3        210  ...       154       248       247       248       253       236   \n",
       "4        203  ...        26        40        64        48        29        46   \n",
       "...      ...  ...       ...       ...       ...       ...       ...       ...   \n",
       "7167     121  ...       108       112       116       114       118       180   \n",
       "7168     175  ...       213       213       213       214       213       211   \n",
       "7169     191  ...       216       215       213       214       214       213   \n",
       "7170     226  ...       112       169       255       255       237       113   \n",
       "7171     174  ...       201       200       197       198       198       197   \n",
       "\n",
       "      pixel781  pixel782  pixel783  pixel784  \n",
       "0          106       112       120       107  \n",
       "1          184       184       182       180  \n",
       "2          226       225       224       222  \n",
       "3          230       240       253       255  \n",
       "4           49        46        46        53  \n",
       "...        ...       ...       ...       ...  \n",
       "7167       184       176       167       163  \n",
       "7168       210       210       209       208  \n",
       "7169       210       211       209       208  \n",
       "7170        91        67        70        63  \n",
       "7171       195       195       193       192  \n",
       "\n",
       "[7172 rows x 785 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asl_tst_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a362d3",
   "metadata": {
    "id": "94a362d3"
   },
   "source": [
    "# Data Visualization and Exploration\n",
    "\n",
    "Once again, let's start off by getting a feel for the Sign Language MNIST dataset. To do so, we'll analyze the values, features, and shape of the data. Further, we'll do some quick visualizes of what the sign language hand gesture images look like.\n",
    "\n",
    "Below we print out the shapes for the all the features in the training and testing sets. Notice, the class labels for each data sample can be accessed through the 'label' column while the rest of the columns correspond to pixels/features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35d4251d",
   "metadata": {
    "id": "35d4251d",
    "outputId": "57ad10c3-a25b-4182-c7d5-62006704af46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (27455, 785)\n",
      "Shape of training labels: (27455,)\n",
      "Shape of testing data: (7172, 785)\n",
      "Shape of testing labels: (7172,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of training data: {asl_trn_df.shape}\")\n",
    "print(f\"Shape of training labels: {asl_trn_df['label'].shape}\")\n",
    "\n",
    "print(f\"Shape of testing data: {asl_tst_df.shape}\")\n",
    "print(f\"Shape of testing labels: {asl_tst_df['label'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53c7b8f0",
   "metadata": {
    "id": "53c7b8f0",
    "outputId": "4ab80c61-169e-45d4-e5c5-7c9338d8ba9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels = np.unique(asl_trn_df['label'])\n",
    "\n",
    "class_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9360b65",
   "metadata": {
    "id": "f9360b65"
   },
   "source": [
    "Recall, we'll store all the unique labels in the data are given by `class_labels`. Keep the `class_labels` variable in mind as we'll need to it in order to get the REAL class labels for our predictions later on.\n",
    "\n",
    "How do we know which label corresponds to which sing language letter? Well, if you do some digging through the Sign Language MNIST dataset Kaggle website you'll find each letter simply corresponds to its numerical position where A starts a 0 and Z ends at 25. The below dictionary gives you the mapping of letters to class labels.\n",
    "\n",
    "Keep in mind, the letters J (corresponding to label 9) and Z (corresponding to label 25) are dropped as their sign language hand gesture requires motion which we can't capture with images. Thus, they are not included in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7cf8d10",
   "metadata": {
    "id": "f7cf8d10",
    "outputId": "62a3b2bc-bc4b-4036-d065-fef96619a423"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of classes: 24\n"
     ]
    }
   ],
   "source": [
    "class_names = {\n",
    "    0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G',\n",
    "    7: 'H', 8: 'I', 10: 'K', 11: 'L', 12: 'M', 13: 'N', 14: 'O',\n",
    "    15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T', 20: 'U', 21: 'V',\n",
    "    22: 'W',23: 'X', 24: 'Y'\n",
    "}\n",
    "\n",
    "print(f\"Total number of classes: {len(class_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2a1073",
   "metadata": {
    "id": "eb2a1073"
   },
   "source": [
    "Once again, let's go ahead and visualize some random data samples from each class to remind ourselves of what the data looks like.\n",
    "\n",
    "Remember, the `display_classes()` function  defined below visualizes random samples from each class. It displays `n_samples` per each unique class in our data. Thus, if `n_samples=5` then 5 random samples for each class will be displayed. However, instead of using a Pandas DataFrame we are going to be using good ol' Matplotlib.\n",
    "\n",
    "Take a second to look through the code. It's a little complex so if you have any questions be sure to ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7555d6c1",
   "metadata": {
    "id": "7555d6c1"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def display_classes(\n",
    "    images: np.ndarray,\n",
    "    labels:np.ndarray,\n",
    "    class_names: dict = None,\n",
    "    shape: tuple=(28,28),\n",
    "    n_samples=5):\n",
    "    \"\"\" Displays random samples from MNIST classes\n",
    "\n",
    "        Args:\n",
    "            images: A NumPy array where each row is a data sample and\n",
    "                each column corresponds to a pixel location.\n",
    "\n",
    "            labels: A NumPy array where each row corresponds to an\n",
    "                image label.\n",
    "\n",
    "            shape: 2D Shape of the images. By default each data sample in images\n",
    "                should be a flattened array.\n",
    "\n",
    "            n_samples: Number of samples to display\n",
    "\n",
    "    \"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = {}\n",
    "\n",
    "    # Get the unique class labels\n",
    "    class_labels = np.unique(labels)\n",
    "    # Compute the total number of images to plot\n",
    "    plot_n_images = len(class_labels) * n_samples\n",
    "\n",
    "    n_cols = n_samples\n",
    "    # Compute the number of rows and round up\n",
    "    n_rows = math.ceil(plot_n_images / n_cols)\n",
    "    # Define subplot\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols*4, n_rows*4))\n",
    "    # Plot images\n",
    "    for i, label in enumerate(class_labels):\n",
    "        # Find all class locations\n",
    "        all_class_locs = np.where(labels==label)[0]\n",
    "        # Randomly choose class data samples\n",
    "        selected_class_locs = np.random.choice(all_class_locs, n_samples, replace=False)\n",
    "        # Plot label/name per row\n",
    "        curr_class_name = class_names.get(label, label)\n",
    "        axs[i, 0].set_ylabel(f\"Class: {curr_class_name}\", size='large')\n",
    "\n",
    "        for j, cls_loc in enumerate(selected_class_locs):\n",
    "            # Reshape image into width x height\n",
    "            img = images[cls_loc].reshape(shape)\n",
    "            # Plot image\n",
    "            axs[i,j].imshow(img, cmap=plt.cm.gray)\n",
    "            # Disable x and y ticks\n",
    "            axs[i,j].set_xticks([])\n",
    "            axs[i,j].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d278ba0f",
   "metadata": {
    "id": "d278ba0f"
   },
   "source": [
    "Checkout the output we get when we run the `display_classes()` function below. Note the following:\n",
    "\n",
    "- The y-axis gives the class name.\n",
    "\n",
    "- Images for each class will look relatively similar. Remember, this dataset is mainly filled with augmented images. This means that the dataset started with very few data samples for each class and then different image augmentations such as rotations, lighting changes, crops, and others were applied to the images to create new images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "700ac503",
   "metadata": {
    "id": "700ac503"
   },
   "outputs": [],
   "source": [
    "def run_display_classes(asl_df):\n",
    "\n",
    "    images = asl_df.drop('label', axis=1).values\n",
    "    labels = asl_df['label'].values\n",
    "\n",
    "    display_classes(\n",
    "        images,\n",
    "        labels,\n",
    "        class_names,\n",
    "        shape=(28,28),\n",
    "        n_samples=5\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a343032",
   "metadata": {
    "id": "6a343032"
   },
   "source": [
    "Random training data samples are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9c350a8",
   "metadata": {
    "id": "f9c350a8",
    "outputId": "461c8294-0791-427a-dc48-c0f48cd96bc3"
   },
   "outputs": [],
   "source": [
    "run_display_classes(asl_trn_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09c1dad",
   "metadata": {
    "id": "d09c1dad"
   },
   "source": [
    "Random test data samples are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d3d4423",
   "metadata": {
    "id": "0d3d4423",
    "outputId": "b1b3cc04-cdce-42ec-e13f-89bd34d6c56a"
   },
   "outputs": [],
   "source": [
    "run_display_classes(asl_tst_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c37216",
   "metadata": {
    "id": "31c37216"
   },
   "source": [
    "# Data Preparation Pipeline\n",
    "\n",
    "Now it's time to recreate our data preparation pipelines we used in prior weeks. In this lab we will, once again, apply all our data cleaning and transformation operations AFTER splitting the data, since the data is technically partly split already.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a29cc4",
   "metadata": {
    "id": "45a29cc4"
   },
   "source": [
    "## Feature-label splitting\n",
    "\n",
    "Below, are the splitting functions we'll be using once again to split our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d732101",
   "metadata": {
    "id": "2d732101"
   },
   "outputs": [],
   "source": [
    "def feature_label_split(df: pd.DataFrame,\n",
    "                        label_name: str) -> Tuple[pd.DataFrame]:\n",
    "    \"\"\" Split dataframe into features and labels\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame containing both features and labels\n",
    "\n",
    "            label_name: Name of the column which contains the labels\n",
    "    \"\"\"\n",
    "\n",
    "    X = df.drop(label_name, axis=1)\n",
    "    y = df[[label_name]].copy()\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f92e775",
   "metadata": {
    "id": "7f92e775"
   },
   "source": [
    "Notice, we need to split both the training and testing data since the features and labels are given in the same CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a4d041c",
   "metadata": {
    "id": "9a4d041c",
    "outputId": "d89b6725-8b5b-4c79-cb9d-6ceab5a65cd7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>150</td>\n",
       "      <td>153</td>\n",
       "      <td>156</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>157</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>149</td>\n",
       "      <td>128</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>103</td>\n",
       "      <td>135</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>186</td>\n",
       "      <td>...</td>\n",
       "      <td>202</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>231</td>\n",
       "      <td>230</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>222</td>\n",
       "      <td>229</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>164</td>\n",
       "      <td>167</td>\n",
       "      <td>170</td>\n",
       "      <td>172</td>\n",
       "      <td>176</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>186</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>133</td>\n",
       "      <td>163</td>\n",
       "      <td>157</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27450</th>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>192</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>194</td>\n",
       "      <td>...</td>\n",
       "      <td>132</td>\n",
       "      <td>165</td>\n",
       "      <td>99</td>\n",
       "      <td>77</td>\n",
       "      <td>52</td>\n",
       "      <td>200</td>\n",
       "      <td>234</td>\n",
       "      <td>200</td>\n",
       "      <td>222</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27451</th>\n",
       "      <td>151</td>\n",
       "      <td>154</td>\n",
       "      <td>157</td>\n",
       "      <td>158</td>\n",
       "      <td>160</td>\n",
       "      <td>161</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>166</td>\n",
       "      <td>167</td>\n",
       "      <td>...</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>196</td>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27452</th>\n",
       "      <td>174</td>\n",
       "      <td>174</td>\n",
       "      <td>174</td>\n",
       "      <td>174</td>\n",
       "      <td>174</td>\n",
       "      <td>175</td>\n",
       "      <td>175</td>\n",
       "      <td>174</td>\n",
       "      <td>173</td>\n",
       "      <td>173</td>\n",
       "      <td>...</td>\n",
       "      <td>121</td>\n",
       "      <td>196</td>\n",
       "      <td>209</td>\n",
       "      <td>208</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27453</th>\n",
       "      <td>177</td>\n",
       "      <td>181</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>187</td>\n",
       "      <td>189</td>\n",
       "      <td>190</td>\n",
       "      <td>191</td>\n",
       "      <td>191</td>\n",
       "      <td>190</td>\n",
       "      <td>...</td>\n",
       "      <td>119</td>\n",
       "      <td>56</td>\n",
       "      <td>27</td>\n",
       "      <td>58</td>\n",
       "      <td>102</td>\n",
       "      <td>79</td>\n",
       "      <td>47</td>\n",
       "      <td>64</td>\n",
       "      <td>87</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27454</th>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>182</td>\n",
       "      <td>181</td>\n",
       "      <td>182</td>\n",
       "      <td>183</td>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>...</td>\n",
       "      <td>108</td>\n",
       "      <td>132</td>\n",
       "      <td>170</td>\n",
       "      <td>194</td>\n",
       "      <td>214</td>\n",
       "      <td>203</td>\n",
       "      <td>197</td>\n",
       "      <td>205</td>\n",
       "      <td>209</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27455 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
       "0         107     118     127     134     139     143     146     150     153   \n",
       "1         155     157     156     156     156     157     156     158     158   \n",
       "2         187     188     188     187     187     186     187     188     187   \n",
       "3         211     211     212     212     211     210     211     210     210   \n",
       "4         164     167     170     172     176     179     180     184     185   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "27450     189     189     190     190     192     193     193     193     193   \n",
       "27451     151     154     157     158     160     161     163     164     166   \n",
       "27452     174     174     174     174     174     175     175     174     173   \n",
       "27453     177     181     184     185     187     189     190     191     191   \n",
       "27454     179     180     180     180     182     181     182     183     182   \n",
       "\n",
       "       pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0          156  ...       207       207       207       207       206   \n",
       "1          157  ...        69       149       128        87        94   \n",
       "2          186  ...       202       201       200       199       198   \n",
       "3          211  ...       235       234       233       231       230   \n",
       "4          186  ...        92       105       105       108       133   \n",
       "...        ...  ...       ...       ...       ...       ...       ...   \n",
       "27450      194  ...       132       165        99        77        52   \n",
       "27451      167  ...       198       198       198       198       198   \n",
       "27452      173  ...       121       196       209       208       206   \n",
       "27453      190  ...       119        56        27        58       102   \n",
       "27454      182  ...       108       132       170       194       214   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0           206       206       204       203       202  \n",
       "1           163       175       103       135       149  \n",
       "2           199       198       195       194       195  \n",
       "3           226       225       222       229       163  \n",
       "4           163       157       163       164       179  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "27450       200       234       200       222       225  \n",
       "27451       196       195       195       195       194  \n",
       "27452       204       203       202       200       200  \n",
       "27453        79        47        64        87        93  \n",
       "27454       203       197       205       209       215  \n",
       "\n",
       "[27455 rows x 784 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27450</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27451</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27452</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27453</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27454</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27455 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "0          3\n",
       "1          6\n",
       "2          2\n",
       "3          2\n",
       "4         13\n",
       "...      ...\n",
       "27450     13\n",
       "27451     23\n",
       "27452     18\n",
       "27453     17\n",
       "27454     23\n",
       "\n",
       "[27455 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def TEST_feature_label_split_train():\n",
    "    X, y = feature_label_split(asl_trn_df, label_name='label')\n",
    "    display(X)\n",
    "    display(y)\n",
    "\n",
    "TEST_feature_label_split_train()\n",
    "garbage_collect(['TEST_feature_label_split_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35e78754",
   "metadata": {
    "id": "35e78754",
    "outputId": "a75fe8e5-f921-42f4-b83a-88de54ec52c4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>150</td>\n",
       "      <td>151</td>\n",
       "      <td>152</td>\n",
       "      <td>...</td>\n",
       "      <td>138</td>\n",
       "      <td>148</td>\n",
       "      <td>127</td>\n",
       "      <td>89</td>\n",
       "      <td>82</td>\n",
       "      <td>96</td>\n",
       "      <td>106</td>\n",
       "      <td>112</td>\n",
       "      <td>120</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126</td>\n",
       "      <td>128</td>\n",
       "      <td>131</td>\n",
       "      <td>132</td>\n",
       "      <td>133</td>\n",
       "      <td>134</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>136</td>\n",
       "      <td>138</td>\n",
       "      <td>...</td>\n",
       "      <td>47</td>\n",
       "      <td>104</td>\n",
       "      <td>194</td>\n",
       "      <td>183</td>\n",
       "      <td>186</td>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "      <td>182</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85</td>\n",
       "      <td>88</td>\n",
       "      <td>92</td>\n",
       "      <td>96</td>\n",
       "      <td>105</td>\n",
       "      <td>123</td>\n",
       "      <td>135</td>\n",
       "      <td>143</td>\n",
       "      <td>147</td>\n",
       "      <td>152</td>\n",
       "      <td>...</td>\n",
       "      <td>68</td>\n",
       "      <td>166</td>\n",
       "      <td>242</td>\n",
       "      <td>227</td>\n",
       "      <td>230</td>\n",
       "      <td>227</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>224</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>203</td>\n",
       "      <td>205</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>207</td>\n",
       "      <td>209</td>\n",
       "      <td>210</td>\n",
       "      <td>209</td>\n",
       "      <td>210</td>\n",
       "      <td>209</td>\n",
       "      <td>...</td>\n",
       "      <td>154</td>\n",
       "      <td>248</td>\n",
       "      <td>247</td>\n",
       "      <td>248</td>\n",
       "      <td>253</td>\n",
       "      <td>236</td>\n",
       "      <td>230</td>\n",
       "      <td>240</td>\n",
       "      <td>253</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>188</td>\n",
       "      <td>191</td>\n",
       "      <td>193</td>\n",
       "      <td>195</td>\n",
       "      <td>199</td>\n",
       "      <td>201</td>\n",
       "      <td>202</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>40</td>\n",
       "      <td>64</td>\n",
       "      <td>48</td>\n",
       "      <td>29</td>\n",
       "      <td>46</td>\n",
       "      <td>49</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7167</th>\n",
       "      <td>135</td>\n",
       "      <td>119</td>\n",
       "      <td>108</td>\n",
       "      <td>102</td>\n",
       "      <td>105</td>\n",
       "      <td>99</td>\n",
       "      <td>61</td>\n",
       "      <td>103</td>\n",
       "      <td>121</td>\n",
       "      <td>133</td>\n",
       "      <td>...</td>\n",
       "      <td>108</td>\n",
       "      <td>112</td>\n",
       "      <td>116</td>\n",
       "      <td>114</td>\n",
       "      <td>118</td>\n",
       "      <td>180</td>\n",
       "      <td>184</td>\n",
       "      <td>176</td>\n",
       "      <td>167</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7168</th>\n",
       "      <td>157</td>\n",
       "      <td>159</td>\n",
       "      <td>161</td>\n",
       "      <td>164</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>171</td>\n",
       "      <td>174</td>\n",
       "      <td>175</td>\n",
       "      <td>176</td>\n",
       "      <td>...</td>\n",
       "      <td>213</td>\n",
       "      <td>213</td>\n",
       "      <td>213</td>\n",
       "      <td>214</td>\n",
       "      <td>213</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>209</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7169</th>\n",
       "      <td>190</td>\n",
       "      <td>191</td>\n",
       "      <td>190</td>\n",
       "      <td>191</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>191</td>\n",
       "      <td>192</td>\n",
       "      <td>...</td>\n",
       "      <td>216</td>\n",
       "      <td>215</td>\n",
       "      <td>213</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>213</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>209</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7170</th>\n",
       "      <td>201</td>\n",
       "      <td>205</td>\n",
       "      <td>208</td>\n",
       "      <td>209</td>\n",
       "      <td>214</td>\n",
       "      <td>216</td>\n",
       "      <td>218</td>\n",
       "      <td>223</td>\n",
       "      <td>226</td>\n",
       "      <td>229</td>\n",
       "      <td>...</td>\n",
       "      <td>112</td>\n",
       "      <td>169</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>237</td>\n",
       "      <td>113</td>\n",
       "      <td>91</td>\n",
       "      <td>67</td>\n",
       "      <td>70</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7171</th>\n",
       "      <td>173</td>\n",
       "      <td>174</td>\n",
       "      <td>173</td>\n",
       "      <td>174</td>\n",
       "      <td>173</td>\n",
       "      <td>173</td>\n",
       "      <td>175</td>\n",
       "      <td>175</td>\n",
       "      <td>174</td>\n",
       "      <td>175</td>\n",
       "      <td>...</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "      <td>197</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>197</td>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "      <td>193</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7172 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
       "0        149     149     150     150     150     151     151     150     151   \n",
       "1        126     128     131     132     133     134     135     135     136   \n",
       "2         85      88      92      96     105     123     135     143     147   \n",
       "3        203     205     207     206     207     209     210     209     210   \n",
       "4        188     191     193     195     199     201     202     203     203   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "7167     135     119     108     102     105      99      61     103     121   \n",
       "7168     157     159     161     164     166     166     171     174     175   \n",
       "7169     190     191     190     191     190     190     192     192     191   \n",
       "7170     201     205     208     209     214     216     218     223     226   \n",
       "7171     173     174     173     174     173     173     175     175     174   \n",
       "\n",
       "      pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0         152  ...       138       148       127        89        82   \n",
       "1         138  ...        47       104       194       183       186   \n",
       "2         152  ...        68       166       242       227       230   \n",
       "3         209  ...       154       248       247       248       253   \n",
       "4         203  ...        26        40        64        48        29   \n",
       "...       ...  ...       ...       ...       ...       ...       ...   \n",
       "7167      133  ...       108       112       116       114       118   \n",
       "7168      176  ...       213       213       213       214       213   \n",
       "7169      192  ...       216       215       213       214       214   \n",
       "7170      229  ...       112       169       255       255       237   \n",
       "7171      175  ...       201       200       197       198       198   \n",
       "\n",
       "      pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0           96       106       112       120       107  \n",
       "1          184       184       184       182       180  \n",
       "2          227       226       225       224       222  \n",
       "3          236       230       240       253       255  \n",
       "4           46        49        46        46        53  \n",
       "...        ...       ...       ...       ...       ...  \n",
       "7167       180       184       176       167       163  \n",
       "7168       211       210       210       209       208  \n",
       "7169       213       210       211       209       208  \n",
       "7170       113        91        67        70        63  \n",
       "7171       197       195       195       193       192  \n",
       "\n",
       "[7172 rows x 784 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7167</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7168</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7169</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7170</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7171</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7172 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label\n",
       "0         6\n",
       "1         5\n",
       "2        10\n",
       "3         0\n",
       "4         3\n",
       "...     ...\n",
       "7167      1\n",
       "7168     12\n",
       "7169      2\n",
       "7170      4\n",
       "7171      2\n",
       "\n",
       "[7172 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def TEST_feature_label_split_test():\n",
    "    X, y = feature_label_split(asl_tst_df, label_name='label')\n",
    "    display(X)\n",
    "    display(y)\n",
    "\n",
    "TEST_feature_label_split_test()\n",
    "garbage_collect(['TEST_feature_label_split_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd7c4a4",
   "metadata": {
    "id": "7cd7c4a4"
   },
   "source": [
    "## Train-validation splitting\n",
    "As we already have a test set. We only need to split the training set into a validation set. Thus, we define the `train_valid_split()` function to split data into train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01a23106",
   "metadata": {
    "id": "01a23106"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_valid_split(X, y, test_size=.2, seed=42):\n",
    "    X_trn, X_vld, y_trn, y_vld = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "    return X_trn, y_trn, X_vld, y_vld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e945c4ea",
   "metadata": {
    "id": "e945c4ea",
    "outputId": "4b32306d-f859-49c9-83d4-d41c8b78011d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trn shape: (21964, 784)\n",
      "y_trn shape: (21964, 1)\n",
      "X_vld shape: (5491, 784)\n",
      "y_vld shape: (5491, 1)\n"
     ]
    }
   ],
   "source": [
    "def TEST_train_valid_split():\n",
    "    # Split into features and labels\n",
    "    X_trn, y_trn = feature_label_split(asl_trn_df, label_name='label')\n",
    "    # Split into train, validation, and test sets\n",
    "    X_trn, y_trn, X_vld, y_vld = train_valid_split(X_trn, y_trn)\n",
    "\n",
    "    print(f\"X_trn shape: {X_trn.shape}\")\n",
    "    print(f\"y_trn shape: {y_trn.shape}\")\n",
    "    print(f\"X_vld shape: {X_vld.shape}\")\n",
    "    print(f\"y_vld shape: {y_vld.shape}\")\n",
    "\n",
    "TEST_train_valid_split()\n",
    "garbage_collect(['TEST_train_valid_split'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2230f3",
   "metadata": {
    "id": "0a2230f3"
   },
   "source": [
    "## Transforming and cleaning data\n",
    "\n",
    "In this lab, we'll need to first clean our labels/targets by converting them to a one-hot encoding using our `OneHotEncoding` class from previous labs. In regards to feature cleaning, all we need to do is standardize the data (recall for neural networks we are going to treats the bias as a separate parameter we need to learn, thus we don't need to add a column of bias terms to our data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4291cb7a",
   "metadata": {
    "id": "4291cb7a"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b46c0b",
   "metadata": {
    "id": "00b46c0b"
   },
   "source": [
    "### Target cleaning\n",
    "Below is the `OneHotEncoding` class using in prior labs. Recall, we'll use this class when using the softmax activation function only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fe94f79",
   "metadata": {
    "id": "7fe94f79"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "class OneHotEncoding(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_names='auto'):\n",
    "        self.feature_names = feature_names\n",
    "        self.encoder = OneHotEncoder(categories=feature_names, sparse_output=False)\n",
    "\n",
    "    def fit(self, X: pd.DataFrame):\n",
    "\n",
    "        self.encoder.fit(X)\n",
    "\n",
    "        # Store names of features\n",
    "        self.feature_names = self.encoder.get_feature_names_out()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "        one_hot =  self.encoder.transform(X)\n",
    "\n",
    "        return pd.DataFrame(one_hot, columns=self.get_feature_names_out())\n",
    "\n",
    "    def get_feature_names_out(self, name=None)-> pd.Series:\n",
    "        return self.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1128bd",
   "metadata": {
    "id": "ef1128bd"
   },
   "source": [
    "Below we provide the `target_pipeline()` for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e69ee176",
   "metadata": {
    "id": "e69ee176"
   },
   "outputs": [],
   "source": [
    "def target_pipeline(\n",
    "    y_trn: pd.DataFrame,\n",
    "    y_vld: pd.DataFrame,\n",
    "    y_tst: pd.DataFrame\n",
    ") ->  List[pd.DataFrame]:\n",
    "    \"\"\" Creates Pipeline to apply data cleaning and transformations\n",
    "        to the targets/labels.\n",
    "\n",
    "        Args:\n",
    "            y_trn: train labels\n",
    "\n",
    "            y_vld: validation labels\n",
    "\n",
    "            y_tst: test labels\n",
    "    \"\"\"\n",
    "\n",
    "    target_pipe = Pipeline([\n",
    "        ('one-hot', OneHotEncoding())\n",
    "    ])\n",
    "\n",
    "    y_trn_clean = target_pipe.fit_transform(y_trn)\n",
    "    y_vld_clean = target_pipe.transform(y_vld)\n",
    "    y_tst_clean = target_pipe.transform(y_tst)\n",
    "\n",
    "    return y_trn_clean, y_vld_clean, y_tst_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae70a734",
   "metadata": {
    "id": "ae70a734",
    "outputId": "93012933-24de-43ed-ec98-61993a7c6b7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_trn shape: (21964, 24)\n",
      "y_vld shape: (5491, 24)\n",
      "y_tst shape: (7172, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JN\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\JN\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "      <th>label_5</th>\n",
       "      <th>label_6</th>\n",
       "      <th>label_7</th>\n",
       "      <th>label_8</th>\n",
       "      <th>label_10</th>\n",
       "      <th>...</th>\n",
       "      <th>label_15</th>\n",
       "      <th>label_16</th>\n",
       "      <th>label_17</th>\n",
       "      <th>label_18</th>\n",
       "      <th>label_19</th>\n",
       "      <th>label_20</th>\n",
       "      <th>label_21</th>\n",
       "      <th>label_22</th>\n",
       "      <th>label_23</th>\n",
       "      <th>label_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21959</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21960</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21961</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21962</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21963</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21964 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label_0  label_1  label_2  label_3  label_4  label_5  label_6  label_7  \\\n",
       "0          0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1          0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2          1.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3          0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4          0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "21959      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "21960      0.0      1.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "21961      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "21962      0.0      0.0      0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "21963      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "       label_8  label_10  ...  label_15  label_16  label_17  label_18  \\\n",
       "0          0.0       1.0  ...       0.0       0.0       0.0       0.0   \n",
       "1          0.0       0.0  ...       1.0       0.0       0.0       0.0   \n",
       "2          0.0       0.0  ...       0.0       0.0       0.0       0.0   \n",
       "3          0.0       1.0  ...       0.0       0.0       0.0       0.0   \n",
       "4          0.0       0.0  ...       0.0       0.0       0.0       0.0   \n",
       "...        ...       ...  ...       ...       ...       ...       ...   \n",
       "21959      0.0       0.0  ...       1.0       0.0       0.0       0.0   \n",
       "21960      0.0       0.0  ...       0.0       0.0       0.0       0.0   \n",
       "21961      0.0       0.0  ...       0.0       0.0       0.0       0.0   \n",
       "21962      0.0       0.0  ...       0.0       0.0       0.0       0.0   \n",
       "21963      0.0       0.0  ...       1.0       0.0       0.0       0.0   \n",
       "\n",
       "       label_19  label_20  label_21  label_22  label_23  label_24  \n",
       "0           0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "1           0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "2           0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "3           0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "4           0.0       0.0       0.0       1.0       0.0       0.0  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "21959       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "21960       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "21961       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "21962       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "21963       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[21964 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def TEST_target_pipeline():\n",
    "    # Split data into features and labels\n",
    "    X_trn, y_trn = feature_label_split(asl_trn_df, label_name='label')\n",
    "    X_tst, y_tst = feature_label_split(asl_tst_df, label_name='label')\n",
    "    # Split into train, validation, and test sets\n",
    "    X_trn, y_trn, X_vld, y_vld = train_valid_split(X_trn, y_trn)\n",
    "    # Clean targets\n",
    "    y_trn, y_vld, y_tst = target_pipeline(y_trn, y_vld, y_tst)\n",
    "\n",
    "    print(f\"y_trn shape: {y_trn.shape}\")\n",
    "    print(f\"y_vld shape: {y_vld.shape}\")\n",
    "    print(f\"y_tst shape: {y_tst.shape}\")\n",
    "\n",
    "    display(y_trn)\n",
    "\n",
    "TEST_target_pipeline()\n",
    "garbage_collect(['TEST_target_pipeline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd557e7c",
   "metadata": {
    "id": "dd557e7c"
   },
   "source": [
    "### Feature cleaning\n",
    "\n",
    "The only feature cleaning classes we'll need is our `Standardization` class which we redefine for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbb7b88f",
   "metadata": {
    "id": "bbb7b88f"
   },
   "outputs": [],
   "source": [
    "class Standardization(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, epsilon=1e-6):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def fit(self, X: Union[np.ndarray, pd.DataFrame], y: pd.DataFrame = None):\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        self.std = np.std(X, axis=0)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = (X  - self.mean) / (self.std + self.epsilon)\n",
    "        return X.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9342ee",
   "metadata": {
    "id": "3a9342ee"
   },
   "source": [
    "Below we provide the `feature_pipeline()` for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be8a828f",
   "metadata": {
    "id": "be8a828f"
   },
   "outputs": [],
   "source": [
    "def feature_pipeline(\n",
    "    X_trn: pd.DataFrame,\n",
    "    X_vld: pd.DataFrame,\n",
    "    X_tst: pd.DataFrame,\n",
    ") -> List[pd.DataFrame]:\n",
    "    \"\"\" Creates Pipeline to apply data cleaning and transformations\n",
    "        to the input features of our data.\n",
    "\n",
    "        Args:\n",
    "            X_trn: train features\n",
    "\n",
    "            X_vld: validation features\n",
    "\n",
    "            X_tst: test features\n",
    "    \"\"\"\n",
    "    feature_pipe = Pipeline([\n",
    "        ('standard', Standardization()),\n",
    "    ])\n",
    "\n",
    "    X_trn_clean = feature_pipe.fit_transform(X_trn)\n",
    "    X_vld_clean = feature_pipe.transform(X_vld)\n",
    "    X_tst_clean = feature_pipe.transform(X_tst)\n",
    "\n",
    "    return X_trn_clean, X_vld_clean, X_tst_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1c00c85",
   "metadata": {
    "id": "f1c00c85",
    "outputId": "8ac112d1-2de9-4712-eb82-36cdcaaee68d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trn shape: (21964, 784)\n",
      "X_tst shape: (7172, 784)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JN\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\JN\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22706</th>\n",
       "      <td>-1.176200</td>\n",
       "      <td>-2.098060</td>\n",
       "      <td>-0.600129</td>\n",
       "      <td>-2.854492</td>\n",
       "      <td>-3.760392</td>\n",
       "      <td>-2.532840</td>\n",
       "      <td>-2.758344</td>\n",
       "      <td>-2.866820</td>\n",
       "      <td>-2.909831</td>\n",
       "      <td>-2.994114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.746585</td>\n",
       "      <td>0.491823</td>\n",
       "      <td>-0.305828</td>\n",
       "      <td>-0.978212</td>\n",
       "      <td>-1.352838</td>\n",
       "      <td>-1.213103</td>\n",
       "      <td>-1.089376</td>\n",
       "      <td>-1.139868</td>\n",
       "      <td>-1.099969</td>\n",
       "      <td>-1.066928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>1.679378</td>\n",
       "      <td>1.688488</td>\n",
       "      <td>1.682091</td>\n",
       "      <td>1.801566</td>\n",
       "      <td>1.339504</td>\n",
       "      <td>0.483779</td>\n",
       "      <td>0.440272</td>\n",
       "      <td>0.848864</td>\n",
       "      <td>-1.776970</td>\n",
       "      <td>-1.394759</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131832</td>\n",
       "      <td>-0.226706</td>\n",
       "      <td>-0.336871</td>\n",
       "      <td>-0.428980</td>\n",
       "      <td>-0.442787</td>\n",
       "      <td>-0.614019</td>\n",
       "      <td>0.061683</td>\n",
       "      <td>-0.602116</td>\n",
       "      <td>-2.167454</td>\n",
       "      <td>-0.306548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>-0.038809</td>\n",
       "      <td>-0.091942</td>\n",
       "      <td>-0.138556</td>\n",
       "      <td>-0.175308</td>\n",
       "      <td>-0.171576</td>\n",
       "      <td>-0.235782</td>\n",
       "      <td>-0.273705</td>\n",
       "      <td>-0.340155</td>\n",
       "      <td>-0.399166</td>\n",
       "      <td>-0.499120</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.994564</td>\n",
       "      <td>-0.624191</td>\n",
       "      <td>0.454717</td>\n",
       "      <td>0.434099</td>\n",
       "      <td>0.137762</td>\n",
       "      <td>-0.046466</td>\n",
       "      <td>-0.505963</td>\n",
       "      <td>-0.443953</td>\n",
       "      <td>-0.942986</td>\n",
       "      <td>-1.361770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21099</th>\n",
       "      <td>-1.974794</td>\n",
       "      <td>-1.897448</td>\n",
       "      <td>-1.754061</td>\n",
       "      <td>-1.501894</td>\n",
       "      <td>-1.493772</td>\n",
       "      <td>-1.398149</td>\n",
       "      <td>-1.416068</td>\n",
       "      <td>-1.410272</td>\n",
       "      <td>-1.409555</td>\n",
       "      <td>-1.394759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.338749</td>\n",
       "      <td>0.293081</td>\n",
       "      <td>0.237418</td>\n",
       "      <td>0.151636</td>\n",
       "      <td>0.122072</td>\n",
       "      <td>0.158483</td>\n",
       "      <td>0.156290</td>\n",
       "      <td>0.204513</td>\n",
       "      <td>0.218689</td>\n",
       "      <td>0.252099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17752</th>\n",
       "      <td>-1.514997</td>\n",
       "      <td>-1.521301</td>\n",
       "      <td>-2.420777</td>\n",
       "      <td>-2.048135</td>\n",
       "      <td>-1.493772</td>\n",
       "      <td>-3.363102</td>\n",
       "      <td>-4.100621</td>\n",
       "      <td>-2.955997</td>\n",
       "      <td>-2.909831</td>\n",
       "      <td>-2.930140</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.853390</td>\n",
       "      <td>-1.174554</td>\n",
       "      <td>-1.578577</td>\n",
       "      <td>-1.009597</td>\n",
       "      <td>0.326049</td>\n",
       "      <td>0.205779</td>\n",
       "      <td>0.266666</td>\n",
       "      <td>0.315227</td>\n",
       "      <td>0.359974</td>\n",
       "      <td>0.407279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21575</th>\n",
       "      <td>-0.643804</td>\n",
       "      <td>-0.718854</td>\n",
       "      <td>-0.779629</td>\n",
       "      <td>-0.825595</td>\n",
       "      <td>-0.819182</td>\n",
       "      <td>-0.650913</td>\n",
       "      <td>-0.473619</td>\n",
       "      <td>-0.221253</td>\n",
       "      <td>-0.092987</td>\n",
       "      <td>0.108634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197574</td>\n",
       "      <td>-0.058540</td>\n",
       "      <td>0.796186</td>\n",
       "      <td>0.983330</td>\n",
       "      <td>0.749693</td>\n",
       "      <td>0.631444</td>\n",
       "      <td>0.392809</td>\n",
       "      <td>-0.095996</td>\n",
       "      <td>-0.487735</td>\n",
       "      <td>-0.446209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>0.808185</td>\n",
       "      <td>0.785735</td>\n",
       "      <td>0.758946</td>\n",
       "      <td>0.735095</td>\n",
       "      <td>0.745866</td>\n",
       "      <td>0.732858</td>\n",
       "      <td>0.640185</td>\n",
       "      <td>0.640785</td>\n",
       "      <td>0.641842</td>\n",
       "      <td>0.620428</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.790645</td>\n",
       "      <td>-1.679053</td>\n",
       "      <td>-1.764833</td>\n",
       "      <td>-1.527444</td>\n",
       "      <td>-1.494053</td>\n",
       "      <td>-1.354991</td>\n",
       "      <td>-1.026304</td>\n",
       "      <td>-1.029154</td>\n",
       "      <td>-1.366840</td>\n",
       "      <td>-1.672129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>1.146982</td>\n",
       "      <td>1.136806</td>\n",
       "      <td>1.117947</td>\n",
       "      <td>1.125267</td>\n",
       "      <td>1.150619</td>\n",
       "      <td>1.147989</td>\n",
       "      <td>1.182808</td>\n",
       "      <td>1.205569</td>\n",
       "      <td>1.223581</td>\n",
       "      <td>1.228183</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.210262</td>\n",
       "      <td>-0.318434</td>\n",
       "      <td>-0.414477</td>\n",
       "      <td>-0.491750</td>\n",
       "      <td>-0.552621</td>\n",
       "      <td>-0.598254</td>\n",
       "      <td>-0.616338</td>\n",
       "      <td>-0.649564</td>\n",
       "      <td>-0.644718</td>\n",
       "      <td>-0.663461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>1.316381</td>\n",
       "      <td>1.312341</td>\n",
       "      <td>1.220519</td>\n",
       "      <td>1.151279</td>\n",
       "      <td>1.150619</td>\n",
       "      <td>1.120313</td>\n",
       "      <td>1.125690</td>\n",
       "      <td>1.116393</td>\n",
       "      <td>1.101110</td>\n",
       "      <td>1.004273</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.461239</td>\n",
       "      <td>-1.220417</td>\n",
       "      <td>-1.671705</td>\n",
       "      <td>0.418406</td>\n",
       "      <td>0.122072</td>\n",
       "      <td>-0.882030</td>\n",
       "      <td>-0.726713</td>\n",
       "      <td>-0.554667</td>\n",
       "      <td>-1.413935</td>\n",
       "      <td>-1.547985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23654</th>\n",
       "      <td>-0.764803</td>\n",
       "      <td>-0.643624</td>\n",
       "      <td>-0.497557</td>\n",
       "      <td>-0.383400</td>\n",
       "      <td>0.125243</td>\n",
       "      <td>0.373077</td>\n",
       "      <td>0.583067</td>\n",
       "      <td>0.789413</td>\n",
       "      <td>0.917403</td>\n",
       "      <td>1.100234</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.308285</td>\n",
       "      <td>-1.709629</td>\n",
       "      <td>-1.749311</td>\n",
       "      <td>-1.982522</td>\n",
       "      <td>-2.247198</td>\n",
       "      <td>-2.080198</td>\n",
       "      <td>-2.098522</td>\n",
       "      <td>-1.772518</td>\n",
       "      <td>-2.340135</td>\n",
       "      <td>-0.989338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21964 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pixel1    pixel2    pixel3    pixel4    pixel5    pixel6    pixel7  \\\n",
       "22706 -1.176200 -2.098060 -0.600129 -2.854492 -3.760392 -2.532840 -2.758344   \n",
       "1231   1.679378  1.688488  1.682091  1.801566  1.339504  0.483779  0.440272   \n",
       "531   -0.038809 -0.091942 -0.138556 -0.175308 -0.171576 -0.235782 -0.273705   \n",
       "21099 -1.974794 -1.897448 -1.754061 -1.501894 -1.493772 -1.398149 -1.416068   \n",
       "17752 -1.514997 -1.521301 -2.420777 -2.048135 -1.493772 -3.363102 -4.100621   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "21575 -0.643804 -0.718854 -0.779629 -0.825595 -0.819182 -0.650913 -0.473619   \n",
       "5390   0.808185  0.785735  0.758946  0.735095  0.745866  0.732858  0.640185   \n",
       "860    1.146982  1.136806  1.117947  1.125267  1.150619  1.147989  1.182808   \n",
       "15795  1.316381  1.312341  1.220519  1.151279  1.150619  1.120313  1.125690   \n",
       "23654 -0.764803 -0.643624 -0.497557 -0.383400  0.125243  0.373077  0.583067   \n",
       "\n",
       "         pixel8    pixel9   pixel10  ...  pixel775  pixel776  pixel777  \\\n",
       "22706 -2.866820 -2.909831 -2.994114  ...  0.746585  0.491823 -0.305828   \n",
       "1231   0.848864 -1.776970 -1.394759  ... -0.131832 -0.226706 -0.336871   \n",
       "531   -0.340155 -0.399166 -0.499120  ... -0.994564 -0.624191  0.454717   \n",
       "21099 -1.410272 -1.409555 -1.394759  ...  0.338749  0.293081  0.237418   \n",
       "17752 -2.955997 -2.909831 -2.930140  ... -0.853390 -1.174554 -1.578577   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "21575 -0.221253 -0.092987  0.108634  ...  0.197574 -0.058540  0.796186   \n",
       "5390   0.640785  0.641842  0.620428  ... -0.790645 -1.679053 -1.764833   \n",
       "860    1.205569  1.223581  1.228183  ... -0.210262 -0.318434 -0.414477   \n",
       "15795  1.116393  1.101110  1.004273  ... -0.461239 -1.220417 -1.671705   \n",
       "23654  0.789413  0.917403  1.100234  ... -1.308285 -1.709629 -1.749311   \n",
       "\n",
       "       pixel778  pixel779  pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "22706 -0.978212 -1.352838 -1.213103 -1.089376 -1.139868 -1.099969 -1.066928  \n",
       "1231  -0.428980 -0.442787 -0.614019  0.061683 -0.602116 -2.167454 -0.306548  \n",
       "531    0.434099  0.137762 -0.046466 -0.505963 -0.443953 -0.942986 -1.361770  \n",
       "21099  0.151636  0.122072  0.158483  0.156290  0.204513  0.218689  0.252099  \n",
       "17752 -1.009597  0.326049  0.205779  0.266666  0.315227  0.359974  0.407279  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "21575  0.983330  0.749693  0.631444  0.392809 -0.095996 -0.487735 -0.446209  \n",
       "5390  -1.527444 -1.494053 -1.354991 -1.026304 -1.029154 -1.366840 -1.672129  \n",
       "860   -0.491750 -0.552621 -0.598254 -0.616338 -0.649564 -0.644718 -0.663461  \n",
       "15795  0.418406  0.122072 -0.882030 -0.726713 -0.554667 -1.413935 -1.547985  \n",
       "23654 -1.982522 -2.247198 -2.080198 -2.098522 -1.772518 -2.340135 -0.989338  \n",
       "\n",
       "[21964 rows x 784 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def TEST_feature_pipeline():\n",
    "    # Split data into features and labels\n",
    "    X_trn, y_trn = feature_label_split(asl_trn_df, label_name='label')\n",
    "    X_tst, y_tst = feature_label_split(asl_tst_df, label_name='label')\n",
    "    # Split into train, validation, and test sets\n",
    "    X_trn, y_trn, X_vld, y_vld = train_valid_split(X_trn, y_trn)\n",
    "    # Apply feature cleaning AFTER splitting\n",
    "    X_trn, X_vld, X_tst = feature_pipeline(X_trn, X_vld, X_tst)\n",
    "\n",
    "    print(f\"X_trn shape: {X_trn.shape}\")\n",
    "    print(f\"X_tst shape: {X_tst.shape}\")\n",
    "    display(X_trn)\n",
    "\n",
    "TEST_feature_pipeline()\n",
    "garbage_collect(['TEST_feature_pipeline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8764e45",
   "metadata": {
    "id": "d8764e45"
   },
   "source": [
    "\n",
    "## Putting it all together\n",
    "\n",
    "In order to make our lives easier we create a function called `data_prep()` which will split our data into the target/feature and apply data cleaning and transformations for us. Thus, whenever we go to implement a new algorithm we can call `data_prep()` to give us our data and override any other variables with the same names!\n",
    "\n",
    "**Take time to also read the DocStrings or in-line documentation, given at the start of the function, which describes what each argument does. You need to understand what each argument does before moving forward!**\n",
    "\n",
    "Note: Any arguments in a function given after the `*` in the function definition below MUST be passed using the keyword. See this [post](https://stackoverflow.com/questions/14301967/bare-asterisk-in-function-arguments) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03b2afd2",
   "metadata": {
    "id": "03b2afd2"
   },
   "outputs": [],
   "source": [
    "def data_prep(\n",
    "    trn_df: pd.DataFrame,\n",
    "    tst_df: pd.DataFrame,\n",
    "    label_name: str,\n",
    "    *,\n",
    "    seed: int = 42,\n",
    "    return_array: bool = False,\n",
    ") -> Tuple[pd.DataFrame]:\n",
    "    \"\"\" Splits data and runs data cleaning and transformations.\n",
    "\n",
    "        Args:\n",
    "            trn_df: A Pandas DataFrame containing the training dataset\n",
    "                for the current lab.\n",
    "\n",
    "            tst_df: A Pandas DataFrame containing the testing dataset\n",
    "                for the current lab.\n",
    "\n",
    "            label_name: Name of the column in the DataFrame which\n",
    "                will be used as the label/target. This will be passed\n",
    "                to the feature_label_split() function.\n",
    "\n",
    "            seed: The seed used when splitting data into train,\n",
    "                validation, and test. This will be passed to the\n",
    "                train_valid_test_split() function.\n",
    "\n",
    "            return_array: A boolean which when True will return all data as NumPy\n",
    "                arrays instead of Pandas DataFrames.\n",
    "    \"\"\"\n",
    "    # Apply feature and label splitting\n",
    "    X_trn, y_trn = feature_label_split(trn_df, label_name=label_name)\n",
    "    X_tst, y_tst = feature_label_split(tst_df, label_name=label_name)\n",
    "\n",
    "    # Split into train and validation sets\n",
    "    X_trn, y_trn, X_vld, y_vld = train_valid_split(X_trn, y_trn, seed=seed)\n",
    "\n",
    "    # Target cleaning\n",
    "    y_trn, y_vld, y_tst = target_pipeline(y_trn, y_vld, y_tst)\n",
    "\n",
    "    # Feature cleaning\n",
    "    X_trn, X_vld, X_tst = feature_pipeline(X_trn, X_vld, X_tst)\n",
    "\n",
    "    # Resets the Pandas index for the Dataframe and series\n",
    "    # This will prevent any headaches when combining or indexing\n",
    "    # our train, validation, and test data in the future.\n",
    "    X_trn.reset_index(inplace=True, drop=True)\n",
    "    y_trn.reset_index(inplace=True, drop=True)\n",
    "    X_vld.reset_index(inplace=True, drop=True)\n",
    "    y_vld.reset_index(inplace=True, drop=True)\n",
    "    X_tst.reset_index(inplace=True, drop=True)\n",
    "    y_tst.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Return data as arrays instead of DataFrames\n",
    "    if return_array:\n",
    "        X_trn, y_trn, X_vld, y_vld, X_tst, y_tst = (X_trn.values,\n",
    "                                                    y_trn.values,\n",
    "                                                    X_vld.values,\n",
    "                                                    y_vld.values,\n",
    "                                                    X_tst.values,\n",
    "                                                    y_tst.values)\n",
    "\n",
    "    return X_trn, y_trn, X_vld, y_vld, X_tst, y_tst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd513c08",
   "metadata": {
    "id": "bd513c08"
   },
   "source": [
    "#### TODO 2\n",
    "Complete this TODO by calling `data_prep()` function as specified below.\n",
    "\n",
    "1. Call the `data_prep()` so that it will return the Sign Language MNIST train, validation, and test data formatted as NumPy arrays. Do so by passing the arguments which correspond to the following:\n",
    "    1. Pass any required arguments (i.e., arguments with no default values).\n",
    "    1. Return all data as NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d32d0bc",
   "metadata": {
    "id": "3d32d0bc",
    "outputId": "59045acb-1287-49c1-b123-c3b8da944383"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JN\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\JN\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trn type: <class 'numpy.ndarray'>\n",
      "X_trn shape: (21964, 784)\n",
      "y_trn shape: (21964, 24)\n",
      "X_vld shape: (5491, 784)\n",
      "y_vld shape: (5491, 24)\n",
      "X_tst shape: (7172, 784)\n",
      "y_tst shape: (7172, 24)\n",
      "Your code PASSED the code check!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JN\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\JN\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# TODO 2.1\n",
    "data = data_prep(trn_df = asl_trn_df, tst_df = asl_tst_df, label_name = \"label\", return_array = True)\n",
    "X_trn, y_trn, X_vld, y_vld, X_tst, y_tst = data\n",
    "\n",
    "print(f\"X_trn type: {type(X_trn)}\")\n",
    "print(f\"X_trn shape: {X_trn.shape}\")\n",
    "print(f\"y_trn shape: {y_trn.shape}\")\n",
    "print(f\"X_vld shape: {X_vld.shape}\")\n",
    "print(f\"y_vld shape: {y_vld.shape}\")\n",
    "print(f\"X_tst shape: {X_tst.shape}\")\n",
    "print(f\"y_tst shape: {y_tst.shape}\")\n",
    "\n",
    "todo_check([\n",
    "    (type(X_trn) is np.ndarray,\"X_trn is not a NumPy Array\"),\n",
    "    (y_trn.shape == (21964, 24), \"y_trn does not have the shape 21964, 24)\"),\n",
    "    (y_vld.shape == (5491, 24), \"y_vld does not have the shape (5491, 24)\"),\n",
    "    (y_tst.shape == (7172, 24), \"y_tst does not have the shape (7172, 24)\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803e551b",
   "metadata": {
    "id": "803e551b"
   },
   "source": [
    "# Defining Classification Metrics and Plots\n",
    "\n",
    "Before we get to our algorithms, let's first define the metrics and plots we'll be using to assess how good predictions are.\n",
    "\n",
    "### Accuracy\n",
    "Below we redefine the `accuracy()` function which you defined in last module's lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8e03f6b",
   "metadata": {
    "id": "c8e03f6b"
   },
   "outputs": [],
   "source": [
    " def accuracy(y, y_hat):\n",
    "    # Convert y from one-hot encoding back to normal\n",
    "    if len(y.shape) > 1 and y.shape[-1] > 1:\n",
    "        y = np.argmax(y, axis=1).reshape(-1,1)\n",
    "    # Make sure y is 2D\n",
    "    elif len(y.shape) == 1:\n",
    "        y = y.reshape(-1, 1)\n",
    "    # Make sure y_hat is 2D\n",
    "    if len(y_hat.shape) == 1:\n",
    "        y_hat = y_hat.reshape(-1, 1)\n",
    "\n",
    "\n",
    "    total_correct = np.sum(y_hat == y)\n",
    "    total_samples = len(y)\n",
    "    print(f\"Accuracy ratio: {total_correct}/{total_samples}\")\n",
    "    return total_correct / total_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7593e1",
   "metadata": {
    "id": "8b7593e1"
   },
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Keep in mind, we will be looking at multi-class classification which means the confusion matrix will be a 3x3 since we have 3 classes. Recall the confusion for **binary classification** looked like the below image.\n",
    "\n",
    "<img src=\"http://www.andrewgurung.com/wp-content/uploads/2018/12/confusion_matrix.png\" width=500 height=500>\n",
    "\n",
    "\n",
    "The below image is what a **multi-class classification confusion matrix** looks like. Further, the below image shows how to determine the false positives, true positives, false negatives, and true negatives for each class. More on this later in the lab.\n",
    "\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/AuTKP.png\" width=300 height=300>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5f47c34",
   "metadata": {
    "id": "b5f47c34"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y, y_hat, class_names=None, figsize=(6, 4)):\n",
    "    # Convert y from one-hot encoding back to normal\n",
    "    if len(y.shape) > 1 and y.shape[-1] > 1:\n",
    "        y = np.argmax(y, axis=1).reshape(-1,1)\n",
    "\n",
    "    cfm = confusion_matrix(y_pred=y_hat, y_true=y)\n",
    "\n",
    "    labels = np.sort(np.unique(y))\n",
    "    if class_names is not None:\n",
    "        classes = []\n",
    "        for l in labels:\n",
    "            class_name = class_names.get(l, l)\n",
    "            classes.append(class_name)\n",
    "        labels = classes\n",
    "\n",
    "    columns, index = labels, labels\n",
    "    cfm_df = pd.DataFrame(cfm, index=index, columns=columns)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(cfm_df, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    plt.xlabel('Predicted', fontsize=14)\n",
    "    plt.ylabel('Acutal', fontsize=14)\n",
    "    return cfm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2a067d",
   "metadata": {
    "id": "0a2a067d"
   },
   "source": [
    "### Other scores\n",
    "\n",
    "Below we define functions for the positive predictive value (PPV) or precision, True positive rate (TPR) or recall, and true negative rate (TNR) or specificity.\n",
    "\n",
    "$$\n",
    "\\text{PPV} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{TNR} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}\n",
    "$$\n",
    "\n",
    "Lastly, we define `compute_scores()` which will automatically compute the PPV, TPR, and TNR scores for each class using the multi-class confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95ac2932",
   "metadata": {
    "id": "95ac2932"
   },
   "outputs": [],
   "source": [
    "def ppv(tp, fp):\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def tpr(tp, fn):\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def tnr(tn, fp):\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "def compute_scores(y, y_hat, class_names=None):\n",
    "    def print_scores(tn, fn, fp, tp):\n",
    "\n",
    "        print(f\"\\tPPV ratio tp/(tp+fp): {tp}/{tp+fp}\")\n",
    "        print(f\"\\tPPV (precision): {ppv(tp=tp, fp=fp) }\\n\")\n",
    "\n",
    "        print(f\"\\tTPR ratio tp/(tp+fn): {tp}/{tp+fn}\")\n",
    "        print(f\"\\tTPR (recall/sensitivity): {tpr(tp=tp, fn=fn)}\\n\")\n",
    "\n",
    "        print(f\"\\tTNR ratio tn/(tn+fp): {tn}/{tn+fp}\")\n",
    "        print(f\"\\tTNR (specificity): {tnr(tn=tn, fp=fp)}\")\n",
    "\n",
    "    if class_names is None:\n",
    "        class_names = {}\n",
    "\n",
    "    # Convert y from one-hot encoding back to normal\n",
    "    if len(y.shape) > 1 and y.shape[-1] > 1:\n",
    "        y = np.argmax(y, axis=1).reshape(-1,1)\n",
    "    # Make sure y is 2D\n",
    "    elif len(y.shape) == 1:\n",
    "        y = y.reshape(-1, 1)\n",
    "    # Make sure y_hat is 2D\n",
    "    if len(y_hat.shape) == 1:\n",
    "        y_hat = y_hat.reshape(-1, 1)\n",
    "\n",
    "    cm = confusion_matrix(y_true=y, y_pred=y_hat)\n",
    "\n",
    "    # Computing multi-class classification tp, fn, tp, tn\n",
    "    fp = cm.sum(axis=0) - np.diag(cm)\n",
    "    fn = cm.sum(axis=1) - np.diag(cm)\n",
    "    tp = np.diag(cm)\n",
    "    tn = cm.sum() - (fp + fn + tp)\n",
    "\n",
    "    ppv_ = ppv(tp=tp, fp=fp)\n",
    "    tpr_ = tpr(tp=tp, fn=fn)\n",
    "    tnr_ = tnr(tn=tn, fp=fp)\n",
    "\n",
    "    class_labels = np.unique(y)\n",
    "\n",
    "    if len(class_labels) == 2:\n",
    "        class_name = class_names.get(class_labels[-1], class_labels[-1])\n",
    "        print(f\"Scores for binary problem: positive label is {class_name}\")\n",
    "        print_scores(tn[-1], fn[-1], fp[-1], tp[-1])\n",
    "    else:\n",
    "        for i, label in enumerate(class_labels):\n",
    "            class_name = class_names.get(label, label)\n",
    "            print(f\"Scores for class {class_name}\")\n",
    "            print_scores(tn[i], fn[i], fp[i], tp[i])\n",
    "\n",
    "    return ppv_, tpr_, tnr_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0738dc54",
   "metadata": {
    "id": "0738dc54"
   },
   "source": [
    "# Multi-Layer Neural Networks\n",
    "Let's get onto adapting the existing neural network code to work with multiple layers. Recall, multi-layer refers to having more than one hidden layer.\n",
    "\n",
    "The below image is a simplified neural network schematic designed to match the dimensions of the Sign Language MNIST dataset. In this section we are going to build the below neural network schematic. Notice the following about the below image:\n",
    "\n",
    "- The image displays no weights in order to reduce clutter. However, keep in mind that every layer is still FULLY CONNECTED to the next layer!\n",
    "\n",
    "- For simplicity, we will only added 2 hidden layers with 2 hidden neurons. This allows us to create one of the simplistic multi-layer neural networks.\n",
    "\n",
    "- There are 784 inputs $x$ corresponding to the 784 features (i.e., pixels) for each data sample in the Sign Language MNIST dataset.\n",
    "\n",
    "- There are 24 outputs $\\hat{y}_\\text{prob}$ corresponding to the 24 predicted probabilities for each classes.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://live.staticflickr.com/65535/52002190784_5c2ed874eb_h.jpg\" width=\"2646\" height=\"1444\" alt=\"ASL_MNIST_nn\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db9276",
   "metadata": {
    "id": "48db9276"
   },
   "source": [
    "## Activation Functions\n",
    "\n",
    "Before we can implement neural networks we need to define our activation functions we'll be using in this lab. Recall each neuron can have an activation function. **The KEY element of neurons in neural networks are non-linear activation functions.** By introducing non-linearity with non-linear activation functions, neural networks are able to learn more complex non-linear models. In this section we are only going to implement the Softmax and ReLU activation functions.\n",
    "\n",
    "Below are classes who have two static methods `activation()` and `derivative()`. Here the  `activation()`  static method computes the equation of the activation function while the `derivative()` static method computes the derivative of the activation function. We'll use the `activation()` static method for the feed-forward processes and the `derivative()` static method for the feedback process.\n",
    "\n",
    "*Note: If you don't know what a static method is, see this [post](https://pythonbasics.org/static-method/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d0bd7",
   "metadata": {
    "id": "c67d0bd7"
   },
   "source": [
    "### Softmax\n",
    "\n",
    "Recall, the softmax activation function is an activation function that squashes values into probabilities such that the new values lie between 0 and 1 and the sum of all the new values is 1.\n",
    "\n",
    "\n",
    "**Activation equation**\n",
    "\n",
    "Once again, the softmax equation is given as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(z)= \\frac{e^{\\zv_i}}{\\sum_{k=1}^K e^{\\zv_k}}\n",
    "\\end{align}\n",
    "$$\n",
    "where each output neuron requires access to the linear combination $\\zv$ of every other output neuron in order to compute the sum $\\sum_{k=1}^K e^{\\zv_k}$.\n",
    "\n",
    "**Derivative equation**\n",
    "\n",
    "The derivative of the softmax is actually rather complex as it depends on multiple linear combinations from multiple output neurons. To keep things simple, we can combine the softmax with the NLL to get a very easy to compute derivative. Instead of computing the derivative for the softmax and NLL separately we'll combine the softmax and NLL and then compute the derivative of the softmax and NLL together (we'll do something like [this](https://d2l.ai/chapter_linear-networks/softmax-regression.html#softmax-and-derivatives) shortly). Thus, we'll set the `derivative()` static method to return an array of 1s which acts as a placeholder derivative. When used in backpropagation, this will act as if no derivative was taken applied for the activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed25bc4b",
   "metadata": {
    "id": "ed25bc4b"
   },
   "source": [
    "#### TODO 3\n",
    "\n",
    "Complete the TODO by implementing the `activation()` and `derivative()` static methods for the `Softmax`.\n",
    "\n",
    "1. Implement the `activation()` by implementing the softmax equation given above. Return the output of the equation.\n",
    "\n",
    "\n",
    "2. Implement the `derivative()` method to which outputs a placeholder derivative for the softmax. To do so, simply return an array of ones that is the same shape as the input `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "809ad83c",
   "metadata": {
    "id": "809ad83c"
   },
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        # TODO 3.1\n",
    "        pass # Replace this line with your code\n",
    "        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        # TODO 3.2\n",
    "        pass # Replace this line with your code\n",
    "        return np.ones_like(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeab029",
   "metadata": {
    "id": "5eeab029"
   },
   "source": [
    "Run the below `TEST_softmax()` function to test your implementation of the `Softmax` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "29964f11",
   "metadata": {
    "id": "29964f11",
    "outputId": "c9ba4f11-793d-4f02-9f79-9d0e7e81b6ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z BEFORE softmax:\n",
      "[[ 5  7 10]\n",
      " [12  3  5]]\n",
      "z AFTER softmax:\n",
      "[[0.00637746 0.04712342 0.94649912]\n",
      " [0.99896578 0.00012328 0.00091094]]\n",
      "z AFTER softmax sum:\n",
      "[[1.]\n",
      " [1.]]\n",
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "def TEST_softmax():\n",
    "    z = np.array([\n",
    "        [5, 7, 10],\n",
    "        [12, 3, 5]\n",
    "    ])\n",
    "    print(f\"z BEFORE softmax:\\n{z}\")\n",
    "    y_soft = Softmax.activation(z.T).T\n",
    "    print(f\"z AFTER softmax:\\n{y_soft}\")\n",
    "    print(f\"z AFTER softmax sum:\\n{y_soft.sum(axis=1, keepdims=True)}\")\n",
    "\n",
    "    todo_check([\n",
    "        (y_soft.shape == (2, 3), \"y_soft shape is incorrect\"),\n",
    "        (np.isclose(y_soft[0, 0], 0.00637746, rtol=.01),\"y_soft values are incorrect\"),\n",
    "    ])\n",
    "\n",
    "TEST_softmax()\n",
    "garbage_collect(['TEST_softmax'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0312a39",
   "metadata": {
    "id": "a0312a39"
   },
   "source": [
    "### ReLU\n",
    "\n",
    "In order to address the issues of tanh and the sigmoid activation functions the *Rectified Linear Unit* (ReLU) which squashes values between 0 and infinity (notice [ReLU is unbounded in the positive direction](https://stackoverflow.com/questions/53191284/numpy-overflow-in-calculations-disrupting-code), thus it can suffer from overflow errors when values are too large). The ReLU activation function was developed for its convenient mathematical properties which allow the gradient to be computed quickly. Furthermore, as we'll see shortly and in the deep learning section, ReLU is the go to activation function when stacking multiple hidden layers.\n",
    "\n",
    "\n",
    "**Activation equation**\n",
    "\n",
    "The ReLU activation function equation is given as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(z) &= \\max(0, z)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Derivative equation**\n",
    "\n",
    "The derivative of the ReLU activation is given as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g'(z) &= \\begin{cases}\n",
    "    & 1 \\quad  \\text{if} \\, z \\ge 0\\\\\n",
    "    & 0 \\quad  \\text{if} \\, z < 0\\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee39d20",
   "metadata": {
    "id": "5ee39d20"
   },
   "source": [
    "#### TODO 4\n",
    "\n",
    "Complete the TODO by implementing the `activation()` and `derivative()` static methods for the `ReLU` class.\n",
    "\n",
    "1. Implement the `activation()` method by implementing the ReLU equation given above. Return the output of the equation.\n",
    "\n",
    "\n",
    "2. Implement the `derivative()` method implementing the derivative of the ReLU equation given above. Return the output of the equation.\n",
    "    1. Hint: Make sure to copy `z` if you use the method shown in the notes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "66114409",
   "metadata": {
    "id": "66114409"
   },
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        # TODO 4.1\n",
    "        pass # Replace this line with your code\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        # TODO 4.2\n",
    "        pass # Replace this line with your code\n",
    "        return (z > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306e7cd",
   "metadata": {
    "id": "0306e7cd"
   },
   "source": [
    "Run the below `TEST_relu()` function to test your implementation of the `ReLU` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "edd13af0",
   "metadata": {
    "id": "edd13af0",
    "outputId": "852fda67-f58a-4894-9c19-c13fb5b9fd49"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "def TEST_relu():\n",
    "    x = np.arange(-10, 10 , .2)\n",
    "    y = ReLU.activation(x)\n",
    "    dy = ReLU.derivative(x)\n",
    "\n",
    "    plt.plot(x ,dy, label='Derivative')\n",
    "    plt.plot(x ,y, label='Activation')\n",
    "    plt.legend()\n",
    "    plt.show();\n",
    "\n",
    "    todo_check([\n",
    "        (np.isclose(y.sum(), 244.9999, rtol=.01), \"y has incorrect values\"),\n",
    "        (np.isclose(dy.sum(), 49.0, rtol=.01),  \"derivative of y `dy` has incorrect values\"),\n",
    "    ])\n",
    "\n",
    "TEST_relu()\n",
    "garbage_collect(['TEST_relu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e75f1c8",
   "metadata": {
    "id": "0e75f1c8"
   },
   "source": [
    "## Layer Class\n",
    "\n",
    "First, we need to start by defining a `Layer` class. Doing so will help our code to be more easily generalizable and organized  going forward. The `Layer` class will keep track of the number of neurons, number of inputs, weights, biases, activation function, linear combination output, and activation output for a given layer.\n",
    "\n",
    "Before moving on, read the documentation for the attributes in the `Layer` class below. The only method the `Layer` class has is the `print_info()` method which will print a summary of all attributes contained within the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "761bc0b4",
   "metadata": {
    "id": "761bc0b4"
   },
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \"\"\"Class for all variables required by a layer\n",
    "\n",
    "        Attributes:\n",
    "            W: NumPy array of weights for all neurons in the layer\n",
    "\n",
    "            b: NumPy array of biases for all neurons in the layer\n",
    "\n",
    "            g: Activation function for all neurons in the layer\n",
    "\n",
    "            name: Name of the layer\n",
    "\n",
    "            neurons: Number of neurons in the layer\n",
    "\n",
    "            inputs: Number of inputs into the layer\n",
    "\n",
    "            Z: Linear combination of weights and inputs for all neurons.\n",
    "                Initialized to an empty array until it is computed and set.\n",
    "\n",
    "            A: Activation output for all neurons. Initialized to an empty\n",
    "                array until it is computed and set.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        W:np.array,\n",
    "        b:np.array,\n",
    "        g: object,\n",
    "        name: str=\"Layer\"\n",
    "    ):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.g = g\n",
    "        self.name = name\n",
    "        self.neurons = len(W)\n",
    "        self.inputs = W.shape[1]\n",
    "        self.Z = np.array([])\n",
    "        self.A = np.array([])\n",
    "\n",
    "    def print_info(self):\n",
    "        \"\"\" Prints infor for all class attributes \"\"\"\n",
    "        print(f\"{self.name}\")\n",
    "        print(f\"\\tNeurons: {self.neurons}\")\n",
    "        print(f\"\\tInputs: {self.inputs}\")\n",
    "        print(f\"\\tWeight shape: {self.W.shape}\")\n",
    "        print(f\"\\tBias shape: {self.b.shape}\")\n",
    "        print(f\"\\tActivation function: {self.g.__name__}\")\n",
    "        print(f\"\\tZ shape: {self.Z.shape}\")\n",
    "        print(f\"\\tA shape: {self.A.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942d96fa",
   "metadata": {
    "id": "942d96fa"
   },
   "source": [
    "## Initializing Weights and Biases\n",
    "\n",
    "The first step, before performing the forward pass, is to initialize the structure of the neural network by initializing the weight and bias parameters for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fc72b2",
   "metadata": {
    "id": "c0fc72b2"
   },
   "source": [
    "#### TODO 5\n",
    "Complete the TODO by implementing the `init_weights()` function which initializes weight values using random uniform distribution.\n",
    "\n",
    "1. Initialize the weights for a layer using the input arguments of `neurons` and `inputs` and return the output. To do so, use the following tips:\n",
    "    1. Weights should ALWAYS have the shape (neurons, inputs). Think about what the inputs are and how many neurons the layer has.\n",
    "    1. Use the NumPy's `np.random.uniform()` function ([docs](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html)) to randomly generate the weight matrix of shape (neurons, inputs). The weight values should be restricted between the range of -0.5 (minimum) and 0.5 (maximum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "67136047",
   "metadata": {
    "id": "67136047"
   },
   "outputs": [],
   "source": [
    "def init_weights(neurons: int, inputs: int):\n",
    "    \"\"\" Initializes weight values using random uniform distribution\n",
    "\n",
    "        Args:\n",
    "            neurons: Number of neurons in the layer\n",
    "\n",
    "            inputs: Number of inputs to the layer\n",
    "    \"\"\"\n",
    "    # TODO 5.1\n",
    "    pass # Replace this line with your code\n",
    "    return np.random.uniform(-0.5, 0.5, (neurons, inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ba3725",
   "metadata": {
    "id": "e2ba3725"
   },
   "source": [
    "Run the below `TEST_init_weights()` function to test your implementation of the `init_weights()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6b9dc0d5",
   "metadata": {
    "id": "6b9dc0d5",
    "outputId": "5a8ef072-64f6-455f-9b77-cb91e07c41c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "def TEST_init_weights(times=100):\n",
    "    all_W = []\n",
    "    for t in range(times):\n",
    "        fake_W = init_weights(10, 100)\n",
    "        all_W.append(fake_W)\n",
    "    all_W = np.concatenate(all_W)\n",
    "\n",
    "    todo_check([\n",
    "        (fake_W.shape == (10, 100), \"fake_W has the wrong weight shapes\"),\n",
    "        (np.all((all_W > -0.5) & (all_W < 0.5)), \"Weight values are not between -0.5 and 0.5\")\n",
    "    ])\n",
    "\n",
    "TEST_init_weights()\n",
    "garbage_collect([\"TEST_init_weights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c53670",
   "metadata": {
    "id": "e5c53670"
   },
   "source": [
    "#### TODO 6\n",
    "Complete the TODO by finishing the implementation of the `init_neural_network()` function which initializes the a weights and biases for a multi-layer neural network.\n",
    "\n",
    "**Be sure to read the documentation inside the function to understand what each variable does.** If you need additional explanations see the notes or ask!\n",
    "\n",
    "1. Determine the input for the the current layer based on the current the for-loop iteration. To do so, define the following if...else statement:\n",
    "    1. `if` the current layer index `l` corresponds to the 1st hidden layer index, set `inputs` equal to the number of inputs features.\n",
    "    1. `else` the current layer is NOT the 1st hidden layer, set `inputs` equal to the number of outputs in the previous layer.\n",
    "        1. Hint: Try indexing `neurons_per_layer`.\n",
    "       \n",
    "       \n",
    "2. Initialize the weights for the current layer in the for-loop iteration. To do so, use the the `init_weights()` function. Store the output into `W`.\n",
    "\n",
    "\n",
    "3. Initialize the biases for the current layer in the for-loop iteration. Store the output into `b`. To do so, use the following tips:\n",
    "    1. The biases should be a vector of all 1s with the shape (neurons, 1). Try using `np.ones()` ([docs](https://numpy.org/doc/stable/reference/generated/numpy.ones.html)).\n",
    "    \n",
    "    \n",
    "4. Initialize an instance of the `Layer` class for the current layer in the for-loop iteration. To do so, pass all the required arguments and the name of the layer, given by `name`. Store the output into `layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7d987e61",
   "metadata": {
    "id": "7d987e61"
   },
   "outputs": [],
   "source": [
    "def init_neural_network(\n",
    "    n_input_features: int,\n",
    "    neurons_per_layer: List[int],\n",
    "    g_hidden: object,\n",
    "    g_output: object,\n",
    "    seed=0\n",
    ")-> List[Layer]:\n",
    "    \"\"\" Initializes weights and biases for a multi-layer neural network\n",
    "\n",
    "        Args:\n",
    "            n_input_features: Number of features the input data has\n",
    "\n",
    "            neurons_per_layer: A list where each element represents\n",
    "                the neurons in a layer. For example, [2, 3] would\n",
    "                create a 2 layer neural network where the hidden layer\n",
    "                has 2 neurons and the output layer has 3 neurons.\n",
    "\n",
    "            g_hidden: Activation function used by ALL neurons\n",
    "                in ALL hidden layers.\n",
    "\n",
    "            g_output: Activation function used by ALL neurons\n",
    "                in the output layer.\n",
    "\n",
    "            seed: Determines global seed to use when initalizing\n",
    "                weights.\n",
    "    \"\"\"\n",
    "    nn = []\n",
    "    # Set the seed to get reproducible intial weight values\n",
    "    np.random.seed(seed)\n",
    "    for l, neurons in enumerate(neurons_per_layer):\n",
    "        # TODO 6.1\n",
    "        if l == 0:\n",
    "            inputs = n_input_features\n",
    "        else:\n",
    "            inputs = neurons_per_layer[l - 1]\n",
    "        # Set activation functions for the output\n",
    "        # layer neurons and set the names of the nn\n",
    "        # g: stores current layer's activation function\n",
    "        # name: stores the current layer's human readable name\n",
    "        if l == len(neurons_per_layer)-1:\n",
    "            g = g_output\n",
    "            name = f\"Layer {l+1}: Output Layer\"\n",
    "        else:\n",
    "            g = g_hidden\n",
    "            name = f\"Layer {l+1}: Hidden Layer\"\n",
    "\n",
    "        # TODO 6.2\n",
    "        W = init_weights(neurons, inputs)\n",
    "\n",
    "        # TODO 6.3\n",
    "        b = np.ones((neurons, 1))\n",
    "\n",
    "        # TODO 6.4\n",
    "        layer = Layer(W, b, g, name)\n",
    "        nn.append(layer)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5067c3d8",
   "metadata": {
    "id": "5067c3d8"
   },
   "source": [
    "Run the below code to test your implementation of the `init_neural_network()` function where we define a neural network with matches our neural network schematic given below.\n",
    "\n",
    "Take note that `neurons_per_layer` argument is a list of integers where each element corresponds to the number of neurons in the layer. The means, the total number of elements corresponds to the total number of layers one wishes to create a neural network with. Further, the last element always corresponds to the number of neurons in the output layer.\n",
    "\n",
    "For example, `neurons_per_layer=[2, 2, 24]` creates a 3 layer neural network where the 1st hidden layer has 2 neurons, the 2nd hidden layer has 2 neurons, and the output layer has 24 neurons.\n",
    "\n",
    "\n",
    "<img src=\"https://live.staticflickr.com/65535/52002190784_5c2ed874eb_h.jpg\" width=\"2646\" height=\"1444\" alt=\"ASL_MNIST_nn\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "01aca892",
   "metadata": {
    "id": "01aca892",
    "outputId": "28b68451-a75f-4f30-bad2-f697000ac9b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "neurons_per_layer = [2 ,2, 24]\n",
    "g_hidden = ReLU\n",
    "g_output = Softmax\n",
    "\n",
    "nn = init_neural_network(\n",
    "    n_input_features=X_trn.shape[1],\n",
    "    neurons_per_layer=neurons_per_layer,\n",
    "    g_hidden=g_hidden,\n",
    "    g_output=g_output\n",
    ")\n",
    "\n",
    "todo_check([\n",
    "    (len(nn) == 3, \"nn has the wrong length\"),\n",
    "    (nn[0].W.shape == (2, 784), \"1st hidden layer weight shapes are incorrect\"),\n",
    "    (nn[0].b.shape  == (2, 1), \"1st hidden layer bias shapes are incorrect\"),\n",
    "    (nn[1].W.shape  == (2, 2), \"2nd hidden layer weight shapes are incorrect\"),\n",
    "    (nn[1].b.shape  == (2, 1), \"2nd hidden layer bias shapes are incorrect\"),\n",
    "    (nn[2].W.shape  == (24, 2), \"Output layer weight shapes are incorrect\"),\n",
    "    (nn[2].b.shape  == (24, 1), \"Output layer bias shapes are incorrect\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2788c40a",
   "metadata": {
    "id": "2788c40a"
   },
   "source": [
    "Below is the output of the `init_neural_network()` function. Notice, it is a list of `Layer` class instances where we have 3 layers: one for the 1st hidden layer, one for the 2nd hidden layer, and one for the output layer. These layers are given in order from 1st hidden layer to output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5c05640b",
   "metadata": {
    "id": "5c05640b",
    "outputId": "91ea5c19-f5eb-4ca4-e35d-5f50d21d4f9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Layer at 0x1888db26f90>,\n",
       " <__main__.Layer at 0x1889041d090>,\n",
       " <__main__.Layer at 0x1889041cf50>]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92db0ab",
   "metadata": {
    "id": "f92db0ab"
   },
   "source": [
    "We can loop over all the `Layer` class instances in `nn` to print the attribute information using the `print_info()` method. Notice, the 1st hidden layer corresponds to element 0, the 2nd hidden layer corresponds to element 1, and the output layer corresponds to element 2. Lastly, notice, `Z`, the linear combination of weights and inputs, and `A`, the output of the neuron, for each layer are empty arrays as they have not yet been computed. Recall, these will be compute during the forward pass!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "26a4ad74",
   "metadata": {
    "id": "26a4ad74",
    "outputId": "a37eb5a7-9b54-47eb-d6e7-7ee153a67e48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Hidden Layer\n",
      "\tNeurons: 2\n",
      "\tInputs: 784\n",
      "\tWeight shape: (2, 784)\n",
      "\tBias shape: (2, 1)\n",
      "\tActivation function: ReLU\n",
      "\tZ shape: (0,)\n",
      "\tA shape: (0,)\n",
      "Layer 2: Hidden Layer\n",
      "\tNeurons: 2\n",
      "\tInputs: 2\n",
      "\tWeight shape: (2, 2)\n",
      "\tBias shape: (2, 1)\n",
      "\tActivation function: ReLU\n",
      "\tZ shape: (0,)\n",
      "\tA shape: (0,)\n",
      "Layer 3: Output Layer\n",
      "\tNeurons: 24\n",
      "\tInputs: 2\n",
      "\tWeight shape: (24, 2)\n",
      "\tBias shape: (24, 1)\n",
      "\tActivation function: Softmax\n",
      "\tZ shape: (0,)\n",
      "\tA shape: (0,)\n"
     ]
    }
   ],
   "source": [
    "for layer in nn:\n",
    "    layer.print_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5798bf8a",
   "metadata": {
    "id": "5798bf8a"
   },
   "source": [
    "## Multi-Layer Forward Pass\n",
    "\n",
    "Time to adapt our new neural network code to perform the feed-forward process for multiple layers. The forward pass or feed-forward process is relatively simple. Recall, every layer uses the same 2 equations to compute the linear combinations $\\Zm$ and activation outputs $\\Am$.\n",
    "$$\n",
    "\\Zm^{[\\text{layer}]} = \\Wm^{[\\text{layer}]} \\Am^{[\\text{layer}-1]} + \\bv^{[\\text{layer}]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Am^{[\\text{layer}]} = g(\\Zm^{[\\text{layer}]})\n",
    "$$\n",
    "where $\\Am^{[0]} = \\Xm$. **Here it will be helpful to about interpret $\\text{layer}$  in $\\Am{^{[\\text{layer}]}}$/$\\Zm{^{[\\text{layer}]}}$/$\\Wm{^{[\\text{layer}]}}$ as the current layer that is being updated in the forward pass process (i.e., each iteration of the for-loop). Thus, $\\text{layer}-1$ can be thought of as the previous layer (i.e., previous iteration of the for-loop).**\n",
    "\n",
    "Since we utilize the same two equations for each layer, we can generalize the code by implementing a `forward()` function which applies the above two equations to each layer in our neural network by creating a for-loop that loops over the list of `Layer` class instances stored in `nn`. Additionally, the loop sets the linear combination variable `linear.Z` and activation output variable `linear.A` for each layer.\n",
    "\n",
    " The following is how each code variable in the `forward()` function will map to the variables in the equations:\n",
    "- `layer` corresponds to $\\text{layer}$\n",
    "- `layer.W` corresponds to $\\Wm^{[\\text{layer}]}$\n",
    "- `layer.b` corresponds to $\\bv^{[\\text{layer}]}$\n",
    "- `A` corresponds to $\\Am^{[\\text{layer}-1]}$ and represents the current layer's input (i.e., the previous layer's output)\n",
    "- `layer.Z` corresponds to $\\Zm^{[\\text{layer}]} $\n",
    "- `layer.g.activation(layer.Z)` corresponds to $g(\\Zm^{[\\text{layer}]})$\n",
    "- `layer.A` corresponds to $\\Am^{[\\text{layer}]} $ the output of the current layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad9b03",
   "metadata": {
    "id": "bbad9b03"
   },
   "source": [
    "#### TODO 7\n",
    "Complete the TODO by finishing the `forward()` function which performs the forward pass for a multi-layer neural network.\n",
    "\n",
    "\n",
    "**Before the loop**\n",
    "\n",
    "1. Start by setting `A` equal to the the transpose of the input features `X`. Recall `A` will act as the input for each layer and will be updated for each iteration of the for-loop!\n",
    "\n",
    "**Inside the loop**\n",
    "\n",
    "2. Compute $\\Zm^{[\\text{layer}]}$ the current layer's linear combination between the layer's inputs and weights. Store the output into the current layer's $\\Zm^{[\\text{layer}]}$ variable given by `layer.Z`.\n",
    "\n",
    "$$\n",
    "\\Zm^{[\\text{layer}]} = \\Wm^{[\\text{layer}]} \\Am^{[\\text{layer}-1]} + \\bv^{[\\text{layer}]}\n",
    "$$\n",
    "\n",
    "3. Compute $\\Am^{[\\text{layer}]}$ the current layer's activation output. Store the output into the current layer's $\\Am^{[\\text{layer}]}$  variable into `layer.A`.\n",
    "\n",
    "4. Update  `A`  to store the input for the next layer by storing the current layer's output `layer.A`.\n",
    "\n",
    "**After the loop**\n",
    "\n",
    "5. Store the transpose of the output of the neural network into `y_hat`.\n",
    "    1. Hint: Recall, `A` will be storing the output of the network (you can also think about this as the input for the loss function). Furthermore, remember, `A` here will have the shape (output neurons, data samples) so we need to transpose it to get (data samples, output neurons) which matches the ground truth shape for `y` which is (data samples, classes). Here, the output layer has a neuron for each class such that the number output layer neurons equals the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a129321a",
   "metadata": {
    "id": "a129321a"
   },
   "outputs": [],
   "source": [
    "def forward(X:np.ndarray, nn: List[Layer], verbose: bool = False):\n",
    "    \"\"\" Performs the forward pass for a multi-layer neural network\n",
    "\n",
    "        Args:\n",
    "            X: Input features. This should be typically be the\n",
    "                training data.\n",
    "\n",
    "            nn: List of Layer class instances which together\n",
    "                define a nueral network.\n",
    "\n",
    "            verbose: If True then information regarding eqation shapes\n",
    "                will be printed for each layer.\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO 7.1\n",
    "    A = X.T\n",
    "\n",
    "    # layer: the current Layer class instance\n",
    "    # l: the index position of layer in the nn list\n",
    "    for l, layer in enumerate(nn):\n",
    "\n",
    "        # TODO 7.2\n",
    "        layer.Z = layer.W @ A + layer.b\n",
    "        if verbose:  print(f\"Z{l+1}{layer.Z.shape} = W{l+1}{layer.W.shape} @ A{l}{A.shape} + b{l+1}{layer.b.shape}\")\n",
    "\n",
    "        # TODO 7.3\n",
    "        layer.A = layer.g().activation(layer.Z)\n",
    "        if verbose: print(f\"A{l+1}{layer.A.shape} = g(Z{l+1}{layer.Z.shape})\")\n",
    "\n",
    "        # TODO 7.4\n",
    "        A = layer.A\n",
    "\n",
    "    # TODO 7.5\n",
    "    y_hat = A.T\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61f8ad6",
   "metadata": {
    "id": "b61f8ad6"
   },
   "source": [
    "Run the below code to test your implementation of the `forward()` function.\n",
    "\n",
    "Notice, we added some useful print statements which print the shapes of all the variables and their corresponding variable name and layer number. For example, \"W1\" in the print statements corresponds to the shape for weights in the 1st hidden layer and \"A0\" corresponds to the shape of the input features which act as inputs into the 1st hidden layer. Thus, the number after each variable corresponds to the layer number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1947e539",
   "metadata": {
    "id": "1947e539",
    "outputId": "e8869cc4-0161-4deb-9658-2b86e4ff3b8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1(2, 21964) = W1(2, 784) @ A0(784, 21964) + b1(2, 1)\n",
      "A1(2, 21964) = g(Z1(2, 21964))\n",
      "Z2(2, 21964) = W2(2, 2) @ A1(2, 21964) + b2(2, 1)\n",
      "A2(2, 21964) = g(Z2(2, 21964))\n",
      "Z3(24, 21964) = W3(24, 2) @ A2(2, 21964) + b3(24, 1)\n",
      "A3(24, 21964) = g(Z3(24, 21964))\n",
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "y_hat_probs = forward(X_trn, nn, verbose=True)\n",
    "\n",
    "todo_check([\n",
    "    (y_hat_probs.shape == (21964, 24), \"y_hat_probs has the wrong shape\"),\n",
    "    (np.isclose(nn[0].Z[0, 0], 8.29492345, rtol=.01), \"1st hidden layer has incorrect Z values\"),\n",
    "    (np.isclose(nn[0].A[0, 0], 8.29492345, rtol=.01), \"1st hidden layer has incorrect A values\"),\n",
    "    (np.isclose(nn[2].Z[0, 0], 0.5277135, rtol=.01), \"2nd hidden layer has incorrect Z values\"),\n",
    "    (np.isclose(nn[2].A[0, 0], 0.01902055, rtol=.01), \"2nd hidden layer has incorrect A values\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e964f2",
   "metadata": {
    "id": "a6e964f2"
   },
   "source": [
    "Below we can see the output transformed by the softmax activation function giving us the probability that each data sample belongs to a given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a49212c3",
   "metadata": {
    "id": "a49212c3",
    "outputId": "fe2b86a6-5d60-43e2-db32-a78f4618ded0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01902055, 0.00858561, 0.05921423, ..., 0.08747698, 0.00889091,\n",
       "        0.01045515],\n",
       "       [0.01203956, 0.00399601, 0.05813763, ..., 0.09986916, 0.00419438,\n",
       "        0.0052512 ],\n",
       "       [0.02536399, 0.01411603, 0.03604656, ..., 0.05785986, 0.03549318,\n",
       "        0.01782481],\n",
       "       ...,\n",
       "       [0.01693732, 0.0066681 , 0.05033411, ..., 0.08722703, 0.01085709,\n",
       "        0.00877691],\n",
       "       [0.02536399, 0.01411603, 0.03604656, ..., 0.05785986, 0.03549318,\n",
       "        0.01782481],\n",
       "       [0.025394  , 0.01414364, 0.03616855, ..., 0.0579605 , 0.03534544,\n",
       "        0.01784557]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ebe5891c",
   "metadata": {
    "id": "ebe5891c",
    "outputId": "83725856-39f5-4b79-e840-b6b8a8b51fdc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21964, 24)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c401f",
   "metadata": {
    "id": "c27c401f"
   },
   "source": [
    "Below we can see all the probabilities for each data sample sum to one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ce7c7f95",
   "metadata": {
    "id": "ce7c7f95",
    "outputId": "03cbaa9f-9c75-40f3-fdf6-f796765975fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_probs.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadbda11",
   "metadata": {
    "id": "dadbda11"
   },
   "source": [
    "If we loop over all the `Layer` class instances in `nn` to print the attribute information using the `print_info()` method again we can see that `Z` and `A` have been set for each layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dbe0811d",
   "metadata": {
    "id": "dbe0811d",
    "outputId": "89c12763-ba0d-41e4-9389-65eebdc6e342"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Hidden Layer\n",
      "\tNeurons: 2\n",
      "\tInputs: 784\n",
      "\tWeight shape: (2, 784)\n",
      "\tBias shape: (2, 1)\n",
      "\tActivation function: ReLU\n",
      "\tZ shape: (2, 21964)\n",
      "\tA shape: (2, 21964)\n",
      "Layer 2: Hidden Layer\n",
      "\tNeurons: 2\n",
      "\tInputs: 2\n",
      "\tWeight shape: (2, 2)\n",
      "\tBias shape: (2, 1)\n",
      "\tActivation function: ReLU\n",
      "\tZ shape: (2, 21964)\n",
      "\tA shape: (2, 21964)\n",
      "Layer 3: Output Layer\n",
      "\tNeurons: 24\n",
      "\tInputs: 2\n",
      "\tWeight shape: (24, 2)\n",
      "\tBias shape: (24, 1)\n",
      "\tActivation function: Softmax\n",
      "\tZ shape: (24, 21964)\n",
      "\tA shape: (24, 21964)\n"
     ]
    }
   ],
   "source": [
    "for layer in nn:\n",
    "    layer.print_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c5b767",
   "metadata": {
    "id": "29c5b767"
   },
   "source": [
    "### Computing the negative log likelihood (NLL) loss\n",
    "\n",
    "As alway for classification problems, we need to  compute the negative log likelihood (NLL) loss. Below we define the `nll` which computes the cost over all data samples. Notice, that we add a small epsilon value to prevent any divide by zero errors/warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "454f3e88",
   "metadata": {
    "id": "454f3e88"
   },
   "outputs": [],
   "source": [
    "def nll(y, y_hat_probs, epsilon=1e-5):\n",
    "    loss = y * np.log(y_hat_probs+epsilon)\n",
    "    cost = -np.sum(loss)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fbed8165",
   "metadata": {
    "id": "fbed8165",
    "outputId": "a561aa91-807a-4132-eec0-93cd7276e6f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(75942.01549529441)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = nll(y=y_trn, y_hat_probs=y_hat_probs)\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945d6be4",
   "metadata": {
    "id": "945d6be4"
   },
   "source": [
    "To compute the average NLL loss we simply divide by the total number of training data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7825199d",
   "metadata": {
    "id": "7825199d",
    "outputId": "0d0c58e0-5846-49d6-d06e-12b544105b9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3.4575676331858682)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_loss = cost / len(y_trn)\n",
    "mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293dc269",
   "metadata": {
    "id": "293dc269"
   },
   "source": [
    "## Multi-Layer Backward Pass\n",
    "\n",
    "Now it's time to generalize our neural network backward pass using backpropagation to work for multiple layers as well! As mentioned in the notes, the generalization of the backpropagation code is where things begin to get slightly more ugly and complex. If you haven't, please review the *Multi-Layer Feedback: Backpropagation* from this modules notes and the *Backpropagation* section from the *Neural Networks: Regression* notes. We'll be assuming you have an idea of the notation and a gernal idea of how backpropagation works.\n",
    "\n",
    "In this section, we will breakdown how to write the generalized `backward()` function. Further, before we do so, we will unfold the for-loop and compute each layer's derivative one at a time using the pattern/trick we learned about in the notes.\n",
    "\n",
    "Recall, **the main trick is to pass $\\frac{\\partial NLL}{\\partial \\Am^{[\\text{layer}-1]}}$ the partial derivative of a layer with respect to its inputs onto the next layer in the backpropagation process.** Doing so will, reduce the redundant computations we have to make due to the same partial derivatives appearing over and over again as we move backwards through the network. See the notes for more information about this pattern/trick."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be060982",
   "metadata": {
    "id": "be060982"
   },
   "source": [
    "### Computing softmax and negative log likelihood (NLL) loss derivative\n",
    "\n",
    "Before even getting the the backward pass code we need to first define the derivative of our loss function. Recall, we are using the NLL loss which means we need to compute its derivaitve.\n",
    "\n",
    "However, recall, in order to simplify the derivative of the softmax we can compute the derivative of the softmax and NLL loss together. The reason why we did not compute the derivative of the softmax when we defined `Softmax` class above is because computing the derivative of the softmax, on its own, is rather complex. The derivate of the softmax actually has two cases where the derivative differs depending on the case.\n",
    "\n",
    "However, we can account for both cases and therefore simplify the derivative by plugging in the softmax into the NLL loss function and then solving for the derivative. For a <u>single data sample</u>, the NLL loss with the softmax plugged into it would look like the following:\n",
    "\n",
    "$$\n",
    "NLL= -\\yv \\log \\frac{e^{\\hat{y}_i}}{\\sum_{k=1}^K e^{\\hat{y}_k}}\n",
    "$$\n",
    "where $\\hat{y}_i = a^{[3]}_i = z^{[3]}_i$ in our 3 layer neural network.\n",
    "\n",
    "By taking the derivative of the above equation we get the following:\n",
    "\n",
    "$$\n",
    "\\frac{d NLL}{d \\hat{\\yv}} = (\\hat{\\yv} - \\yv)\n",
    "$$\n",
    "where $\\hat{\\yv}$ is vector of output probabilities (one probability for each class) for a single data sample and $\\yv$ is a one-hot encoded vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b9ff10",
   "metadata": {
    "id": "46b9ff10"
   },
   "source": [
    "#### TODO 8\n",
    "Compute the TODO by implementing the `delta_softmax_nll()` function which computes the derivative of the softmax and NLL loss function together at once.\n",
    "\n",
    "1. Convert the derivative equation for the softmax and NLL loss into code. Return the output of the equation.\n",
    "\n",
    "$$\n",
    "\\frac{d NLL}{d \\hat{\\yv}} = (\\hat{\\yv} - \\yv)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "63ea4a4b",
   "metadata": {
    "id": "63ea4a4b"
   },
   "outputs": [],
   "source": [
    "# TODO 8.1\n",
    "def delta_softmax_nll(y, y_hat_probs):\n",
    "    pass # Replace this line with your codes\n",
    "    return y_hat_probs - y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d279838",
   "metadata": {
    "id": "4d279838"
   },
   "source": [
    "Run the below `TEST_delta_softmax_nll()` function to test your implementation of the `delta_softmax_nll()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8a30ada6",
   "metadata": {
    "id": "8a30ada6",
    "outputId": "19d70eed-baf7-44bf-beb2-81b9597057ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "def TEST_delta_softmax_nll():\n",
    "    fake_yoh = np.array([\n",
    "        [0., 1., 0.],\n",
    "        [0., 0., 1.]\n",
    "    ])\n",
    "    fake_probs = np.array([\n",
    "        [.1, .5, .4],\n",
    "        [.1, .2, .7],\n",
    "    ])\n",
    "    delta_sm_nll = delta_softmax_nll(y=fake_yoh, y_hat_probs=fake_probs)\n",
    "\n",
    "    todo_check([\n",
    "        (np.all(np.isclose(delta_sm_nll.flatten(), [ 0.1, -0.5,  0.4,  0.1,  0.2, -0.3], rtol=.01)), \"delta_sm_nll has incorrect values\"),\n",
    "    ])\n",
    "\n",
    "TEST_delta_softmax_nll()\n",
    "garbage_collect(['TEST_delta_softmax_nll'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5931cb",
   "metadata": {
    "id": "2c5931cb"
   },
   "source": [
    "### Output layer\n",
    "As mentioned, before we write the generalized backward pass code, we will first unfold the loop. This means we will implement each iteration of the for-loop manually so we can more clearly see what is happening before we add more levels of abstraction by using a for-loop.\n",
    "\n",
    "Recall our network schematic is as follows:\n",
    "\n",
    "<img src=\"https://live.staticflickr.com/65535/52002190784_5c2ed874eb_h.jpg\" width=\"2646\" height=\"1444\" alt=\"ASL_MNIST_nn\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27212d1c",
   "metadata": {
    "id": "27212d1c"
   },
   "source": [
    "Thus, starting with the output layer, we can to compute the partial derivative of NLL with respect to $\\Zm^{[3]}$ using our first shared terms $\\frac{\\partial NLL}{\\partial \\Am^{[3]}}\\frac{\\partial \\Am^{[3]}}{\\partial \\Zm^{[3]}}$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\Zm^{[3]}} &= \\frac{\\partial NLL}{\\partial \\Am^{[3]}}^\\top \\frac{\\partial \\Am^{[3]}}{\\partial \\Zm^{[3]}} \\\\\n",
    "&= (\\hat{\\Ym}_{\\text{prob}} - \\Ym)^\\top * g'(\\Zm^{[3]})\n",
    "\\end{align}\n",
    "$$\n",
    "where $g'(\\Zm^{[3]})$ is the derivative of the activation function for output layer's neurons and $\\frac{\\partial NLL}{\\partial \\Am^{[3]}} = \\frac{\\partial NLL}{\\partial \\hat{\\Ym}_{\\text{prob}}}$ is the partial derivative of the NLL loss with respect to the output of the network, which we already computed above.\n",
    "\n",
    "Remember, $g'(\\Zm^{[3]})$  would normally be the derivative of the softmax function. However, since we already computed it when we computed $\\frac{\\partial NLL}{\\partial \\Am^{[3]}}$ then $g'(\\Zm^{[3]}) = \\mathbf{1}$. Meaning, we set the softmax derivative to be a column vectors of 1s such that it WILL NOT influence our gradient equations! Thus, we can also just ignore it.\n",
    "\n",
    "Finally, note $\\hat{\\Ym}_{\\text{prob}}$ and $\\Ym$ are matrices because $\\hat{\\Ym}_{\\text{prob}}$contains multiple data samples and predicted probabilities for each class while $\\Ym$ contains one-hot encodings for multiple data samples.\n",
    "\n",
    "Using $\\frac{\\partial NLL}{\\partial \\Zm^{[3]}}$  we can compute the gradients for the weights $\\Wm^{[3]}$ and biases $\\bv^{[3]}$ as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\Wm^{[3]}} &= \\frac{\\partial NLL}{\\partial \\Zm^{[3]}} \\cdot \\frac{\\partial \\Zm^{[3]}}{\\partial \\Wm^{[3]}} ^\\top  \\\\\n",
    "&= \\frac{\\partial NLL}{\\partial \\Zm^{[3]}} \\cdot \\Am^{[2]\\top},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\bv^{[3]}} &= \\frac{\\partial NLL}{\\partial \\Zm^{[3]}} \\cdot \\frac{\\partial \\Zm^{[3]}}{\\partial \\bv^{[3]}}^\\top  \\\\\n",
    "&= \\frac{\\partial NLL}{\\partial \\Zm^{[3]}}  \\cdot \\mathbf{1}^\\top.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Finally, we can compute the partial derivative of NLL with respect to the layer inputs $\\Am^{[2]}$ by taking the dot product between $\\frac{\\partial NLL}{\\partial \\Zm^{[3]}}$ and $\\frac{\\partial \\Zm^{[3]}}{\\partial \\Am^{[2]}}$ as given:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\Am^{[2]}}& = \\frac{\\partial \\Zm^{[3]}}{\\partial \\Am^{[2]}}^\\top \\cdot \\frac{\\partial NLL}{\\partial \\Zm^{[3]}} \\\\\n",
    "&= \\Wm^{[3]\\top} \\cdot \\frac{\\partial NLL}{\\partial \\Zm^{[3]}}\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\frac{\\partial NLL}{\\partial \\Am^{[2]}}$ will be passed on and utilized by the next layer's gradient equations. **Now here is the big idea, pattern, or trick. Notice, for this and proceeding steps we always pass on the partial derivative of the NLL with respect to the current layers input's (i.e., previous layer's outputs) $\\frac{\\partial NLL}{\\partial \\Am^{[\\text{layer-1}]}}$ to the next layer in the backpropagation process!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96afd759",
   "metadata": {
    "id": "96afd759"
   },
   "source": [
    "#### TODO 9\n",
    "Complete the TODO by performing the backpropagation step for the output layer. Use the below tables that map equation variables to code variables to help solve this TODO.\n",
    "\n",
    "<font color='red'>**Refer to the notes if you are struggling!**</font>\n",
    "\n",
    "**Setting the current layer**\n",
    "\n",
    "1. Set `output_layer` to store the element which corresponds to the output layer within `nn` the list of `Layer` class instances.\n",
    "    1. Hint: Recall, `nn` stores all the `Layer` class instances where the first element corresponds to the 1st hidden layer and the last element corresponds to the output layer.\n",
    "\n",
    "**Computing `delta_A3` $\\frac{\\partial NLL}{\\partial \\Am^{[3]}}$**\n",
    "\n",
    "2. Compute $\\frac{\\partial NLL}{\\partial \\Am^{[3]}}$ the partial derivative of the loss $NLL$ with respect to the neural network output $\\hat{\\Ym} = \\Am^{[3]\\top}$. Use the below equation for guidance Store the output into `delta_A3`.\n",
    "    1. Hint: Be sure to transpose the solution to the partial derivative $\\frac{\\partial NLL}{\\partial \\Am^{[3]}}$ as neural networks require the shape (classes, data samples).\n",
    "    \n",
    "    $$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\Am^{[3]}} =  (\\hat{\\Ym}_{\\text{prob}} - \\Ym)^\\top\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Computing `delta_Z3` $\\frac{\\partial NLL}{\\partial \\Zm^{[3]}}$**\n",
    "       \n",
    "\n",
    "3. Compute $\\frac{\\partial NLL}{\\partial \\Zm^{[3]}}$ the partial derivative of the loss $NLL$ with respect to the linear combinations of weights and inputs for the output layer. Store the output into `delta_Z3`\n",
    "    1. Hint: Use `delta_A3` which we computed in the previous TODO.\n",
    "    \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\Zm^{[3]}} &= \\frac{\\partial NLL}{\\partial \\Am^{[3]}}^\\top \\frac{\\partial \\Am^{[3]}}{\\partial \\Zm^{[3]}} \\\\\n",
    "&= (\\hat{\\Ym}_{\\text{prob}} - \\Ym)^\\top * g'(\\Zm^{[3]})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Computing `delta_W3` $\\frac{\\partial NLL}{\\partial \\Wm^{[3]}}$ and `delta_b3` $\\frac{\\partial NLL}{\\partial \\bv^{[3]}}$**\n",
    "\n",
    "4. Compute $\\frac{\\partial NLL}{\\partial \\Wm^{[3]}}$ the partial derivative of the loss $NLL$ with respect to the output layer weights. Store the output into `delta_W3`.\n",
    "    1. Hint: Use `delta_Z3` which we computed in the previous TODO.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\Wm^{[3]}} &= \\frac{\\partial NLL}{\\partial \\Zm^{[3]}} \\cdot \\frac{\\partial \\Zm^{[3]}}{\\partial \\Wm^{[3]}}^\\top  \\\\\n",
    "&= \\frac{\\partial NLL}{\\partial \\Zm^{[3]}} \\cdot \\Am^{[2]\\top},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "5. Compute $\\frac{\\partial NLL}{\\partial \\bv^{[3]}}$ the partial derivative of the loss $NLL$ with respect to the output layer biases. Store the output into `delta_b3`.\n",
    "    1. Hint: Use `delta_Z3` which we computed in the previous TODO.\n",
    "    1. Hint: You'll need to define an `np.ones()` array with shape (1, data samples) and transpose it to represent $\\mathbf{1}^\\top$. A second option is that you can just sum over `delta_Z3` and not include the vector of ones or the dot product.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\bv^{[3]}} &= \\frac{\\partial NLL}{\\partial \\Zm^{[3]}} \\cdot \\frac{\\partial \\Zm^{[3]}}{\\partial \\bv^{[3]}}^\\top  \\\\\n",
    "&= \\frac{\\partial NLL}{\\partial \\Zm^{[3]}}  \\cdot \\mathbf{1}^\\top.\n",
    "\\end{align}\n",
    "$$\n",
    "**Computing `delta_A2` $\\frac{\\partial NLL}{\\partial \\Am^{[2]}}$**\n",
    "\n",
    "6. Compute $\\frac{\\partial NLL}{\\partial \\Am^{[2]}}$ the partial derivative of the loss $NLL$ with respect to the output layer's inputs $\\Am^{[2]}$. This partial derivative will be used to by the next layer in the backpropagation process. Store the output into `delta_A2`.\n",
    "    1. Hint: Use `delta_Z3` which we computed in the previous TODO.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\Am^{[2]}}& = \\frac{\\partial \\Zm^{[3]}}{\\partial \\Am^{[2]}}^\\top \\cdot \\frac{\\partial NLL}{\\partial \\Zm^{[3]}} \\\\\n",
    "&= \\Wm^{[3]\\top} \\cdot \\frac{\\partial NLL}{\\partial \\Zm^{[3]}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9fbc4127",
   "metadata": {
    "id": "9fbc4127",
    "outputId": "07e982b7-eaaa-4ae0-ca38-e043f651d1ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 3: Output Layer\n",
      "\tNeurons: 24\n",
      "\tInputs: 2\n",
      "\tWeight shape: (24, 2)\n",
      "\tBias shape: (24, 1)\n",
      "\tActivation function: Softmax\n",
      "\tZ shape: (24, 21964)\n",
      "\tA shape: (24, 21964)\n",
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "# TODO 9.1\n",
    "output_layer = nn[-1]\n",
    "\n",
    "layer.print_info()\n",
    "\n",
    "todo_check([\n",
    "    (output_layer.name == 'Layer 3: Output Layer', \"output_layer has wrong name. Make sure you selected the output layer from nn.\"),\n",
    "    (output_layer.neurons == 24, \"output_layer has the wrong number of output neurons.\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0ef87339",
   "metadata": {
    "id": "0ef87339",
    "outputId": "60f01568-6998-4802-9411-3dfd69b50c32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_A3(24, 21964) = (y_hat_probs(21964, 24) - y(21964, 24))^T\n",
      "delta_A3 Shape: (24, 21964)\n",
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "print(f\"delta_A3(24, 21964) = (y_hat_probs(21964, 24) - y(21964, 24))^T\")\n",
    "\n",
    "# TODO 9.2\n",
    "delta_A3 = (y_hat_probs - y_trn).T\n",
    "print(f\"delta_A3 Shape: {delta_A3.shape}\")\n",
    "\n",
    "todo_check([\n",
    "    (delta_A3.shape == (24, 21964), \"delta_A3 has incorrect shape\"),\n",
    "    (np.all(np.isclose(delta_A3[0, :3], np.array([ 0.01902055,  0.01203956, -0.97463601]), rtol=.01)), \"delta_A3 has incorrect values\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "65a526e5",
   "metadata": {
    "id": "65a526e5",
    "outputId": "7695ee6f-8410-4b8f-8037-8ad54738ca4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_Z3(24, 21964) = delta_A3(24, 21964) * g'(Z3(24, 21964))\n",
      "delta_Z3 Shape: (24, 21964)\n",
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "print(f\"delta_Z3(24, 21964) = delta_A3(24, 21964) * g'(Z3(24, 21964))\")\n",
    "\n",
    "# TODO 9.3\n",
    "delta_Z3 = delta_A3\n",
    "print(f\"delta_Z3 Shape: {delta_Z3.shape}\")\n",
    "\n",
    "todo_check([\n",
    "    (delta_Z3.shape == (24, 21964), \"delta_Z3 has incorrect shape\"),\n",
    "    (np.all(np.isclose(delta_Z3[0, :3], np.array([ 0.01902055,  0.01203956, -0.97463601]), rtol=.01)), \"delta_Z3 has incorrect values\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "50ff464b",
   "metadata": {
    "id": "50ff464b",
    "outputId": "cfe41050-72a6-4bb9-99e3-1cd0ec2ad299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_W3(24, 2) = delta_Z3(24, 21964) @ A2(2, 21964)^T\n",
      "delta_W3 Shape: (24, 2)\n",
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "print(f\"delta_W3(24, 2) = delta_Z3(24, 21964) @ A2(2, 21964)^T\")\n",
    "\n",
    "# TODO 9.4\n",
    "delta_W3 = delta_Z3 @ nn[1].A.T\n",
    "print(f\"delta_W3 Shape: {delta_W3.shape}\")\n",
    "\n",
    "todo_check([\n",
    "    (delta_W3.shape == (24, 2), \"delta_W3 has incorrect shape\"),\n",
    "    (np.all(np.isclose(delta_W3[:3, 0], np.array([ 37.32041819, -330.15201929,   25.23538659]), rtol=.01)), \"delta_W3 has incorrect values\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137bd8b7",
   "metadata": {
    "id": "137bd8b7"
   },
   "source": [
    "Below we compute the average weight gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c53a0321",
   "metadata": {
    "id": "c53a0321",
    "outputId": "2d8ed413-65b9-48b8-e040-a38d56c1443e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W3 average gradient shape: (24, 2)\n"
     ]
    }
   ],
   "source": [
    "W3_avg_grad = delta_W3 / len(y_trn)\n",
    "print(f\"W3 average gradient shape: {W3_avg_grad.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "776cbd0b",
   "metadata": {
    "id": "776cbd0b",
    "outputId": "16af751f-9681-42c6-da97-ecb8b4e04fc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_b3(24, 1) = delta_Z3(24, 21964) @ 1(1, 21964)^T\n",
      "delta_b3 Shape: (24, 1)\n",
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "print(f\"delta_b3(24, 1) = delta_Z3(24, 21964) @ 1(1, 21964)^T\")\n",
    "\n",
    "# TODO 9.5\n",
    "delta_b3 = delta_Z3 @ np.ones((delta_Z3.shape[1], 1))\n",
    "print(f\"delta_b3 Shape: {delta_b3.shape}\")\n",
    "\n",
    "todo_check([\n",
    "    (delta_b3.shape == (24, 1), \"delta_b3 has incorrect shape\"),\n",
    "    (np.all(np.isclose(delta_b3[:3], np.array([[-428.714],[-575.719],[ 120.967]]), rtol=.01)), \"delta_b3 has incorrect values\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc6379b",
   "metadata": {
    "id": "ffc6379b"
   },
   "source": [
    "Below we compute the average bias gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4dda5600",
   "metadata": {
    "id": "4dda5600",
    "outputId": "858f0e40-7f70-4c7c-fb8b-06ebb24b220c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b3 average gradient shape: (24, 1)\n"
     ]
    }
   ],
   "source": [
    "b3_avg_grad = delta_b3 / len(y_trn)\n",
    "print(f\"b3 average gradient shape: {b3_avg_grad.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b5c26cea",
   "metadata": {
    "id": "b5c26cea",
    "outputId": "09d9b467-5a9b-4229-8a3b-8c057e4ec615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_A2(2, 21964) = W3(24, 2)^T @ delta_Z3(24, 21964)\n",
      "delta_A2 Shape: (2, 21964)\n",
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "print(f\"delta_A2(2, 21964) = W3(24, 2)^T @ delta_Z3(24, 21964)\")\n",
    "# TODO 9.6\n",
    "delta_A2 = output_layer.W.T @ delta_Z3\n",
    "print(f\"delta_A2 Shape: {delta_A2.shape}\")\n",
    "\n",
    "todo_check([\n",
    "    (delta_A2.shape == (2, 21964), \"delta_A2 has incorrect shape\"),\n",
    "    (np.all(np.isclose(delta_A2[0, :3], np.array([-0.17170991, -0.35322078,  0.3286222 ]), rtol=.01)), \"delta_A2 has incorrect values\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78930fc",
   "metadata": {
    "id": "f78930fc"
   },
   "source": [
    "Using the gradients we can update the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "094644f0",
   "metadata": {
    "id": "094644f0"
   },
   "outputs": [],
   "source": [
    "alpha = .01\n",
    "\n",
    "output_layer_W = output_layer.W - alpha * W3_avg_grad\n",
    "output_layer_b = output_layer.b - alpha * b3_avg_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c4f9f0aa",
   "metadata": {
    "id": "c4f9f0aa",
    "outputId": "3c5f4f7d-3518-4ad6-a92d-16699ba2bf1a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.18238512, -0.17018285],\n",
       "       [-0.48015804, -0.45861651],\n",
       "       [-0.2421898 ,  0.24006837],\n",
       "       [ 0.12828053,  0.26928703],\n",
       "       [ 0.26864753,  0.35588184],\n",
       "       [ 0.22007348,  0.47770646],\n",
       "       [ 0.39882941,  0.08634762],\n",
       "       [ 0.08835016, -0.46543183],\n",
       "       [ 0.49851207, -0.36764655],\n",
       "       [ 0.24019181,  0.32038573],\n",
       "       [-0.12676579, -0.30266708],\n",
       "       [-0.40125143,  0.24837352],\n",
       "       [-0.04741295,  0.21372759],\n",
       "       [ 0.41540044, -0.35284351],\n",
       "       [ 0.41916545, -0.08834976],\n",
       "       [-0.19473793,  0.44177782],\n",
       "       [ 0.49075854, -0.3005795 ],\n",
       "       [ 0.15688462, -0.39269459],\n",
       "       [ 0.15086542,  0.32657477],\n",
       "       [ 0.18457183, -0.08242249],\n",
       "       [-0.11684895, -0.1065296 ],\n",
       "       [ 0.08964875,  0.38064993],\n",
       "       [ 0.42912051, -0.44579756],\n",
       "       [-0.3183132 , -0.38707663]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5e933609",
   "metadata": {
    "id": "5e933609",
    "outputId": "841469d1-c97a-4264-99a1-f8ba1d84689a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00019519],\n",
       "       [1.00026212],\n",
       "       [0.99994492],\n",
       "       [0.99982964],\n",
       "       [0.99957296],\n",
       "       [0.99943854],\n",
       "       [0.99991901],\n",
       "       [1.00021296],\n",
       "       [1.00019477],\n",
       "       [0.99970365],\n",
       "       [1.00026541],\n",
       "       [0.99992083],\n",
       "       [0.99993923],\n",
       "       [1.00021737],\n",
       "       [1.00005113],\n",
       "       [0.99967193],\n",
       "       [1.00021104],\n",
       "       [1.00024127],\n",
       "       [0.99975044],\n",
       "       [1.00011469],\n",
       "       [1.00015076],\n",
       "       [0.99968914],\n",
       "       [1.00022647],\n",
       "       [1.00027654]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78147176",
   "metadata": {
    "id": "78147176"
   },
   "source": [
    "### Hidden layer 2\n",
    "\n",
    "Moving onto the 2nd or last hidden layer, we can to compute the partial derivative of NLL with respect to $\\Zm^{[2]}$ using $\\frac{\\partial NLL}{\\partial \\Am^{[2]}}$ from the previous layer and $\\frac{\\partial \\Am^{[2]}}{\\partial \\Zm^{[2]}}$ from the current layer.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\Zm^{[2]}} &= \\frac{\\partial NLL}{\\partial \\Am^{[2]}}\\frac{\\partial \\Am^{[2]}}{\\partial \\Zm^{[2]}} \\\\\n",
    "&=   \\frac{\\partial NLL}{\\partial \\Am^{[2]}} * g'(\\Zm^{[2]})\n",
    "\\end{align}\n",
    "$$\n",
    "where $g'(\\Zm^{[2]})$ is the derivative of the activation function for the neurons in last hidden layer. Thus, this is just the derivative of the ReLU activation function.\n",
    "\n",
    "Using $\\frac{\\partial NLL}{\\partial \\Zm^{[2]}}$  we can compute the gradients for the weights $\\Wm^{[2]}$ and biases  $\\bv^{[2]}$ as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\Wm^{[2]}} &= \\frac{\\partial NLL}{\\partial \\Zm^{[2]}} \\cdot \\frac{\\partial \\Zm^{[2]}}{\\partial \\Wm^{[2]}}^\\top \\\\\n",
    "&= \\frac{\\partial NLL}{\\partial \\Zm^{[2]}} \\cdot \\Am^{[1]\\top},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\bv^{[2]}} &= \\frac{\\partial NLL}{\\partial \\Zm^{[2]}} \\cdot  \\frac{\\partial \\Zm^{[2]}}{\\partial \\bv^{[2]}}^\\top \\\\\n",
    "&= \\frac{\\partial NLL}{\\partial \\Zm^{[2]}} \\cdot \\mathbf{1}^\\top.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Finally, we can compute the partial derivative of NLL with respect to the 2nd hidden layer's inputs $\\Am^{[1]}$ by taking the dot product between $\\frac{\\partial NLL}{\\partial \\Zm^{[2]}}$ and $\\frac{\\partial \\Zm^{[2]}}{\\partial \\Am^{[1]}}$ as given:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\Am^{[1]}} &= \\frac{\\partial \\Zm^{[2]}}{\\partial \\Am^{[1]}}^\\top \\cdot \\frac{\\partial NLL}{\\partial \\Zm^{[2]}} \\\\\n",
    "&= \\Wm^{[2]\\top} \\cdot \\frac{\\partial NLL}{\\partial \\Zm^{[2]}}\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\frac{\\partial NLL}{\\partial \\Am^{[1]}}$ will be passed on and utilized by the next layer's gradient equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c587b12f",
   "metadata": {
    "id": "c587b12f"
   },
   "source": [
    "#### TODO 10\n",
    "Complete the TODO by performing the backpropagation step for the 2nd hidden layer. Use the below tables that map equation variables to code variables to help solve this TODO.\n",
    "\n",
    "<font color='red'>**Refer to the notes if you are struggling!**</font>\n",
    "\n",
    "**Setting the current layer**\n",
    "\n",
    "1. Set `layer2` to store the element which corresponds to the 2nd hidden layer within `nn` the list of `Layer` class instances.\n",
    "    1. Hint: Recall, `nn` stores all the `Layer` class instances where the first element corresponds to the 1st hidden layer and the last element corresponds to the output layer.\n",
    "\n",
    "**Computing `delta_Z2` $\\frac{\\partial NLL}{\\partial \\Zm^{[2]}}$**\n",
    "\n",
    "2. Compute $\\frac{\\partial NLL}{\\partial \\Zm^{[2]}}$ the partial derivative of the loss $NLL$ with respect to the linear combinations of weights and inputs for the 2nd hidden layer. Store the output into `delta_Z2`\n",
    "    1. Hint: Use `delta_A2` which we computed in TODO 9\n",
    "    \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\Zm^{[2]}} &= \\frac{\\partial NLL}{\\partial \\Am^{[2]}}\\frac{\\partial \\Am^{[2]}}{\\partial \\Zm^{[2]}} \\\\\n",
    "&=   \\frac{\\partial NLL}{\\partial \\Am^{[2]}} * g'(\\Zm^{[2]})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Computing `delta_W2` $\\frac{\\partial NLL}{\\partial \\Wm^{[2]}}$ and `delta_b2` $\\frac{\\partial NLL}{\\partial \\bv^{[2]}}$**\n",
    "\n",
    "3. Compute $\\frac{\\partial NLL}{\\partial \\Wm^{[2]}}$ the partial derivative of the loss $NLL$ with respect to the 2nd hidden layer weights. Store the output into `delta_W2`.\n",
    "    1. Hint: Use `delta_Z2` which we computed in the previous TODO\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\Wm^{[2]}} &= \\frac{\\partial NLL}{\\partial \\Zm^{[2]}} \\cdot \\frac{\\partial \\Zm^{[2]}}{\\partial \\Wm^{[2]}}^\\top \\\\\n",
    "&= \\frac{\\partial NLL}{\\partial \\Zm^{[2]}} \\cdot \\Am^{[1]\\top},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "4. Compute $\\frac{\\partial NLL}{\\partial \\bv^{[2]}}$ the partial derivative of the loss $NLL$ with respect to the 2nd hidden layer biases. Store the output into `delta_b2`.\n",
    "    1. Hint: Use `delta_Z2` which we computed in the previous TODO\n",
    "    1. Hint: You'll need to define an `np.ones()` array with shape (1, data samples) and transpose it to represent $\\mathbf{1}^\\top$. A second option is that you can just sum over `delta_Z2` and not include the vector of ones or the dot product.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\bv^{[2]}} &= \\frac{\\partial NLL}{\\partial \\Zm^{[2]}} \\cdot  \\frac{\\partial \\Zm^{[2]}}{\\partial \\bv^{[2]}}^\\top \\\\\n",
    "&= \\frac{\\partial NLL}{\\partial \\Zm^{[2]}} \\cdot \\mathbf{1}^\\top.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Computing `delta_A1` $\\frac{\\partial NLL}{\\partial \\Am^{[1]}}$**\n",
    "\n",
    "\n",
    "5. Compute $\\frac{\\partial NLL}{\\partial \\Am^{[1]}}$ the partial derivative of the loss $NLL$ with respect to the 2nd hidden layer's inputs. This partial derivative will be used to by the next layer in the backpropagation process. Store the output into `delta_A1`.\n",
    "    1. Hint: Use `delta_Z2` which we computed in the previous TODO\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\Am^{[1]}}& = \\big ( \\frac{\\partial \\Zm^{[2]}}{\\partial \\Am^{[1]}}  \\big )^\\top \\cdot \\frac{\\partial NLL}{\\partial \\Zm^{[2]}} \\\\\n",
    "&= \\Wm^{[2]\\top} \\cdot \\frac{\\partial NLL}{\\partial \\Zm^{[2]}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2a14d8e2",
   "metadata": {
    "id": "2a14d8e2",
    "outputId": "99aa249e-562c-4d93-bc5d-03a95361884f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 2: Hidden Layer\n",
      "\tNeurons: 2\n",
      "\tInputs: 2\n",
      "\tWeight shape: (2, 2)\n",
      "\tBias shape: (2, 1)\n",
      "\tActivation function: ReLU\n",
      "\tZ shape: (2, 21964)\n",
      "\tA shape: (2, 21964)\n",
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "# TODO 10.1\n",
    "layer2 = nn[1]\n",
    "\n",
    "layer2.print_info()\n",
    "\n",
    "todo_check([\n",
    "    (layer2.name == 'Layer 2: Hidden Layer', \"layer2 has wrong name. Make sure you selected the 2nd hidden layer from nn.\"),\n",
    "    (layer2.neurons == 2, \"layer2 has the wrong number of hidden neurons.\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bae34c74",
   "metadata": {
    "id": "bae34c74",
    "outputId": "69ad0c95-9f66-4f93-decb-754baf56267b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_Z2(2, 21964) = delta_A2(2, 21964) * g'(Z2(2, 21964))\n",
      "delta_Z2 Shape: (2, 21964)\n",
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "print(f\"delta_Z2(2, 21964) = delta_A2(2, 21964) * g'(Z2(2, 21964))\")\n",
    "\n",
    "# TODO 10.2\n",
    "delta_Z2 = delta_A2 * layer2.g().derivative(layer2.Z)\n",
    "print(f\"delta_Z2 Shape: {delta_Z2.shape}\")\n",
    "\n",
    "todo_check([\n",
    "    (delta_Z2.shape == (2, 21964), \"delta_Z2 has incorrect shape\"),\n",
    "    (np.all(np.isclose(delta_Z2[-1, -3:], np.array([0.5756628, 0.53552323, 0.15871728]))), \"delta_Z2 has incorrect values\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e2ac570b",
   "metadata": {
    "id": "e2ac570b",
    "outputId": "468b26c8-1a98-4ecd-d562-33f795704d73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_W2(2, 2) = delta_Z2(2, 21964) @ A1(2, 21964)^T\n",
      "delta_W2 Shape: (2, 2)\n",
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "print(f\"delta_W2(2, 2) = delta_Z2(2, 21964) @ A1(2, 21964)^T\")\n",
    "\n",
    "# TODO 10.3\n",
    "delta_W2 = delta_Z2 @ nn[0].A.T\n",
    "print(f\"delta_W2 Shape: {delta_W2.shape}\")\n",
    "\n",
    "todo_check([\n",
    "    (delta_W2.shape == (2, 2),  \"delta_W2 has incorrect shape\"),\n",
    "    (np.all(np.isclose(delta_W2[0], np.array([-12.65654051, 166.15485242]), rtol=.01)), \"delta_W2 has incorrect values\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bf2420",
   "metadata": {
    "id": "e9bf2420"
   },
   "source": [
    "Below we compute the average weight gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9881eace",
   "metadata": {
    "id": "9881eace",
    "outputId": "45e41665-59b8-46cb-d6e2-430e2df03d08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2 average gradient shape: (2, 2)\n"
     ]
    }
   ],
   "source": [
    "W2_avg_grad = delta_W2 / len(y_trn)\n",
    "print(f\"W2 average gradient shape: {W2_avg_grad.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "13e30061",
   "metadata": {
    "id": "13e30061",
    "outputId": "634a15e4-e375-4249-9400-8ffa6d7d5920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_b2(2, 1) = delta_Z2(2, 21964) @ 1(1, 21964)^T\n",
      "delta_b2 Shape: (2, 1)\n",
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "print(f\"delta_b2(2, 1) = delta_Z2(2, 21964) @ 1(1, 21964)^T\")\n",
    "\n",
    "# TODO 10.4\n",
    "delta_b2 = delta_Z2 @ np.ones((delta_Z2.shape[1], 1))\n",
    "print(f\"delta_b2 Shape: {delta_b2.shape}\")\n",
    "\n",
    "todo_check([\n",
    "    (delta_b2.shape == (2, 1), \"delta_b2 has incorrect shape\"),\n",
    "    (np.all(np.isclose(delta_b2[:3], np.array([[426.635],  [4040.90]]), rtol=.01)), \"delta_b2 has incorrect values\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5082312d",
   "metadata": {
    "id": "5082312d"
   },
   "source": [
    "Below we compute the average bias gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9807c46c",
   "metadata": {
    "id": "9807c46c",
    "outputId": "5ee7ed09-4f28-46da-a292-2be0e47b4771"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b2 average gradient shape: (2, 1)\n"
     ]
    }
   ],
   "source": [
    "b2_avg_grad = delta_b2 / len(y_trn)\n",
    "print(f\"b2 average gradient shape: {b2_avg_grad.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e8c002a9",
   "metadata": {
    "id": "e8c002a9",
    "outputId": "42a60ee2-86a1-45a3-f346-5946c7d14ab7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_A2(2, 21964) = W2(2, 2)^T @ delta_Z2(2, 21964)\n",
      "delta_A1 Shape: (2, 21964)\n",
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "print(f\"delta_A2(2, 21964) = W2(2, 2)^T @ delta_Z2(2, 21964)\")\n",
    "# TODO 10.5\n",
    "delta_A1 = layer2.W.T @ delta_Z2\n",
    "print(f\"delta_A1 Shape: {delta_A1.shape}\")\n",
    "\n",
    "todo_check([\n",
    "    (delta_A1.shape == (2, 21964), \"delta_A1 has incorrect shape\"),\n",
    "    (np.all(np.isclose(delta_A1[0, :3], np.array([-0.00849924,  0.0338268 , -0.02477116]), rtol=.01)), \"delta_A1 has incorrect values\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba07d0ef",
   "metadata": {
    "id": "ba07d0ef"
   },
   "source": [
    "Using the gradients we can update the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "08a4dff1",
   "metadata": {
    "id": "08a4dff1"
   },
   "outputs": [],
   "source": [
    "alpha = .01\n",
    "\n",
    "layer2_W = layer2.W - alpha * W2_avg_grad\n",
    "layer2_b = layer2.b - alpha * b2_avg_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "beb0dcde",
   "metadata": {
    "id": "beb0dcde",
    "outputId": "c2f60858-8042-42f6-cd07-9c6d2715f5fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1420103 , -0.06493366],\n",
       "       [ 0.08560394,  0.21142467]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer2_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "83d40065",
   "metadata": {
    "id": "83d40065",
    "outputId": "02bb9590-b043-4093-fabc-b3fd6caa1cdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99980576],\n",
       "       [0.99816022]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer2_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f726423",
   "metadata": {
    "id": "1f726423"
   },
   "source": [
    "### Hidden layer 1\n",
    "\n",
    "\n",
    "Moving onto with the 1st hidden layer, we can to compute the partial derivative of NLL with respect to $\\Zm^{[1]}$ using $\\frac{\\partial NLL}{\\partial \\Am^{[1]}}$ from the previous layer and $\\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]}}$ from the current layer.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\Zm^{[1]}} &= \\frac{\\partial NLL}{\\partial \\Am^{[1]}}\\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]}} \\\\\n",
    "&=   \\frac{\\partial NLL}{\\partial \\Am^{[1]}} * g'(\\Zm^{[1]})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $g'(\\Zm^{[1]})$ is the derivative of the activation function for 1st hidden layer's neurons which is the ReLU activation function.\n",
    "\n",
    "Using $\\frac{\\partial NLL}{\\partial \\Zm^{[1]}}$  we can compute the gradients for the weights $\\Wm^{[1]}$ and biases  $\\bv^{[1]}$ as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\Wm^{[1]}} &=  \\frac{\\partial NLL}{\\partial \\Zm^{[1]}} \\cdot \\frac{\\partial \\Zm^{[1]}}{\\partial \\Wm^{[1]}}^\\top \\\\\n",
    "&= \\frac{\\partial NLL}{\\partial \\Zm^{[1]}} \\cdot \\Am^{[0]\\top},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\bv^{[1]}} &=   \\frac{\\partial NLL}{\\partial \\Zm^{[1]}}  \\cdot  \\frac{\\partial \\Zm^{[1]}}{\\partial \\bv^{[1]}} \\\\\n",
    "&= \\frac{\\partial NLL}{\\partial \\Zm^{[1]}} \\cdot \\mathbf{1}^\\top.\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\Am^{[0]} = \\Xm$.\n",
    "\n",
    "Since we have reached the first hidden layer, we no longer need to compute the partial derivative with respect to the current layer's input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ed12f0",
   "metadata": {
    "id": "63ed12f0"
   },
   "source": [
    "#### TODO 11\n",
    "Complete the TODO by performing the backpropagation step for the 1st hidden layer. Use the below tables that map equation variables to code variables to help solve this TODO.\n",
    "\n",
    "<font color='red'>**Refer to the notes if you are struggling!**</font>\n",
    "\n",
    "\n",
    "**Setting the current layer**\n",
    "\n",
    "1. Set `layer1` to store the element which corresponds to the 1st hidden layer within `nn` the list of `Layer` class instances.\n",
    "    1. Hint: Recall, `nn` stores all the `Layer` class instances where the first element corresponds to the 1st hidden layer and the last element corresponds to the 1st hidden layer.\n",
    "\n",
    "**Computing `delta_Z1` $\\frac{\\partial NLL}{\\partial \\Zm^{[1]}}$**\n",
    "\n",
    "2. Compute $\\frac{\\partial NLL}{\\partial \\Zm^{[1]}}$ the partial derivative of the loss $NLL$ with respect to the linear combinations of weights and inputs for the 1st hidden layer. Store the output into `delta_Z1`\n",
    "    1. Hint: Use `delta_A1` which we computed in TODO 10.\n",
    "    \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\Zm^{[1]}} &= \\frac{\\partial NLL}{\\partial \\Am^{[1]}}\\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]}} \\\\\n",
    "&=   \\frac{\\partial NLL}{\\partial \\Am^{[1]}} * g'(\\Zm^{[1]})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Computing `delta_W1` $\\frac{\\partial NLL}{\\partial \\Wm^{[1]}}$ and `delta_b1` $\\frac{\\partial NLL}{\\partial \\bv^{[1]}}$**\n",
    "\n",
    "3. Compute $\\frac{\\partial NLL}{\\partial \\Wm^{[1]}}$ the partial derivative of the loss $NLL$ with respect to the 1st hidden layer weights. Store the output into `delta_W1`.\n",
    "    1. Hint: Use `delta_Z1` which we computed in the previous TODO\n",
    "    1. Hint: Try using `X_trn` with shape (data samples, features) or `A0` with shape (features, data samples) where `A0 = X_trn.T`.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\Wm^{[1]}} &=  \\frac{\\partial NLL}{\\partial \\Zm^{[1]}} \\cdot \\frac{\\partial \\Zm^{[1]}}{\\partial \\Wm^{[1]}}^\\top \\\\\n",
    "&= \\frac{\\partial NLL}{\\partial \\Zm^{[1]}} \\cdot \\Am^{[0]\\top},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "4. Compute $\\frac{\\partial NLL}{\\partial \\bv^{[1]}}$the partial derivative of the loss $NLL$ with respect to the 1st hidden layer biases. Store the output into `delta_b1`.\n",
    "    1. Hint: Use `delta_Z1` which we computed in the previous TODO\n",
    "    1. Hint: You'll need to define an `np.ones()` array with shape (1, data samples) and transpose it to represent $\\mathbf{1}^\\top$. A second option is that you can just sum over `delta_Z1` and not include the vector of ones or the dot product.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\bv^{[1]}} &=   \\frac{\\partial NLL}{\\partial \\Zm^{[1]}}  \\cdot  \\frac{\\partial \\Zm^{[1]}}{\\partial \\bv^{[1]}} \\\\\n",
    "&= \\frac{\\partial NLL}{\\partial \\Zm^{[1]}} \\cdot \\mathbf{1}^\\top.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0cababa0",
   "metadata": {
    "id": "0cababa0",
    "outputId": "373e88d0-e499-4354-ea02-d32fb1120f46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Hidden Layer\n",
      "\tNeurons: 2\n",
      "\tInputs: 784\n",
      "\tWeight shape: (2, 784)\n",
      "\tBias shape: (2, 1)\n",
      "\tActivation function: ReLU\n",
      "\tZ shape: (2, 21964)\n",
      "\tA shape: (2, 21964)\n",
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "# TODO 11.1\n",
    "layer1 = nn[0]\n",
    "\n",
    "layer1.print_info()\n",
    "\n",
    "todo_check([\n",
    "    (layer1.name == 'Layer 1: Hidden Layer', \"layer1 has wrong name. Make sure you selected the 1st hidden layer from nn.\"),\n",
    "    (layer1.neurons == 2, \"layer1 has the wrong number of hidden neurons.\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "45d888d3",
   "metadata": {
    "id": "45d888d3",
    "outputId": "e9a76a99-b204-46d2-9534-1188a394a775"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_Z1(2, 21964) = delta_A1(2, 21964) * g'(Z1(2, 21964))\n",
      "delta_Z1 Shape: (2, 21964)\n",
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "print(f\"delta_Z1(2, 21964) = delta_A1(2, 21964) * g'(Z1(2, 21964))\")\n",
    "\n",
    "# TODO 11.2\n",
    "delta_Z1 = delta_A1 * layer1.g().derivative(layer1.Z)\n",
    "print(f\"delta_Z1 Shape: {delta_Z1.shape}\")\n",
    "\n",
    "todo_check([\n",
    "    (delta_Z1.shape == (2, 21964), \"delta_Z1 has incorrect shape\"),\n",
    "    (np.all(np.isclose(delta_Z1[0, :3], np.array([-0.00849924,  0.0338268 , -0.]), rtol=.01)), \"delta_Z1 has incorrect values\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e4c29d80",
   "metadata": {
    "id": "e4c29d80",
    "outputId": "41b6c114-7e74-41c1-b7df-12d8ae23f60b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_W1(2, 784) = delta_Z1(2, 21964) @ A0(784, 21964)^T\n",
      "delta_W1 Shape: (2, 784)\n",
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "print(f\"delta_W1(2, 784) = delta_Z1(2, 21964) @ A0(784, 21964)^T\")\n",
    "\n",
    "A0 = X_trn.T\n",
    "\n",
    "# TODO 11.3\n",
    "delta_W1 = delta_Z1 @ A0.T\n",
    "print(f\"delta_W1 Shape: {delta_W1.shape}\")\n",
    "\n",
    "todo_check([\n",
    "    (delta_W1.shape == (2, 784), \"delta_W1 has incorrect shape\"),\n",
    "    (np.all(np.isclose(delta_W1[0, :3], np.array([11.35838323,  5.78225958,  5.29613148]), rtol=.01)), \"delta_Z1 has incorrect values\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decc1a10",
   "metadata": {
    "id": "decc1a10"
   },
   "source": [
    "Below we compute the average weight gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "46c79218",
   "metadata": {
    "id": "46c79218",
    "outputId": "78fa2e32-2f94-4fd7-fc5b-3bd043415995"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 average gradient shape: (2, 784)\n"
     ]
    }
   ],
   "source": [
    "W1_avg_grad = delta_W1 / len(y_trn)\n",
    "print(f\"W1 average gradient shape: {W1_avg_grad.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f5870000",
   "metadata": {
    "id": "f5870000",
    "outputId": "69bedb00-6b8c-4baa-ac3f-fcb251e5eb6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_b1(2, 1) = delta_Z1(2, 21964) @ 1(1, 21964)^T\n",
      "delta_b1 Shape: (2, 1)\n",
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "print(f\"delta_b1(2, 1) = delta_Z1(2, 21964) @ 1(1, 21964)^T\")\n",
    "\n",
    "# TODO 11.4\n",
    "delta_b1 = delta_Z1 @ np.ones((delta_Z1.shape[1], 1))\n",
    "print(f\"delta_b1 Shape: {delta_b1.shape}\")\n",
    "\n",
    "todo_check([\n",
    "    (delta_b1.shape == (2, 1), \"delta_b1 has incorrect shape\"),\n",
    "    (np.all(np.isclose(delta_b1, np.array([[212.659], [687.138]]), rtol=.01)), \"deltab1 has incorrect values\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5b264a",
   "metadata": {
    "id": "3d5b264a"
   },
   "source": [
    "Below we compute the average bias gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a85144e1",
   "metadata": {
    "id": "a85144e1",
    "outputId": "ab42d936-ec70-4361-f5c3-184a49d1da2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1 average gradient shape: (2, 1)\n"
     ]
    }
   ],
   "source": [
    "b1_avg_grad = delta_b1 / len(y_trn)\n",
    "print(f\"b1 average gradient shape: {b1_avg_grad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2469c5",
   "metadata": {
    "id": "6b2469c5"
   },
   "source": [
    "Using the gradients we can update the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bae32d28",
   "metadata": {
    "id": "bae32d28"
   },
   "outputs": [],
   "source": [
    "alpha = .01\n",
    "\n",
    "layer1_W = layer1.W - alpha * W1_avg_grad\n",
    "layer1_b = layer1.b - alpha * b1_avg_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "39683987",
   "metadata": {
    "id": "39683987",
    "outputId": "314efd61-5bd5-4a6d-eed8-1a51eb0cff71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04880833,  0.21518673,  0.10276096, ..., -0.07310531,\n",
       "         0.34284715,  0.31802343],\n",
       "       [-0.39767211, -0.34370353, -0.19588569, ..., -0.16001108,\n",
       "        -0.4315027 , -0.2711931 ]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1825efef",
   "metadata": {
    "id": "1825efef",
    "outputId": "a7d78255-db44-4af9-9f51-2b6a8ea75b15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99990318],\n",
       "       [0.99968715]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ede8fdc",
   "metadata": {
    "id": "5ede8fdc"
   },
   "source": [
    "### General backward pass\n",
    "\n",
    "Now it's time to write the `backward()` which will generalize the previous backpropagation code by utilizing a for-loop. The main idea is use a for-loop to iterate over the layers in the neural network in the reverse order (output layer to 1st hidden layer).\n",
    "\n",
    "The `backward()` function can be summarized as follows where it will be helpful to about interpret $\\text{layer}$  in $\\Am{^{[\\text{layer}]}}$/$\\Zm{^{[\\text{layer}]}}$/$\\Wm{^{[\\text{layer}]}}$ as the current layer that is being updated in the backpropagation process:\n",
    "\n",
    "1. We first compute the partial derivative of the loss function $\\frac{\\partial NLL}{\\partial \\Am^{[\\text{layer}]}}$. The output is stored into `delta_A`. Once the loop starts, `delta_A` will be updated to store the value of $\\frac{\\partial NLL}{\\partial \\Am^{[\\text{layer}-1]}}$ as this is the variable that will be updated, passed on, and used to compute the weight and bias gradients for the next layer in the backpropagation process (i.e., the next loop).\n",
    "\n",
    "1. Next, the for-loop begins storing the inputs for the current layer into `A`.\n",
    "\n",
    "1. The partial derivative $\\frac{\\partial NLL}{\\partial \\Zm^{[\\text{layer}]}}$ is the computed as it is required for coming computations.\n",
    "\n",
    "1. The gradients and average gradients for the weights $\\frac{\\partial NLL}{\\partial \\Wm^{[\\text{layer}]}}$ biases $\\frac{\\partial NLL}{\\partial \\bv^{[\\text{layer}]}}$ are computed.\n",
    "\n",
    "1. The partial derivative $\\frac{\\partial NLL}{\\partial \\Am^{[\\text{layer}-1]}}$ is computed and stored in  `delta_A`. `delta_A` will be passed onto the next layer in the backpropgation process (i.e., the next for-loop iteration) and used when computing the next iteration's gradient equations.\n",
    "\n",
    "1. The values of the weights and biases are updated using the gradient descent update equation. This is only done once  $\\frac{\\partial NLL}{\\partial \\Am^{[\\text{layer}-1]}}$ has been computed.\n",
    "\n",
    "1. Repeat starting from step 2 until the loop updates ALL parameters in the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcc98e3",
   "metadata": {
    "id": "cfcc98e3"
   },
   "source": [
    "With this in mind, the `backward()` function will compute the solutions to following equations EVERY iteration of the for-loop:\n",
    "\n",
    "**Partial derivatives**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\Zm^{[\\text{layer}]}} &= \\frac{\\partial NLL}{\\partial \\Am^{[\\text{layer}]}} * \\frac{\\partial \\Am^{[\\text{layer}]}}{\\partial \\Zm^{[\\text{layer}]}} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\Wm^{[\\text{layer}]}} &= \\frac{\\partial NLL}{\\partial \\Zm^{[\\text{layer}]}} \\cdot \\frac{\\partial \\Zm^{[\\text{layer}]}}{\\partial \\Wm^{[\\text{layer}]}}^\\top \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\bv^{[\\text{layer}]}} &= \\frac{\\partial NLL}{\\partial \\Zm^{[\\text{layer}]}} \\cdot \\frac{\\partial \\Zm^{[\\text{layer}]}}{\\partial \\bv^{[\\text{layer}]}}^\\top \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\Am^{[\\text{layer-1}]}}& =\\frac{\\partial \\Zm^{[\\text{layer}]}}{\\partial \\Am^{[\\text{layer-1}]}} ^\\top \\cdot \\frac{\\partial NLL}{\\partial \\Zm^{[\\text{layer}]}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Parameter updates**\n",
    "$$\n",
    "\\Wm^{[\\text{layer}]} = \\Wm^{[\\text{layer}]} - \\alpha * \\frac{\\partial NLL}{\\partial \\Wm^{[\\text{layer}]}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bv^{[\\text{layer}]} = \\bv^{[\\text{layer}]} - \\alpha * \\frac{\\partial NLL}{\\partial \\bv^{[\\text{layer}]}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cae8dc",
   "metadata": {
    "id": "06cae8dc"
   },
   "source": [
    "#### TODO 12\n",
    "Using the previous TODOs as a guide, complete the TODO by finishing the `backward()` function implementation. The following is how each code variable in the `backward()` function will map to the variables in the equations:\n",
    "- `layer` corresponds to $\\text{layer}$\n",
    "- `layer.W` corresponds to $\\Wm^{[\\text{layer}]}$\n",
    "- `layer.b` corresponds to $\\bv^{[\\text{layer}]}$\n",
    "- `A` corresponds to $\\Am^{[\\text{layer}-1]}$ and represents the current layer's input (i.e., the previous layer's output)\n",
    "- `layer.Z` corresponds to $\\Zm^{[\\text{layer}]} $\n",
    "- `layer.g.derivative(layer.Z)` corresponds to $g'(\\Zm^{[\\text{layer}]})$\n",
    "- `layer.A` corresponds to $\\Am^{[\\text{layer}]} $ the output of the current layer\n",
    "\n",
    "**Before the loop**\n",
    "\n",
    "1. Initialize `delta_A` by storing the solution to the partial derivative $\\frac{\\partial NLL}{\\partial \\Am^{[\\text{output layer}]}}^\\top$ of loss $NLL$ with respect to the neural network output where $\\hat{\\Ym} = \\Am^{[\\text{output layer}]\\top}$.\n",
    "    1. Hint: Be sure to transpose the solution to the partial derivative $\\frac{\\partial NLL}{\\partial \\Am^{[\\text{output layer}]}}$ as neural networks require the shape (classes, data samples).\n",
    "\n",
    "**During the loop**\n",
    "\n",
    "2. Compute $\\frac{\\partial NLL}{\\partial \\Zm^{[\\text{layer}]}}$ the partial derivative of loss $NLL$ with respect to the linear combinations of weights and inputs for the output layer. Store the output into `delta_Z`\n",
    "    \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\Zm^{[\\text{layer}]}} &= \\frac{\\partial NLL}{\\partial \\Am^{[\\text{layer}]}} * \\frac{\\partial \\Am^{[\\text{layer}]}}{\\partial \\Zm^{[\\text{layer}]}} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "3. Compute $\\frac{\\partial NLL}{\\partial \\Wm^{[\\text{layer}]}}$ the partial derivative of loss $NLL$ with respect to the current layer's weights. Store the output into `delta_W`.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\Wm^{[\\text{layer}]}} &= \\frac{\\partial NLL}{\\partial \\Zm^{[\\text{layer}]}} \\cdot \\frac{\\partial \\Zm^{[\\text{layer}]}}{\\partial \\Wm^{[\\text{layer}]}}^\\top \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "4.  Compute $\\frac{\\partial NLL}{\\partial \\bv^{[\\text{layer}]}}$ the partial derivative of loss $NLL$ with respect to the current layer's biases. Store the output into `delta_b`.\n",
    "    1. Hint: You'll need to define an `np.ones()` array with shape (1, data samples) and transpose it to represent $\\mathbf{1}^\\top$. A second option is that you can just sum over `delta_Z` and not include the vector of ones or the dot product.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\bv^{[\\text{layer}]}} &= \\frac{\\partial NLL}{\\partial \\Zm^{[\\text{layer}]}} \\cdot \\frac{\\partial \\Zm^{[\\text{layer}]}}{\\partial \\bv^{[\\text{layer}]}}^\\top\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "5. Update the value of `delta_A` by computing $\\frac{\\partial NLL}{\\partial \\Am^{[\\text{layer-1}]}}$ the partial derivative of loss $NLL$ with respect to the current layer's inputs. This partial derivative will be used to by the next layer in the backpropagation process.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial NLL}{\\partial \\Am^{[\\text{layer-1}]}}& = \\frac{\\partial \\Zm^{[\\text{layer}]}}{\\partial \\Am^{[\\text{layer-1}]}} ^\\top \\cdot \\frac{\\partial NLL}{\\partial \\Zm^{[\\text{layer}]}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "6. Update the current layer's weights using the gradient descent update equation. Recall, the average weight gradient is stored in `W_avg_grad`. Store the output back into `layer.W`.\n",
    "\n",
    "$$\n",
    "\\Wm^{[\\text{layer}]} = \\Wm^{[\\text{layer}]} - \\alpha * \\frac{\\partial NLL}{\\partial \\Wm^{[\\text{layer}]}}\n",
    "$$\n",
    "\n",
    "7. Update the current layer's biases using the gradient descent update equation. Recall, the average bias gradient stored in `b_avg_grad`. Store the output back into `layer.b`.\n",
    "\n",
    "$$\n",
    "\\bv^{[\\text{layer}]} = \\bv^{[\\text{layer}]} - \\alpha * \\frac{\\partial NLL}{\\partial \\bv^{[\\text{layer}]}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9efd44f7",
   "metadata": {
    "id": "9efd44f7"
   },
   "outputs": [],
   "source": [
    "def backward(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    y_hat_probs: np.ndarray,\n",
    "    nn: List[Layer],\n",
    "    alpha: float,\n",
    "    verbose: bool = False\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\" Performs the feedback process for a multi-layer neural network\n",
    "\n",
    "        Args:\n",
    "            X: Input features\n",
    "\n",
    "            y: Input labels\n",
    "\n",
    "            y_hat_probs: Input probability predictions\n",
    "\n",
    "            nn: List of Layer class instances which together\n",
    "                define a nueral network.\n",
    "\n",
    "            alpha: Learning rate\n",
    "\n",
    "            verbose: If True then information regarding eqation shapes\n",
    "                will be printed for each layer.\n",
    "    \"\"\"\n",
    "    W_grads = []  # Non-essential to the algorithm\n",
    "    b_grads = []  # Non-essential to the algorithm\n",
    "\n",
    "    # Get index of nn and reverse the order\n",
    "    # to start with output layer.\n",
    "    layer_index = np.arange(len(nn))[::-1]\n",
    "\n",
    "    # TODO 12.1\n",
    "    delta_A = (y_hat_probs - y).T\n",
    "\n",
    "    for l, layer in zip(layer_index, nn[::-1]):\n",
    "        # Get inputs from previous layer or grab\n",
    "        # feature inputs for 1st hidden layer.\n",
    "        if l == 0:\n",
    "            A = X.T\n",
    "        else:\n",
    "            prev_layer = nn[l-1]\n",
    "            A = prev_layer.A\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{layer.name}\")\n",
    "            print(f\"\\tInputs: A{l}{A.shape}\")\n",
    "\n",
    "        # TODO 12.2\n",
    "        delta_Z = delta_A * layer.g().derivative(layer.Z)\n",
    "        if verbose: print(f\"\\tdelta_Z{l+1}{delta_Z.shape} = delta_A{l+1}{delta_A.shape} * g'(Z{l+1}{layer.Z.shape})\")\n",
    "\n",
    "        # TODO 12.3\n",
    "        delta_W = delta_Z @ A.T\n",
    "        if verbose: print(f\"\\tdelta_W{l+1}{delta_W.shape} = delta_Z{l+1}{delta_Z.shape} @ A{l}^T{A.T.shape}\")\n",
    "\n",
    "        W_avg_grad = delta_W / len(y)\n",
    "\n",
    "        # Track layer weight gradients, non-essential to the algorithm\n",
    "        W_grads.append(delta_W)\n",
    "        if verbose: print(f\"\\tW{l+1} average gradient shape: {W_avg_grad.shape}\")\n",
    "\n",
    "        # TODO 12.4\n",
    "        delta_b = np.sum(delta_Z, axis=1, keepdims=True)\n",
    "        if verbose: print(f\"\\tdelta_b{l+1}{delta_b.shape} = sum(delta_Z{l+1}{delta_Z.shape})\")\n",
    "\n",
    "        b_avg_grad = delta_b / len(y) # np.mean(delta_b, axis=1, keepdims=True)\n",
    "        if verbose: print(f\"\\tb{l+1} average gradient shape: {b_avg_grad.shape}\")\n",
    "\n",
    "        # Track layer bias gradients, non-essential to the algorithm\n",
    "        b_grads.append(delta_b)#np.sum(delta_b, axis=1, keepdims=True))\n",
    "\n",
    "        #TODO 12.5\n",
    "        delta_A = layer.W.T @ delta_Z\n",
    "        if verbose: print(f\"\\tdelta_A{l}{delta_A.shape} = W{l+1}^T{layer.W.T.shape} @ Z{l+1}{delta_Z.shape} \")\n",
    "\n",
    "        # TODO 12.6\n",
    "        layer.W = layer.W - alpha * W_avg_grad\n",
    "\n",
    "        # TODO 12.7\n",
    "        layer.b = layer.b - alpha * b_avg_grad\n",
    "\n",
    "    # Reverse the order of the gradients\n",
    "    # so the first element corresponds\n",
    "    # to the first layer's gradients.\n",
    "    # Non-essential to the algorithm.\n",
    "    W_grads = W_grads[::-1]\n",
    "    b_grads = b_grads[::-1]\n",
    "\n",
    "    return W_grads, b_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbed81da",
   "metadata": {
    "id": "bbed81da"
   },
   "source": [
    "Run the below code to test your implementation of the `backward()` function.\n",
    "\n",
    "Notice, we added some useful print statements which print the shapes of all the variables and their corresponding variable name and layer number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6ce44963",
   "metadata": {
    "id": "6ce44963",
    "outputId": "24bfb481-0ee6-47b5-e3d0-e203ee4165e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 3: Output Layer\n",
      "\tInputs: A2(2, 21964)\n",
      "\tdelta_Z3(24, 21964) = delta_A3(24, 21964) * g'(Z3(24, 21964))\n",
      "\tdelta_W3(24, 2) = delta_Z3(24, 21964) @ A2^T(21964, 2)\n",
      "\tW3 average gradient shape: (24, 2)\n",
      "\tdelta_b3(24, 1) = sum(delta_Z3(24, 21964))\n",
      "\tb3 average gradient shape: (24, 1)\n",
      "\tdelta_A2(2, 21964) = W3^T(2, 24) @ Z3(24, 21964) \n",
      "Layer 2: Hidden Layer\n",
      "\tInputs: A1(2, 21964)\n",
      "\tdelta_Z2(2, 21964) = delta_A2(2, 21964) * g'(Z2(2, 21964))\n",
      "\tdelta_W2(2, 2) = delta_Z2(2, 21964) @ A1^T(21964, 2)\n",
      "\tW2 average gradient shape: (2, 2)\n",
      "\tdelta_b2(2, 1) = sum(delta_Z2(2, 21964))\n",
      "\tb2 average gradient shape: (2, 1)\n",
      "\tdelta_A1(2, 21964) = W2^T(2, 2) @ Z2(2, 21964) \n",
      "Layer 1: Hidden Layer\n",
      "\tInputs: A0(784, 21964)\n",
      "\tdelta_Z1(2, 21964) = delta_A1(2, 21964) * g'(Z1(2, 21964))\n",
      "\tdelta_W1(2, 784) = delta_Z1(2, 21964) @ A0^T(21964, 784)\n",
      "\tW1 average gradient shape: (2, 784)\n",
      "\tdelta_b1(2, 1) = sum(delta_Z1(2, 21964))\n",
      "\tb1 average gradient shape: (2, 1)\n",
      "\tdelta_A0(784, 21964) = W1^T(784, 2) @ Z1(2, 21964) \n",
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "neurons_per_layer = [2 ,2, 24]\n",
    "g_hidden = ReLU\n",
    "g_output = Softmax\n",
    "\n",
    "nn = init_neural_network(\n",
    "    n_input_features=X_trn.shape[1],\n",
    "    neurons_per_layer=neurons_per_layer,\n",
    "    g_hidden=g_hidden,\n",
    "    g_output=g_output\n",
    ")\n",
    "\n",
    "# Perform forward pass to get predictions\n",
    "y_hat_probs_trn = forward(X_trn, nn, verbose=False)\n",
    "\n",
    "# Compute backwards pass to update weights/biases\n",
    "alpha = .01\n",
    "W_grads, b_grads = backward(\n",
    "    X=X_trn,\n",
    "    y=y_trn,\n",
    "    y_hat_probs=y_hat_probs_trn,\n",
    "    nn=nn,\n",
    "    alpha=alpha,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "todo_check([\n",
    "    (np.all(np.isclose(W_grads[0][0, :3], np.array([11.35838323,  5.78225958,  5.29613148]),rtol=.01)), \"W_grads have incorrect values\"),\n",
    "    (np.all(np.isclose(b_grads[0][0], np.array([212.65913781]),rtol=.01)), \"b_grads have incorrect values\"),\n",
    "    (np.all(np.isclose(nn[1].W[0, :3], np.array([-0.1420103 , -0.06493366]),rtol=.01)), \"1st hidden layer weight values are incorrect\"),\n",
    "    (np.all(np.isclose(nn[1].b, np.array([[0.99980576], [0.99816022]]),rtol=.01)), \"1st hidden layer bais values are incorrect\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450922c3",
   "metadata": {
    "id": "450922c3"
   },
   "source": [
    "Below we can see the **sum** of gradients for the weights that were stored in `W_grads`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "89f76b91",
   "metadata": {
    "id": "89f76b91",
    "outputId": "1486bde1-f6d5-4702-d357-eb7f5a10913e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 11.35838323,   5.78225958,   5.29613148, ...,  21.24561643,\n",
       "          16.98731453,  21.69524497],\n",
       "        [188.59425776, 190.82670484, 185.34574151, ..., 245.62349138,\n",
       "         218.61473253, 221.16992465]]),\n",
       " array([[  -12.65654051,   166.15485242],\n",
       "        [11690.96344676, 24087.58367676]]),\n",
       " array([[   37.32041819, -1896.35692842],\n",
       "        [ -330.15201929, -1117.15152712],\n",
       "        [   25.23538659,   387.93849967],\n",
       "        [   73.13686428,  1102.58115151],\n",
       "        [  597.20997353,  1505.90345203],\n",
       "        [  539.85524678,  2865.11967771],\n",
       "        [   -9.20001449,   811.6685355 ],\n",
       "        [ -422.78134202,  -661.39161195],\n",
       "        [   31.85761809, -1707.59125401],\n",
       "        [  341.28076343,  1382.54749058],\n",
       "        [ -394.65115585, -1056.17155159],\n",
       "        [   24.85736073,   510.62135435],\n",
       "        [  146.02085117,   -21.5852698 ],\n",
       "        [   15.84355583, -1257.99566468],\n",
       "        [   12.19197666,   -52.24227094],\n",
       "        [   10.85145073,  2821.14507835],\n",
       "        [ -234.67139154, -1160.31957595],\n",
       "        [ -101.63131399, -1779.29200846],\n",
       "        [  106.70195666,  1621.95510553],\n",
       "        [ -160.96759589,  -536.72409068],\n",
       "        [ -186.01076093,  -764.31599507],\n",
       "        [  138.53095683,  2014.84074395],\n",
       "        [ -119.37908018, -1477.77299999],\n",
       "        [ -141.44970533, -1535.41034051]])]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e2e6f5",
   "metadata": {
    "id": "57e2e6f5"
   },
   "source": [
    "Here the 1st element corresponds to the weight gradients for the 1st hidden layer. This means that the last element corresponds to the gradients for the output layer weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fbf51cda",
   "metadata": {
    "id": "fbf51cda",
    "outputId": "0dc3b1aa-224f-4e2d-9b1c-1614ace2a64d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11.35838323,   5.78225958,   5.29613148, ...,  21.24561643,\n",
       "         16.98731453,  21.69524497],\n",
       "       [188.59425776, 190.82670484, 185.34574151, ..., 245.62349138,\n",
       "        218.61473253, 221.16992465]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st layer weight gradients\n",
    "W_grads[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b7e0a",
   "metadata": {
    "id": "277b7e0a"
   },
   "source": [
    "Below we can see the  **sum** of gradients for the biases that were stored in `b_grads`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "21806a6b",
   "metadata": {
    "id": "21806a6b",
    "outputId": "d61f56c3-67cb-4f78-a8dc-94d9aad16e1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[212.65913781],\n",
       "        [687.13851994]]),\n",
       " array([[ 426.63513844],\n",
       "        [4040.90226638]]),\n",
       " array([[-428.71478013],\n",
       "        [-575.71967579],\n",
       "        [ 120.96792661],\n",
       "        [ 374.1748449 ],\n",
       "        [ 937.95774496],\n",
       "        [1233.19968125],\n",
       "        [ 177.88025006],\n",
       "        [-467.74436456],\n",
       "        [-427.78299194],\n",
       "        [ 650.90195609],\n",
       "        [-582.93556338],\n",
       "        [ 173.8902293 ],\n",
       "        [ 133.47775314],\n",
       "        [-477.42243943],\n",
       "        [-112.31219657],\n",
       "        [ 720.56609029],\n",
       "        [-463.53178292],\n",
       "        [-529.92721326],\n",
       "        [ 548.14226517],\n",
       "        [-251.89887324],\n",
       "        [-331.12893032],\n",
       "        [ 682.77017051],\n",
       "        [-497.41271862],\n",
       "        [-607.39738213]])]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0f855f",
   "metadata": {
    "id": "fb0f855f"
   },
   "source": [
    "Here the 1st element corresponds to the bias gradients for the 1st hidden layer. This means that the last element corresponds to the gradients for the output layer biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "10aad706",
   "metadata": {
    "id": "10aad706",
    "outputId": "c64c675d-0ab0-4000-bed8-a709b957e189"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[212.65913781],\n",
       "       [687.13851994]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st layer bias gradients\n",
    "b_grads[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b39482",
   "metadata": {
    "id": "51b39482"
   },
   "source": [
    "## Putting it all together\n",
    "\n",
    "With the `forwards()` and `backward()` functions compete, we can begin to put the feed-forward and feedback processes together to learn the optimal parameters for our neural network over many epochs. To do so, we'll use mini-batch gradient descent like we have done in the previous modules.\n",
    "\n",
    "Below we redefine the `get_mini_batches()` function. Further, we redefine `sum_gradients()` which helps us track the the absolute sum of the weight and bias gradients for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "58999ae7",
   "metadata": {
    "id": "58999ae7"
   },
   "outputs": [],
   "source": [
    "def get_mini_batches(\n",
    "    data_len: int,\n",
    "    batch_size: int = 32\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\" Generates mini-batches based on the data indexes\n",
    "\n",
    "        Args:\n",
    "            data_len: Length of the data\n",
    "\n",
    "            batch_size: Size of each mini-batch where the last mini-batch\n",
    "                might be smaller than the rest if the batch_size does not\n",
    "                evenly divide the data length.\n",
    "\n",
    "    \"\"\"\n",
    "    X_idx = np.arange(data_len)\n",
    "    np.random.shuffle(X_idx)\n",
    "    batches = [X_idx[i:i+batch_size] for i in range(0, data_len, batch_size)]\n",
    "\n",
    "    return batches\n",
    "\n",
    "def sum_gradients(grad_sum, new_grad):\n",
    "    \"\"\" Computes absolute sum of gradients for each layer \"\"\"\n",
    "    if len(grad_sum) == 0:\n",
    "        return new_grad\n",
    "\n",
    "    for l in range(len(grad_sum)):\n",
    "        grad_sum[l] += np.abs(new_grad[l])\n",
    "    return grad_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c98f87",
   "metadata": {
    "id": "60c98f87"
   },
   "source": [
    "#### TODO 13\n",
    "Complete the TODO by implementing the `NeuralNetwork()` which runs <u>mini-batch gradient descent</u>. You need to simply call the functions we have already defined such as `init_neural_network()`, `forward()`, `backward()`.\n",
    "    \n",
    "**Take time to also read the DocStrings or in-line documentation, given at the start of the class, which describes what each argument and class attribute/variable does. You need to understand what each argument and class attribute does before moving forward!**\n",
    "\n",
    "\n",
    "**`fit()` TODOs**\n",
    "\n",
    "1. Call the `init_neural_network()` function to initialize the weights. Store the output into `self.nn`.\n",
    "    1. Hint: Don't for get to pass `self.seed`!\n",
    "\n",
    "\n",
    "2. Call the `forward()` function to get the probability predictions for the current mini-batch. Store the outputs into `y_hat_probs`.\n",
    "\n",
    "\n",
    "3. Call the `backward()` function to update the weights and biases of our neural network for the current mini-batch. Store the outputs into `W_grads`and `b_grads`.\n",
    "\n",
    "\n",
    "4. Compute the NLL loss for the current mini-batch using the `nll()` function. Store the output into `trn_batch_loss`.\n",
    "\n",
    "\n",
    "**`predict()` TODOs**\n",
    "\n",
    "5. Call the `forward()` function to get the predictions for the passed data `X`. Store the outputs into `y_hat_probs`.\n",
    "6. Compute the predicted class label index for each data sample by taking the argmax over the columns (i.e., classes) of `y_hat_probs`. Store the output into `y_hat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "64d7d531",
   "metadata": {
    "id": "64d7d531"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(BaseEstimator):\n",
    "    \"\"\" Runs the initialization and training process for a multi-layer neural network.\n",
    "\n",
    "        Attributes:\n",
    "            neurons_per_layer: A list where each element represents\n",
    "                the neurons in a layer. For example, [2, 3] would\n",
    "                create a 2 layer neural network where the hidden layer\n",
    "                has 2 neurons and the output layer has 3 neurons.\n",
    "\n",
    "            g_hidden: Activation function used by ALL neurons\n",
    "                in ALL hidden layers.\n",
    "\n",
    "            g_output: Activation function used by ALL neurons\n",
    "                in the output layer.\n",
    "\n",
    "            alpha: Learning rate or step size used by gradient descent.\n",
    "\n",
    "            epochs: Number of times data is used to update the weights.\n",
    "                Each epoch means a data sample was used to update the weights at least\n",
    "                once.\n",
    "\n",
    "            batch_size: Mini-batch size used to determine the size of\n",
    "                mini-batches.\n",
    "\n",
    "            seed: Random seed to use when initializing the layers of\n",
    "                the neural network.\n",
    "\n",
    "            verbose: If True, print statements inside the train() method will\n",
    "                be printed.\n",
    "\n",
    "            nn: A list of Layer class instances which define the neural\n",
    "                network.\n",
    "\n",
    "            avg_trn_loss_tracker: A list that tracks the average training\n",
    "                loss per epoch.\n",
    "\n",
    "            avg_vld_loss_tracker: A list that tracks the average validation loss per epoch.\n",
    "\n",
    "            avg_W_grads_tracker: A list of arrays that track the average weight\n",
    "                gradient for each layer over all epochs.\n",
    "\n",
    "            avg_b_grads_tracker: A list that tracks the average bias\n",
    "                gradient for each layer over all epochs.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        neurons_per_layer: List[int],\n",
    "        g_hidden: object,\n",
    "        g_output: object,\n",
    "        alpha: float = .001,\n",
    "        epochs: int = 1,\n",
    "        batch_size: int = 64,\n",
    "        seed: int = 0,\n",
    "        verbose: bool = False,\n",
    "    ):\n",
    "        self.neurons_per_layer = neurons_per_layer\n",
    "        self.g_hidden = g_hidden\n",
    "        self.g_output = g_output\n",
    "        self.alpha = alpha\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        X_vld: np.ndarray = None,\n",
    "        y_vld: np.ndarray = None,\n",
    "    ) -> None:\n",
    "        \"\"\" Initializes and trains the defined neural network using gradient descent\n",
    "\n",
    "            Args:\n",
    "                X: Training features/data\n",
    "\n",
    "                y: Training targets/labels\n",
    "\n",
    "                X_vld: Validation features/data which are used for\n",
    "                    computing the validation loss after every epoch.\n",
    "\n",
    "                y_vld: Validation targets/labels which are used for\n",
    "                    computing the validation loss after every epoch.\n",
    "        \"\"\"\n",
    "        m = len(X)\n",
    "        self.avg_trn_loss_tracker = []\n",
    "        self.avg_vld_loss_tracker = []\n",
    "        self.avg_W_grads_tracker = []\n",
    "        self.avg_b_grads_tracker = []\n",
    "\n",
    "        # TODO 13.1\n",
    "        self.nn = init_neural_network(n_input_features=X.shape[1], neurons_per_layer=self.neurons_per_layer, g_hidden=self.g_hidden, g_output=self.g_output, seed=self.seed)\n",
    "\n",
    "        for e in range(self.epochs):\n",
    "            if self.verbose: print(f\"Epoch: {e+1}\")\n",
    "            batches = get_mini_batches(data_len=m, batch_size=self.batch_size)\n",
    "            total_trn_batch_loss = 0\n",
    "            total_batch_W_grads = []\n",
    "            total_batch_b_grads = []\n",
    "            for mb in batches:\n",
    "                # Forward pass to get predictions\n",
    "                # TODO 13.2\n",
    "                y_hat_probs = forward(X=X[mb], nn=self.nn)\n",
    "\n",
    "                # Backward pass to get gradients\n",
    "                # TODO 13.3\n",
    "                W_grads, b_grads = backward(X=X[mb], y=y[mb], y_hat_probs=y_hat_probs, nn=self.nn, alpha=self.alpha)\n",
    "\n",
    "                # Absolute sum gradients\n",
    "                # Non-essential to algorithm\n",
    "                total_batch_W_grads = sum_gradients(total_batch_W_grads, W_grads)\n",
    "                total_batch_b_grads = sum_gradients(total_batch_b_grads, b_grads)\n",
    "\n",
    "                # TODO 13.4\n",
    "                trn_batch_loss = nll(y_hat_probs=y_hat_probs, y=y[mb])\n",
    "                total_trn_batch_loss += trn_batch_loss\n",
    "\n",
    "            # Compute average nll loss for the epoch\n",
    "            avg_trn_loss = total_trn_batch_loss / m\n",
    "\n",
    "            # Track average nll loss for the epoch\n",
    "            self.avg_trn_loss_tracker.append(avg_trn_loss)\n",
    "            if self.verbose: print(f\"\\tTraining loss: {avg_trn_loss}\")\n",
    "\n",
    "            # Compute average gradients over samples and number of weights/baises\n",
    "            # Non-essential to algorithm\n",
    "            avg_W_grads = [np.sum(g/m) / g.size for g in total_batch_W_grads]\n",
    "            self.avg_W_grads_tracker.append(avg_W_grads)\n",
    "            avg_b_grads = [np.sum(g/m) / g.size for g in total_batch_b_grads]\n",
    "            self.avg_b_grads_tracker.append(avg_b_grads)\n",
    "\n",
    "            # Check/track validation nll loss if validation\n",
    "            # data is passed.\n",
    "            if X_vld is not None and y_vld is not None:\n",
    "                m_vld  = len(X_vld)\n",
    "\n",
    "                y_hat_probs_vld = self.predict_proba(X=X_vld)\n",
    "\n",
    "                vld_loss= nll(y_hat_probs=y_hat_probs_vld, y=y_vld)\n",
    "                avg_vld_loss = vld_loss / m_vld\n",
    "\n",
    "                self.avg_vld_loss_tracker.append(avg_vld_loss)\n",
    "                if self.verbose: print(f\"\\tValidation loss: {avg_vld_loss}\")\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Make predictions using parameters learned during training.\n",
    "\n",
    "            Args:\n",
    "                X: Features/data to make predictions with\n",
    "        \"\"\"\n",
    "        # TODO 13.5\n",
    "        y_hat_probs = forward(X=X, nn=self.nn)\n",
    "        # TODO 13.6\n",
    "        y_hat = np.argmax(y_hat_probs, axis=1)\n",
    "        return y_hat.reshape(-1, 1)\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Predict probabilities using parameters learned during training.\n",
    "\n",
    "            Args:\n",
    "                X: Features/data to make predictions with\n",
    "\n",
    "        \"\"\"\n",
    "        return forward(X=X, nn=self.nn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedb90e3",
   "metadata": {
    "id": "fedb90e3"
   },
   "source": [
    "Run the below `TEST_neural_network()` function to test your implementation of the `NeuralNetwork` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "73107806",
   "metadata": {
    "id": "73107806",
    "outputId": "c8e11746-5bce-4b66-ca60-32584f789827"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\tTraining loss: 2.3656210614014173\n",
      "Epoch: 2\n",
      "\tTraining loss: 1.2905010069500504\n",
      "Epoch: 3\n",
      "\tTraining loss: 1.1526384235816356\n",
      "Epoch: 4\n",
      "\tTraining loss: 1.1309027918943533\n",
      "Epoch: 5\n",
      "\tTraining loss: 1.1240815924624135\n",
      "Epoch: 6\n",
      "\tTraining loss: 1.1201679001112657\n",
      "Epoch: 7\n",
      "\tTraining loss: 1.118855169102074\n",
      "Epoch: 8\n",
      "\tTraining loss: 1.1167433087410734\n",
      "Epoch: 9\n",
      "\tTraining loss: 1.1140905494951443\n",
      "Epoch: 10\n",
      "\tTraining loss: 1.1111833770025175\n",
      "Epoch: 11\n",
      "\tTraining loss: 1.1100257261094377\n",
      "Epoch: 12\n",
      "\tTraining loss: 1.1079044533390416\n",
      "Epoch: 13\n",
      "\tTraining loss: 1.107580925543181\n",
      "Epoch: 14\n",
      "\tTraining loss: 1.106305044246343\n",
      "Epoch: 15\n",
      "\tTraining loss: 1.1040930555581907\n",
      "Epoch: 16\n",
      "\tTraining loss: 1.1037406089273603\n",
      "Epoch: 17\n",
      "\tTraining loss: 1.1027783891778424\n",
      "Epoch: 18\n",
      "\tTraining loss: 1.1016246518811044\n",
      "Epoch: 19\n",
      "\tTraining loss: 1.0994253967495815\n",
      "Epoch: 20\n",
      "\tTraining loss: 1.0986605844968507\n",
      "Epoch: 21\n",
      "\tTraining loss: 1.096929749827587\n",
      "Epoch: 22\n",
      "\tTraining loss: 1.0972689047983721\n",
      "Epoch: 23\n",
      "\tTraining loss: 1.096595639524615\n",
      "Epoch: 24\n",
      "\tTraining loss: 1.094944941717529\n",
      "Epoch: 25\n",
      "\tTraining loss: 1.0940983101753157\n",
      "Epoch: 26\n",
      "\tTraining loss: 1.093336262067386\n",
      "Epoch: 27\n",
      "\tTraining loss: 1.0925319190035412\n",
      "Epoch: 28\n",
      "\tTraining loss: 1.0922218572163416\n",
      "Epoch: 29\n",
      "\tTraining loss: 1.090993079177035\n",
      "Epoch: 30\n",
      "\tTraining loss: 1.0894035841298413\n",
      "Epoch: 31\n",
      "\tTraining loss: 1.0888787414482612\n",
      "Epoch: 32\n",
      "\tTraining loss: 1.0889215204253166\n",
      "Epoch: 33\n",
      "\tTraining loss: 1.0885308128862847\n",
      "Epoch: 34\n",
      "\tTraining loss: 1.0869932979143297\n",
      "Epoch: 35\n",
      "\tTraining loss: 1.0867403497516295\n",
      "Epoch: 36\n",
      "\tTraining loss: 1.084727570757532\n",
      "Epoch: 37\n",
      "\tTraining loss: 1.0831624046732444\n",
      "Epoch: 38\n",
      "\tTraining loss: 1.0836073806538211\n",
      "Epoch: 39\n",
      "\tTraining loss: 1.0823408053714878\n",
      "Epoch: 40\n",
      "\tTraining loss: 1.081165714343434\n",
      "Epoch: 41\n",
      "\tTraining loss: 1.0805530636304246\n",
      "Epoch: 42\n",
      "\tTraining loss: 1.080704808436714\n",
      "Epoch: 43\n",
      "\tTraining loss: 1.0774611716716904\n",
      "Epoch: 44\n",
      "\tTraining loss: 1.076676560102918\n",
      "Epoch: 45\n",
      "\tTraining loss: 1.0760266324039498\n",
      "Epoch: 46\n",
      "\tTraining loss: 1.0752433062548776\n",
      "Epoch: 47\n",
      "\tTraining loss: 1.074929687060421\n",
      "Epoch: 48\n",
      "\tTraining loss: 1.0734174566905248\n",
      "Epoch: 49\n",
      "\tTraining loss: 1.0719304039258568\n",
      "Epoch: 50\n",
      "\tTraining loss: 1.0703796062195992\n",
      "Epoch: 51\n",
      "\tTraining loss: 1.0697736986051176\n",
      "Epoch: 52\n",
      "\tTraining loss: 1.0698028361088805\n",
      "Epoch: 53\n",
      "\tTraining loss: 1.0672756777592634\n",
      "Epoch: 54\n",
      "\tTraining loss: 1.0670922334845625\n",
      "Epoch: 55\n",
      "\tTraining loss: 1.0672820341706082\n",
      "Epoch: 56\n",
      "\tTraining loss: 1.0659854063192764\n",
      "Epoch: 57\n",
      "\tTraining loss: 1.0644045929239558\n",
      "Epoch: 58\n",
      "\tTraining loss: 1.0623574011740056\n",
      "Epoch: 59\n",
      "\tTraining loss: 1.0619593495233\n",
      "Epoch: 60\n",
      "\tTraining loss: 1.0611688482848012\n",
      "Epoch: 61\n",
      "\tTraining loss: 1.0600782068534527\n",
      "Epoch: 62\n",
      "\tTraining loss: 1.0591417102358576\n",
      "Epoch: 63\n",
      "\tTraining loss: 1.0567272610348593\n",
      "Epoch: 64\n",
      "\tTraining loss: 1.0546400013249704\n",
      "Epoch: 65\n",
      "\tTraining loss: 1.0528544900055388\n",
      "Epoch: 66\n",
      "\tTraining loss: 1.0514313227613294\n",
      "Epoch: 67\n",
      "\tTraining loss: 1.0505508794759741\n",
      "Epoch: 68\n",
      "\tTraining loss: 1.0488730780570323\n",
      "Epoch: 69\n",
      "\tTraining loss: 1.0471503363871748\n",
      "Epoch: 70\n",
      "\tTraining loss: 1.045526739132928\n",
      "Epoch: 71\n",
      "\tTraining loss: 1.0448906966319678\n",
      "Epoch: 72\n",
      "\tTraining loss: 1.0414888481752012\n",
      "Epoch: 73\n",
      "\tTraining loss: 1.0389301088386185\n",
      "Epoch: 74\n",
      "\tTraining loss: 1.0364188816546624\n",
      "Epoch: 75\n",
      "\tTraining loss: 1.0334235886355223\n",
      "Epoch: 76\n",
      "\tTraining loss: 1.0310499412434055\n",
      "Epoch: 77\n",
      "\tTraining loss: 1.0300345618149132\n",
      "Epoch: 78\n",
      "\tTraining loss: 1.026363419151109\n",
      "Epoch: 79\n",
      "\tTraining loss: 1.0226296020396801\n",
      "Epoch: 80\n",
      "\tTraining loss: 1.0205376823457393\n",
      "Epoch: 81\n",
      "\tTraining loss: 1.017147042094759\n",
      "Epoch: 82\n",
      "\tTraining loss: 1.0146961497811342\n",
      "Epoch: 83\n",
      "\tTraining loss: 1.011835945440401\n",
      "Epoch: 84\n",
      "\tTraining loss: 1.0090402582917775\n",
      "Epoch: 85\n",
      "\tTraining loss: 1.0069487622776314\n",
      "Epoch: 86\n",
      "\tTraining loss: 1.0050926132633855\n",
      "Epoch: 87\n",
      "\tTraining loss: 1.002042635225813\n",
      "Epoch: 88\n",
      "\tTraining loss: 0.9998359460913278\n",
      "Epoch: 89\n",
      "\tTraining loss: 0.996550358780137\n",
      "Epoch: 90\n",
      "\tTraining loss: 0.993299273910907\n",
      "Epoch: 91\n",
      "\tTraining loss: 0.9928832393554635\n",
      "Epoch: 92\n",
      "\tTraining loss: 0.989007840288072\n",
      "Epoch: 93\n",
      "\tTraining loss: 0.9874875517944135\n",
      "Epoch: 94\n",
      "\tTraining loss: 0.9858115396729664\n",
      "Epoch: 95\n",
      "\tTraining loss: 0.9839120858995262\n",
      "Epoch: 96\n",
      "\tTraining loss: 0.9804601454485926\n",
      "Epoch: 97\n",
      "\tTraining loss: 0.9777868979414981\n",
      "Epoch: 98\n",
      "\tTraining loss: 0.9755036092910477\n",
      "Epoch: 99\n",
      "\tTraining loss: 0.9728662375536123\n",
      "Epoch: 100\n",
      "\tTraining loss: 0.970810242565231\n",
      "Epoch: 101\n",
      "\tTraining loss: 0.9682510502048144\n",
      "Epoch: 102\n",
      "\tTraining loss: 0.9652924560355078\n",
      "Epoch: 103\n",
      "\tTraining loss: 0.9624920892596035\n",
      "Epoch: 104\n",
      "\tTraining loss: 0.9614148329805122\n",
      "Epoch: 105\n",
      "\tTraining loss: 0.9582351709969305\n",
      "Epoch: 106\n",
      "\tTraining loss: 0.9558026344457059\n",
      "Epoch: 107\n",
      "\tTraining loss: 0.9539365081902647\n",
      "Epoch: 108\n",
      "\tTraining loss: 0.9512518028646033\n",
      "Epoch: 109\n",
      "\tTraining loss: 0.9492282603939077\n",
      "Epoch: 110\n",
      "\tTraining loss: 0.945190603068573\n",
      "Epoch: 111\n",
      "\tTraining loss: 0.9438913663317166\n",
      "Epoch: 112\n",
      "\tTraining loss: 0.9429391228458124\n",
      "Epoch: 113\n",
      "\tTraining loss: 0.9384064925238734\n",
      "Epoch: 114\n",
      "\tTraining loss: 0.9353394567792273\n",
      "Epoch: 115\n",
      "\tTraining loss: 0.934627896940036\n",
      "Epoch: 116\n",
      "\tTraining loss: 0.9300581316961385\n",
      "Epoch: 117\n",
      "\tTraining loss: 0.9284447439219035\n",
      "Epoch: 118\n",
      "\tTraining loss: 0.9261313541392839\n",
      "Epoch: 119\n",
      "\tTraining loss: 0.92185871946911\n",
      "Epoch: 120\n",
      "\tTraining loss: 0.9191270655227033\n",
      "Epoch: 121\n",
      "\tTraining loss: 0.9166910154471668\n",
      "Epoch: 122\n",
      "\tTraining loss: 0.9142021442371896\n",
      "Epoch: 123\n",
      "\tTraining loss: 0.9117962466747057\n",
      "Epoch: 124\n",
      "\tTraining loss: 0.908756387058789\n",
      "Epoch: 125\n",
      "\tTraining loss: 0.906684984793007\n",
      "Epoch: 126\n",
      "\tTraining loss: 0.9033286533159008\n",
      "Epoch: 127\n",
      "\tTraining loss: 0.9006040895581455\n",
      "Epoch: 128\n",
      "\tTraining loss: 0.8968858602699779\n",
      "Epoch: 129\n",
      "\tTraining loss: 0.8954078806368193\n",
      "Epoch: 130\n",
      "\tTraining loss: 0.8914840020908804\n",
      "Epoch: 131\n",
      "\tTraining loss: 0.889165667339079\n",
      "Epoch: 132\n",
      "\tTraining loss: 0.886226858925823\n",
      "Epoch: 133\n",
      "\tTraining loss: 0.8829758764002663\n",
      "Epoch: 134\n",
      "\tTraining loss: 0.8795154459553337\n",
      "Epoch: 135\n",
      "\tTraining loss: 0.8776026170469851\n",
      "Epoch: 136\n",
      "\tTraining loss: 0.87430039393504\n",
      "Epoch: 137\n",
      "\tTraining loss: 0.8706167065566935\n",
      "Epoch: 138\n",
      "\tTraining loss: 0.8674659605916227\n",
      "Epoch: 139\n",
      "\tTraining loss: 0.8651744397651339\n",
      "Epoch: 140\n",
      "\tTraining loss: 0.8625438426124709\n",
      "Epoch: 141\n",
      "\tTraining loss: 0.8608700233832745\n",
      "Epoch: 142\n",
      "\tTraining loss: 0.8565198942389398\n",
      "Epoch: 143\n",
      "\tTraining loss: 0.8532883437922986\n",
      "Epoch: 144\n",
      "\tTraining loss: 0.8510185514751067\n",
      "Epoch: 145\n",
      "\tTraining loss: 0.8484494208933206\n",
      "Epoch: 146\n",
      "\tTraining loss: 0.8438992183372839\n",
      "Epoch: 147\n",
      "\tTraining loss: 0.8404320948265308\n",
      "Epoch: 148\n",
      "\tTraining loss: 0.8380746254929499\n",
      "Epoch: 149\n",
      "\tTraining loss: 0.8344047415506929\n",
      "Epoch: 150\n",
      "\tTraining loss: 0.8316167356946422\n",
      "Epoch: 151\n",
      "\tTraining loss: 0.8278481434690876\n",
      "Epoch: 152\n",
      "\tTraining loss: 0.8244838979601287\n",
      "Epoch: 153\n",
      "\tTraining loss: 0.8206533202027089\n",
      "Epoch: 154\n",
      "\tTraining loss: 0.8175535762780457\n",
      "Epoch: 155\n",
      "\tTraining loss: 0.8137393993499167\n",
      "Epoch: 156\n",
      "\tTraining loss: 0.8092700871100555\n",
      "Epoch: 157\n",
      "\tTraining loss: 0.806137726952855\n",
      "Epoch: 158\n",
      "\tTraining loss: 0.8031008630118405\n",
      "Epoch: 159\n",
      "\tTraining loss: 0.7978860875886712\n",
      "Epoch: 160\n",
      "\tTraining loss: 0.7920482742480293\n",
      "Epoch: 161\n",
      "\tTraining loss: 0.7886726996986526\n",
      "Epoch: 162\n",
      "\tTraining loss: 0.7839855585095831\n",
      "Epoch: 163\n",
      "\tTraining loss: 0.7818190704759559\n",
      "Epoch: 164\n",
      "\tTraining loss: 0.7757096389935465\n",
      "Epoch: 165\n",
      "\tTraining loss: 0.772440241276259\n",
      "Epoch: 166\n",
      "\tTraining loss: 0.767839295631076\n",
      "Epoch: 167\n",
      "\tTraining loss: 0.7628041046410581\n",
      "Epoch: 168\n",
      "\tTraining loss: 0.7584349211179348\n",
      "Epoch: 169\n",
      "\tTraining loss: 0.7543278900990348\n",
      "Epoch: 170\n",
      "\tTraining loss: 0.7503762072118325\n",
      "Epoch: 171\n",
      "\tTraining loss: 0.7466321013360556\n",
      "Epoch: 172\n",
      "\tTraining loss: 0.741847861287035\n",
      "Epoch: 173\n",
      "\tTraining loss: 0.7398212183943649\n",
      "Epoch: 174\n",
      "\tTraining loss: 0.7336123891544329\n",
      "Epoch: 175\n",
      "\tTraining loss: 0.7295570129352124\n",
      "Epoch: 176\n",
      "\tTraining loss: 0.7242720692317975\n",
      "Epoch: 177\n",
      "\tTraining loss: 0.7204505913309238\n",
      "Epoch: 178\n",
      "\tTraining loss: 0.716586274794311\n",
      "Epoch: 179\n",
      "\tTraining loss: 0.7111579143360388\n",
      "Epoch: 180\n",
      "\tTraining loss: 0.7055735808272564\n",
      "Epoch: 181\n",
      "\tTraining loss: 0.7007161208902867\n",
      "Epoch: 182\n",
      "\tTraining loss: 0.6962908285762455\n",
      "Epoch: 183\n",
      "\tTraining loss: 0.6906307257292096\n",
      "Epoch: 184\n",
      "\tTraining loss: 0.6879318566010308\n",
      "Epoch: 185\n",
      "\tTraining loss: 0.6818888160823661\n",
      "Epoch: 186\n",
      "\tTraining loss: 0.6770805653919495\n",
      "Epoch: 187\n",
      "\tTraining loss: 0.6743377564371551\n",
      "Epoch: 188\n",
      "\tTraining loss: 0.6680065213779562\n",
      "Epoch: 189\n",
      "\tTraining loss: 0.6646794324441856\n",
      "Epoch: 190\n",
      "\tTraining loss: 0.6595052217429198\n",
      "Epoch: 191\n",
      "\tTraining loss: 0.6555883589162487\n",
      "Epoch: 192\n",
      "\tTraining loss: 0.6513719268499606\n",
      "Epoch: 193\n",
      "\tTraining loss: 0.6469149950446834\n",
      "Epoch: 194\n",
      "\tTraining loss: 0.6413968852932092\n",
      "Epoch: 195\n",
      "\tTraining loss: 0.6370145382813452\n",
      "Epoch: 196\n",
      "\tTraining loss: 0.6327685268896348\n",
      "Epoch: 197\n",
      "\tTraining loss: 0.6288460523002272\n",
      "Epoch: 198\n",
      "\tTraining loss: 0.6246789477767796\n",
      "Epoch: 199\n",
      "\tTraining loss: 0.6211817689852293\n",
      "Epoch: 200\n",
      "\tTraining loss: 0.6169847814826527\n",
      "Epoch: 201\n",
      "\tTraining loss: 0.6132135647347433\n",
      "Epoch: 202\n",
      "\tTraining loss: 0.6077950132546277\n",
      "Epoch: 203\n",
      "\tTraining loss: 0.6055268757414161\n",
      "Epoch: 204\n",
      "\tTraining loss: 0.6006320165652365\n",
      "Epoch: 205\n",
      "\tTraining loss: 0.5965035218396222\n",
      "Epoch: 206\n",
      "\tTraining loss: 0.5931936342787018\n",
      "Epoch: 207\n",
      "\tTraining loss: 0.5904936621276428\n",
      "Epoch: 208\n",
      "\tTraining loss: 0.5865740718659459\n",
      "Epoch: 209\n",
      "\tTraining loss: 0.5861506566818689\n",
      "Epoch: 210\n",
      "\tTraining loss: 0.5780821985170361\n",
      "Epoch: 211\n",
      "\tTraining loss: 0.57488941614104\n",
      "Epoch: 212\n",
      "\tTraining loss: 0.5711556242291281\n",
      "Epoch: 213\n",
      "\tTraining loss: 0.568142966082643\n",
      "Epoch: 214\n",
      "\tTraining loss: 0.563896630065657\n",
      "Epoch: 215\n",
      "\tTraining loss: 0.5607439533173756\n",
      "Epoch: 216\n",
      "\tTraining loss: 0.5580985797985277\n",
      "Epoch: 217\n",
      "\tTraining loss: 0.5547488476127436\n",
      "Epoch: 218\n",
      "\tTraining loss: 0.5497551723992242\n",
      "Epoch: 219\n",
      "\tTraining loss: 0.5467921635382135\n",
      "Epoch: 220\n",
      "\tTraining loss: 0.5421346807342585\n",
      "Epoch: 221\n",
      "\tTraining loss: 0.5391195762189083\n",
      "Epoch: 222\n",
      "\tTraining loss: 0.536115224241043\n",
      "Epoch: 223\n",
      "\tTraining loss: 0.5332765911476391\n",
      "Epoch: 224\n",
      "\tTraining loss: 0.5307664290275511\n",
      "Epoch: 225\n",
      "\tTraining loss: 0.5268847580915796\n",
      "Epoch: 226\n",
      "\tTraining loss: 0.5256614554286131\n",
      "Epoch: 227\n",
      "\tTraining loss: 0.5200133105346522\n",
      "Epoch: 228\n",
      "\tTraining loss: 0.517655196368591\n",
      "Epoch: 229\n",
      "\tTraining loss: 0.5137230784423197\n",
      "Epoch: 230\n",
      "\tTraining loss: 0.5108418224110831\n",
      "Epoch: 231\n",
      "\tTraining loss: 0.5078019301105314\n",
      "Epoch: 232\n",
      "\tTraining loss: 0.5056073608247394\n",
      "Epoch: 233\n",
      "\tTraining loss: 0.5020197199458957\n",
      "Epoch: 234\n",
      "\tTraining loss: 0.4983660351305905\n",
      "Epoch: 235\n",
      "\tTraining loss: 0.4948735741475473\n",
      "Epoch: 236\n",
      "\tTraining loss: 0.4918559960588684\n",
      "Epoch: 237\n",
      "\tTraining loss: 0.48897257501458774\n",
      "Epoch: 238\n",
      "\tTraining loss: 0.48735895689521236\n",
      "Epoch: 239\n",
      "\tTraining loss: 0.4822459896606629\n",
      "Epoch: 240\n",
      "\tTraining loss: 0.48019356431120636\n",
      "Epoch: 241\n",
      "\tTraining loss: 0.4780695858040424\n",
      "Epoch: 242\n",
      "\tTraining loss: 0.4736205604435374\n",
      "Epoch: 243\n",
      "\tTraining loss: 0.4717923226433417\n",
      "Epoch: 244\n",
      "\tTraining loss: 0.4717751219405334\n",
      "Epoch: 245\n",
      "\tTraining loss: 0.4656745925050309\n",
      "Epoch: 246\n",
      "\tTraining loss: 0.46262757605062116\n",
      "Epoch: 247\n",
      "\tTraining loss: 0.46113437056596035\n",
      "Epoch: 248\n",
      "\tTraining loss: 0.45614121438682764\n",
      "Epoch: 249\n",
      "\tTraining loss: 0.4548477526334219\n",
      "Epoch: 250\n",
      "\tTraining loss: 0.45202796963114394\n",
      "Epoch: 251\n",
      "\tTraining loss: 0.4494354113133232\n",
      "Epoch: 252\n",
      "\tTraining loss: 0.44669936044985575\n",
      "Epoch: 253\n",
      "\tTraining loss: 0.4459346360115912\n",
      "Epoch: 254\n",
      "\tTraining loss: 0.4408533113241954\n",
      "Epoch: 255\n",
      "\tTraining loss: 0.44165523483892655\n",
      "Epoch: 256\n",
      "\tTraining loss: 0.43523842724424505\n",
      "Epoch: 257\n",
      "\tTraining loss: 0.4351699280020261\n",
      "Epoch: 258\n",
      "\tTraining loss: 0.43090310417038274\n",
      "Epoch: 259\n",
      "\tTraining loss: 0.42832981192319836\n",
      "Epoch: 260\n",
      "\tTraining loss: 0.4261453162606034\n",
      "Epoch: 261\n",
      "\tTraining loss: 0.42268975427291033\n",
      "Epoch: 262\n",
      "\tTraining loss: 0.42294246652336687\n",
      "Epoch: 263\n",
      "\tTraining loss: 0.417471376455639\n",
      "Epoch: 264\n",
      "\tTraining loss: 0.4152009259780818\n",
      "Epoch: 265\n",
      "\tTraining loss: 0.4128330915502025\n",
      "Epoch: 266\n",
      "\tTraining loss: 0.4109619507393683\n",
      "Epoch: 267\n",
      "\tTraining loss: 0.4075657315306131\n",
      "Epoch: 268\n",
      "\tTraining loss: 0.40678803220730414\n",
      "Epoch: 269\n",
      "\tTraining loss: 0.4052213204412428\n",
      "Epoch: 270\n",
      "\tTraining loss: 0.4036884410592829\n",
      "Epoch: 271\n",
      "\tTraining loss: 0.3997146960796445\n",
      "Epoch: 272\n",
      "\tTraining loss: 0.3978307868533816\n",
      "Epoch: 273\n",
      "\tTraining loss: 0.3937289774098879\n",
      "Epoch: 274\n",
      "\tTraining loss: 0.39078872422069655\n",
      "Epoch: 275\n",
      "\tTraining loss: 0.3883592423690888\n",
      "Epoch: 276\n",
      "\tTraining loss: 0.3871623834978565\n",
      "Epoch: 277\n",
      "\tTraining loss: 0.38495473041610767\n",
      "Epoch: 278\n",
      "\tTraining loss: 0.3826610508090548\n",
      "Epoch: 279\n",
      "\tTraining loss: 0.38147553816712243\n",
      "Epoch: 280\n",
      "\tTraining loss: 0.3783218088989923\n",
      "Epoch: 281\n",
      "\tTraining loss: 0.37688949287643586\n",
      "Epoch: 282\n",
      "\tTraining loss: 0.3739528766120642\n",
      "Epoch: 283\n",
      "\tTraining loss: 0.3733802760664222\n",
      "Epoch: 284\n",
      "\tTraining loss: 0.3703500020717122\n",
      "Epoch: 285\n",
      "\tTraining loss: 0.3673058987162455\n",
      "Epoch: 286\n",
      "\tTraining loss: 0.36626088832818654\n",
      "Epoch: 287\n",
      "\tTraining loss: 0.3649052713573047\n",
      "Epoch: 288\n",
      "\tTraining loss: 0.36197235839633374\n",
      "Epoch: 289\n",
      "\tTraining loss: 0.36009949877482167\n",
      "Epoch: 290\n",
      "\tTraining loss: 0.35917526993143917\n",
      "Epoch: 291\n",
      "\tTraining loss: 0.35630054567621094\n",
      "Epoch: 292\n",
      "\tTraining loss: 0.3554621071488169\n",
      "Epoch: 293\n",
      "\tTraining loss: 0.3526653893634511\n",
      "Epoch: 294\n",
      "\tTraining loss: 0.35126259474000604\n",
      "Epoch: 295\n",
      "\tTraining loss: 0.34809948083058784\n",
      "Epoch: 296\n",
      "\tTraining loss: 0.35090881694318815\n",
      "Epoch: 297\n",
      "\tTraining loss: 0.34607689055564883\n",
      "Epoch: 298\n",
      "\tTraining loss: 0.34528403052249906\n",
      "Epoch: 299\n",
      "\tTraining loss: 0.3447046375368968\n",
      "Epoch: 300\n",
      "\tTraining loss: 0.34059470393820457\n",
      "Epoch: 301\n",
      "\tTraining loss: 0.33879434636171923\n",
      "Epoch: 302\n",
      "\tTraining loss: 0.33955425792944915\n",
      "Epoch: 303\n",
      "\tTraining loss: 0.3369626313327808\n",
      "Epoch: 304\n",
      "\tTraining loss: 0.33634685876366904\n",
      "Epoch: 305\n",
      "\tTraining loss: 0.33491877945420534\n",
      "Epoch: 306\n",
      "\tTraining loss: 0.3318718020785218\n",
      "Epoch: 307\n",
      "\tTraining loss: 0.33082836069072785\n",
      "Epoch: 308\n",
      "\tTraining loss: 0.32933149219326\n",
      "Epoch: 309\n",
      "\tTraining loss: 0.3235999191984191\n",
      "Epoch: 310\n",
      "\tTraining loss: 0.32566412379821025\n",
      "Epoch: 311\n",
      "\tTraining loss: 0.32622008084740645\n",
      "Epoch: 312\n",
      "\tTraining loss: 0.3241210779720225\n",
      "Epoch: 313\n",
      "\tTraining loss: 0.322488950508035\n",
      "Epoch: 314\n",
      "\tTraining loss: 0.3214589653495673\n",
      "Epoch: 315\n",
      "\tTraining loss: 0.3185260352367031\n",
      "Epoch: 316\n",
      "\tTraining loss: 0.3158177743234149\n",
      "Epoch: 317\n",
      "\tTraining loss: 0.3156690455147908\n",
      "Epoch: 318\n",
      "\tTraining loss: 0.3142447589392131\n",
      "Epoch: 319\n",
      "\tTraining loss: 0.31207477764155694\n",
      "Epoch: 320\n",
      "\tTraining loss: 0.3132139040082983\n",
      "Epoch: 321\n",
      "\tTraining loss: 0.3105624517408495\n",
      "Epoch: 322\n",
      "\tTraining loss: 0.30919101840552726\n",
      "Epoch: 323\n",
      "\tTraining loss: 0.30888521347003245\n",
      "Epoch: 324\n",
      "\tTraining loss: 0.3057180198823317\n",
      "Epoch: 325\n",
      "\tTraining loss: 0.3018202638431713\n",
      "Epoch: 326\n",
      "\tTraining loss: 0.3037936161009677\n",
      "Epoch: 327\n",
      "\tTraining loss: 0.3008804707196091\n",
      "Epoch: 328\n",
      "\tTraining loss: 0.2994224810084369\n",
      "Epoch: 329\n",
      "\tTraining loss: 0.3037734436601862\n",
      "Epoch: 330\n",
      "\tTraining loss: 0.2972239939879645\n",
      "Epoch: 331\n",
      "\tTraining loss: 0.2966856115631977\n",
      "Epoch: 332\n",
      "\tTraining loss: 0.2952390226235631\n",
      "Epoch: 333\n",
      "\tTraining loss: 0.29459620498761874\n",
      "Epoch: 334\n",
      "\tTraining loss: 0.2912571653631831\n",
      "Epoch: 335\n",
      "\tTraining loss: 0.29189074191826314\n",
      "Epoch: 336\n",
      "\tTraining loss: 0.28945581641868745\n",
      "Epoch: 337\n",
      "\tTraining loss: 0.2908100734144939\n",
      "Epoch: 338\n",
      "\tTraining loss: 0.2889250188943152\n",
      "Epoch: 339\n",
      "\tTraining loss: 0.28733674045701046\n",
      "Epoch: 340\n",
      "\tTraining loss: 0.28696542621200616\n",
      "Epoch: 341\n",
      "\tTraining loss: 0.28593951803875534\n",
      "Epoch: 342\n",
      "\tTraining loss: 0.2847470412722054\n",
      "Epoch: 343\n",
      "\tTraining loss: 0.2828771347830367\n",
      "Epoch: 344\n",
      "\tTraining loss: 0.2816660523328347\n",
      "Epoch: 345\n",
      "\tTraining loss: 0.28042368863745315\n",
      "Epoch: 346\n",
      "\tTraining loss: 0.2801590189540982\n",
      "Epoch: 347\n",
      "\tTraining loss: 0.2794720447630681\n",
      "Epoch: 348\n",
      "\tTraining loss: 0.2783287243004787\n",
      "Epoch: 349\n",
      "\tTraining loss: 0.27546298630970256\n",
      "Epoch: 350\n",
      "\tTraining loss: 0.27658504882488966\n",
      "Epoch: 351\n",
      "\tTraining loss: 0.27455212161775894\n",
      "Epoch: 352\n",
      "\tTraining loss: 0.275151681564352\n",
      "Epoch: 353\n",
      "\tTraining loss: 0.2697294131909262\n",
      "Epoch: 354\n",
      "\tTraining loss: 0.2699209355503084\n",
      "Epoch: 355\n",
      "\tTraining loss: 0.27008512810776086\n",
      "Epoch: 356\n",
      "\tTraining loss: 0.2675014762521598\n",
      "Epoch: 357\n",
      "\tTraining loss: 0.26898448324454677\n",
      "Epoch: 358\n",
      "\tTraining loss: 0.2673833195242995\n",
      "Epoch: 359\n",
      "\tTraining loss: 0.2654594632509578\n",
      "Epoch: 360\n",
      "\tTraining loss: 0.2650200559454925\n",
      "Epoch: 361\n",
      "\tTraining loss: 0.26305631256788414\n",
      "Epoch: 362\n",
      "\tTraining loss: 0.26372622659572786\n",
      "Epoch: 363\n",
      "\tTraining loss: 0.26030685575415285\n",
      "Epoch: 364\n",
      "\tTraining loss: 0.2609153152635889\n",
      "Epoch: 365\n",
      "\tTraining loss: 0.26230702746316087\n",
      "Epoch: 366\n",
      "\tTraining loss: 0.25917286248873933\n",
      "Epoch: 367\n",
      "\tTraining loss: 0.2603544336907929\n",
      "Epoch: 368\n",
      "\tTraining loss: 0.25987716625396173\n",
      "Epoch: 369\n",
      "\tTraining loss: 0.2566856578752859\n",
      "Epoch: 370\n",
      "\tTraining loss: 0.2562369380283864\n",
      "Epoch: 371\n",
      "\tTraining loss: 0.25419983290046744\n",
      "Epoch: 372\n",
      "\tTraining loss: 0.2521123197306495\n",
      "Epoch: 373\n",
      "\tTraining loss: 0.2536086769934566\n",
      "Epoch: 374\n",
      "\tTraining loss: 0.25359766430709113\n",
      "Epoch: 375\n",
      "\tTraining loss: 0.24975941792153283\n",
      "Epoch: 376\n",
      "\tTraining loss: 0.2511154114411967\n",
      "Epoch: 377\n",
      "\tTraining loss: 0.24973860919337545\n",
      "Epoch: 378\n",
      "\tTraining loss: 0.24960206968172452\n",
      "Epoch: 379\n",
      "\tTraining loss: 0.24855889466051764\n",
      "Epoch: 380\n",
      "\tTraining loss: 0.24911978894132497\n",
      "Epoch: 381\n",
      "\tTraining loss: 0.2468104431141367\n",
      "Epoch: 382\n",
      "\tTraining loss: 0.24516516206949143\n",
      "Epoch: 383\n",
      "\tTraining loss: 0.2447206032086951\n",
      "Epoch: 384\n",
      "\tTraining loss: 0.24538220282087508\n",
      "Epoch: 385\n",
      "\tTraining loss: 0.24343768583656328\n",
      "Epoch: 386\n",
      "\tTraining loss: 0.24191046578803146\n",
      "Epoch: 387\n",
      "\tTraining loss: 0.24455823357918724\n",
      "Epoch: 388\n",
      "\tTraining loss: 0.24263611587022074\n",
      "Epoch: 389\n",
      "\tTraining loss: 0.24234572611344893\n",
      "Epoch: 390\n",
      "\tTraining loss: 0.23946572706520286\n",
      "Epoch: 391\n",
      "\tTraining loss: 0.24392219327021086\n",
      "Epoch: 392\n",
      "\tTraining loss: 0.23801133000664867\n",
      "Epoch: 393\n",
      "\tTraining loss: 0.23817796070227035\n",
      "Epoch: 394\n",
      "\tTraining loss: 0.24019977505516626\n",
      "Epoch: 395\n",
      "\tTraining loss: 0.23515945007949576\n",
      "Epoch: 396\n",
      "\tTraining loss: 0.23609422623419288\n",
      "Epoch: 397\n",
      "\tTraining loss: 0.23549259328017874\n",
      "Epoch: 398\n",
      "\tTraining loss: 0.23638340315369452\n",
      "Epoch: 399\n",
      "\tTraining loss: 0.2339259265082738\n",
      "Epoch: 400\n",
      "\tTraining loss: 0.23370315158552019\n",
      "Epoch: 401\n",
      "\tTraining loss: 0.23218631834502626\n",
      "Epoch: 402\n",
      "\tTraining loss: 0.2315605682730124\n",
      "Epoch: 403\n",
      "\tTraining loss: 0.23121105876041922\n",
      "Epoch: 404\n",
      "\tTraining loss: 0.23478009779837333\n",
      "Epoch: 405\n",
      "\tTraining loss: 0.23240973020165887\n",
      "Epoch: 406\n",
      "\tTraining loss: 0.22994347219519642\n",
      "Epoch: 407\n",
      "\tTraining loss: 0.22896269905096192\n",
      "Epoch: 408\n",
      "\tTraining loss: 0.22791842884467656\n",
      "Epoch: 409\n",
      "\tTraining loss: 0.22696972191906814\n",
      "Epoch: 410\n",
      "\tTraining loss: 0.22568582047220379\n",
      "Epoch: 411\n",
      "\tTraining loss: 0.22659899655665636\n",
      "Epoch: 412\n",
      "\tTraining loss: 0.2259584862289413\n",
      "Epoch: 413\n",
      "\tTraining loss: 0.22470921470714916\n",
      "Epoch: 414\n",
      "\tTraining loss: 0.22436392973334812\n",
      "Epoch: 415\n",
      "\tTraining loss: 0.2221726462811429\n",
      "Epoch: 416\n",
      "\tTraining loss: 0.2249412137326379\n",
      "Epoch: 417\n",
      "\tTraining loss: 0.22192270803282188\n",
      "Epoch: 418\n",
      "\tTraining loss: 0.22153780103778134\n",
      "Epoch: 419\n",
      "\tTraining loss: 0.22163384401224318\n",
      "Epoch: 420\n",
      "\tTraining loss: 0.2186811254113838\n",
      "Epoch: 421\n",
      "\tTraining loss: 0.22196129040384704\n",
      "Epoch: 422\n",
      "\tTraining loss: 0.22170167812394861\n",
      "Epoch: 423\n",
      "\tTraining loss: 0.21822824374770294\n",
      "Epoch: 424\n",
      "\tTraining loss: 0.21881136377140373\n",
      "Epoch: 425\n",
      "\tTraining loss: 0.21894857913120255\n",
      "Epoch: 426\n",
      "\tTraining loss: 0.21854203887155094\n",
      "Epoch: 427\n",
      "\tTraining loss: 0.2169623836511141\n",
      "Epoch: 428\n",
      "\tTraining loss: 0.2195381276883628\n",
      "Epoch: 429\n",
      "\tTraining loss: 0.2183970305282583\n",
      "Epoch: 430\n",
      "\tTraining loss: 0.21609704273747266\n",
      "Epoch: 431\n",
      "\tTraining loss: 0.2130928408976169\n",
      "Epoch: 432\n",
      "\tTraining loss: 0.21420680629687092\n",
      "Epoch: 433\n",
      "\tTraining loss: 0.2144166803782814\n",
      "Epoch: 434\n",
      "\tTraining loss: 0.21548929050183854\n",
      "Epoch: 435\n",
      "\tTraining loss: 0.21412837351170705\n",
      "Epoch: 436\n",
      "\tTraining loss: 0.212217830605121\n",
      "Epoch: 437\n",
      "\tTraining loss: 0.21250286608893942\n",
      "Epoch: 438\n",
      "\tTraining loss: 0.21288629886299038\n",
      "Epoch: 439\n",
      "\tTraining loss: 0.2103480155004537\n",
      "Epoch: 440\n",
      "\tTraining loss: 0.21092166350273073\n",
      "Epoch: 441\n",
      "\tTraining loss: 0.21282983726063673\n",
      "Epoch: 442\n",
      "\tTraining loss: 0.20845023347382657\n",
      "Epoch: 443\n",
      "\tTraining loss: 0.2112634280128323\n",
      "Epoch: 444\n",
      "\tTraining loss: 0.2080735882317993\n",
      "Epoch: 445\n",
      "\tTraining loss: 0.2119186166332632\n",
      "Epoch: 446\n",
      "\tTraining loss: 0.20793158686917995\n",
      "Epoch: 447\n",
      "\tTraining loss: 0.20748242164469052\n",
      "Epoch: 448\n",
      "\tTraining loss: 0.20639395312829795\n",
      "Epoch: 449\n",
      "\tTraining loss: 0.2076306040245383\n",
      "Epoch: 450\n",
      "\tTraining loss: 0.20841070738759462\n",
      "Epoch: 451\n",
      "\tTraining loss: 0.2051469292624771\n",
      "Epoch: 452\n",
      "\tTraining loss: 0.20457056291790127\n",
      "Epoch: 453\n",
      "\tTraining loss: 0.20338727419565789\n",
      "Epoch: 454\n",
      "\tTraining loss: 0.20576225810989146\n",
      "Epoch: 455\n",
      "\tTraining loss: 0.20564179242407077\n",
      "Epoch: 456\n",
      "\tTraining loss: 0.2064065530709583\n",
      "Epoch: 457\n",
      "\tTraining loss: 0.20800240616374832\n",
      "Epoch: 458\n",
      "\tTraining loss: 0.2026146218369893\n",
      "Epoch: 459\n",
      "\tTraining loss: 0.20275199978476874\n",
      "Epoch: 460\n",
      "\tTraining loss: 0.2031947197366369\n",
      "Epoch: 461\n",
      "\tTraining loss: 0.2008305617870434\n",
      "Epoch: 462\n",
      "\tTraining loss: 0.2039012817598904\n",
      "Epoch: 463\n",
      "\tTraining loss: 0.20260976970757624\n",
      "Epoch: 464\n",
      "\tTraining loss: 0.20153391126478248\n",
      "Epoch: 465\n",
      "\tTraining loss: 0.1995665243179116\n",
      "Epoch: 466\n",
      "\tTraining loss: 0.19903906349988057\n",
      "Epoch: 467\n",
      "\tTraining loss: 0.1988656799656668\n",
      "Epoch: 468\n",
      "\tTraining loss: 0.20278483284318383\n",
      "Epoch: 469\n",
      "\tTraining loss: 0.19929243751273662\n",
      "Epoch: 470\n",
      "\tTraining loss: 0.20050758673088082\n",
      "Epoch: 471\n",
      "\tTraining loss: 0.19795126139877223\n",
      "Epoch: 472\n",
      "\tTraining loss: 0.19920929875659768\n",
      "Epoch: 473\n",
      "\tTraining loss: 0.19499263962024013\n",
      "Epoch: 474\n",
      "\tTraining loss: 0.19551502434179602\n",
      "Epoch: 475\n",
      "\tTraining loss: 0.19601750291772763\n",
      "Epoch: 476\n",
      "\tTraining loss: 0.19974911170959564\n",
      "Epoch: 477\n",
      "\tTraining loss: 0.1985935750699823\n",
      "Epoch: 478\n",
      "\tTraining loss: 0.1956746303002406\n",
      "Epoch: 479\n",
      "\tTraining loss: 0.19552837548079854\n",
      "Epoch: 480\n",
      "\tTraining loss: 0.19900485890741262\n",
      "Epoch: 481\n",
      "\tTraining loss: 0.19219778233898716\n",
      "Epoch: 482\n",
      "\tTraining loss: 0.19474202629658893\n",
      "Epoch: 483\n",
      "\tTraining loss: 0.1937898164018222\n",
      "Epoch: 484\n",
      "\tTraining loss: 0.1992496021954276\n",
      "Epoch: 485\n",
      "\tTraining loss: 0.19337489000493432\n",
      "Epoch: 486\n",
      "\tTraining loss: 0.19235268223225982\n",
      "Epoch: 487\n",
      "\tTraining loss: 0.19198398966865574\n",
      "Epoch: 488\n",
      "\tTraining loss: 0.1922005600014894\n",
      "Epoch: 489\n",
      "\tTraining loss: 0.19237435652392992\n",
      "Epoch: 490\n",
      "\tTraining loss: 0.18835838495267387\n",
      "Epoch: 491\n",
      "\tTraining loss: 0.19705269309445517\n",
      "Epoch: 492\n",
      "\tTraining loss: 0.19507487142525226\n",
      "Epoch: 493\n",
      "\tTraining loss: 0.1927970558080754\n",
      "Epoch: 494\n",
      "\tTraining loss: 0.1917628259498525\n",
      "Epoch: 495\n",
      "\tTraining loss: 0.19450601992285066\n",
      "Epoch: 496\n",
      "\tTraining loss: 0.18753008713551342\n",
      "Epoch: 497\n",
      "\tTraining loss: 0.18830690605231912\n",
      "Epoch: 498\n",
      "\tTraining loss: 0.18943574004670644\n",
      "Epoch: 499\n",
      "\tTraining loss: 0.18936328823146895\n",
      "Epoch: 500\n",
      "\tTraining loss: 0.18665082230193872\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train NLL loss: 0.18756346192770018\n",
      "Accuracy ratio: 475/500\n",
      "Accuracy: 0.95\n",
      "Your code PASSED the code check!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAs1UlEQVR4nO3deXjcZbn/8fedfWuSpknXdF+ALtBCKaVsLcgOckBZVBRQwINy8Mhh1aMHPfrT4zmiIioiAqJsIjsiS1nL1pLWAi2lULrQdE2aNnua7f79Md+UNJkk0yTTSTKf13XNNTPfbe4n0LnnWb7PY+6OiIjEr4RYByAiIrGlRCAiEueUCERE4pwSgYhInFMiEBGJc0oEIiJxTolApBNmdoyZrY51HCLRpEQgfZaZrTezz8QyBndf5O4HROv6Znaymb1qZpVmVmJmr5jZZ6P1eSLhKBFIXDOzxBh+9ueBh4B7gEJgGPB94MxuXMvMTP+epVv0P470O2aWYGY3mNnHZrbDzP5qZnmt9j9kZlvNrDz4tT2t1b67zex3Zva0mVUDC4KaxzVm9m5wzoNmlhYcP9/Milud3+Gxwf7rzGyLmW02s0vNzM1sUpgyGHAz8N/ufoe7l7t7s7u/4u6XBcfcZGZ/aXXOuOB6ScH7l83sx2b2OlADfMfMitp8zrfN7IngdaqZ/Z+ZfWJm28zsNjNL7+F/DhkAlAikP7oK+BfgOGAksBP4Tav9/wAmA0OBZcC9bc7/IvBjYBDwWrDtPOAUYDxwMHBxJ58f9lgzOwW4GvgMMCmIryMHAKOBv3VyTCS+DFxOqCy/Bg4ws8mt9n8RuC94/T/AFGBmEN8oQjUQiXNKBNIffR34rrsXu/tu4Cbg8y2/lN39TnevbLXvEDPLaXX+4+7+evALvC7Ydou7b3b3MuBJQl+WHeno2POAu9x9pbvXAD/o5BpDguctEZa5I3cHn9fo7uXA48AXAIKEcCDwRFADuQz4truXuXsl8P+AC3r4+TIAKBFIfzQWeNTMdpnZLmAV0AQMM7NEM/tp0GxUAawPzslvdf7GMNfc2up1DZDVyed3dOzINtcO9zktdgTPIzo5JhJtP+M+gkRAqDbwWJCUCoAMYGmrv9szwXaJc0oE0h9tBE5199xWjzR330Toy+8sQs0zOcC44BxrdX60ptzdQqjTt8XoTo5dTagcn+vkmGpCX94thoc5pm1ZngPyzWwmoYTQ0ixUCtQC01r9zXLcvbOEJ3FCiUD6umQzS2v1SAJuA35sZmMBzKzAzM4Kjh8E7Cb0izuDUPPH/vJX4BIzO8jMMuik/d1D879fDXzPzC4xs+ygE/xoM7s9OGw5cKyZjQmatm7sKgB3byTU7/C/QB7wfLC9GfgD8AszGwpgZqPM7OTuFlYGDiUC6eueJvRLtuVxE/Ar4AngOTOrBN4CjgiOvwfYAGwC3g/27Rfu/g/gFuAlYA3wZrBrdwfH/w04H/gqsBnYBvyIUDs/7v488CDwLrAUeCrCUO4jVCN6KEgMLa4P4noraDZbSKjTWuKcaWEakegws4OAFUBqmy9kkT5FNQKRXmRmZ5tZipkNJjRc80klAenrlAhEetfXgRLgY0Ijma6IbTgiXVPTkIhInFONQEQkziXFOoB9lZ+f7+PGjYt1GCIi/crSpUtL3T3sDYT9LhGMGzeOoqKirg8UEZE9zGxDR/vUNCQiEueUCERE4pwSgYhInOt3fQQiMnA1NDRQXFxMXV1d1wdLWGlpaRQWFpKcnBzxOUoEItJnFBcXM2jQIMaNG0doCQXZF+7Ojh07KC4uZvz48RGfp6YhEekz6urqGDJkiJJAN5kZQ4YM2ecalRKBiPQpSgI9052/X9wkgtVbK7n5udWUVoWdEVhEJG7FTSJYs72KW15cQ1l1faxDEZE+aMeOHcycOZOZM2cyfPhwRo0ated9fX3n3xtFRUVcddVV+/R548aNo7S0tCch95q46SxuqS01a5I9EQljyJAhLF++HICbbrqJrKwsrrnmmj37GxsbSUoK/5U5e/ZsZs+evT/CjIq4qREktCSC5tjGISL9x8UXX8zVV1/NggULuP7661myZAnz5s1j1qxZzJs3j9WrVwPw8ssvc8YZZwChJPLVr36V+fPnM2HCBG655ZYuP+fmm29m+vTpTJ8+nV/+8pcAVFdXc/rpp3PIIYcwffp0HnzwQQBuuOEGpk6dysEHH7xXouqJOKoRhDKBR23dchHpTT94ciXvb67o1WtOHZnNf505bZ/O+fDDD1m4cCGJiYlUVFTw6quvkpSUxMKFC/nOd77Dww8/3O6cDz74gJdeeonKykoOOOAArrjiig7H9S9dupS77rqLxYsX4+4cccQRHHfccaxdu5aRI0fy97//HYDy8nLKysp49NFH+eCDDzAzdu3atc9/g3DipkbQ0o+uliER2RfnnnsuiYmJQOjL+Nxzz2X69Ol8+9vfZuXKlWHPOf3000lNTSU/P5+hQ4eybdu2Dq//2muvcfbZZ5OZmUlWVhbnnHMOixYtYsaMGSxcuJDrr7+eRYsWkZOTQ3Z2NmlpaVx66aU88sgjZGRk9EoZ46ZGkNBSI1AiEOkX9vWXe7RkZmbuef29732PBQsW8Oijj7J+/Xrmz58f9pzU1NQ9rxMTE2ls7Hi10o4WB5syZQpLly7l6aef5sYbb+Skk07i+9//PkuWLOGFF17ggQce4NZbb+XFF1/sXsFaiZ8agTqLRaSHysvLGTVqFAB33313r1zz2GOP5bHHHqOmpobq6moeffRRjjnmGDZv3kxGRgYXXngh11xzDcuWLaOqqory8nJOO+00fvnLX+7p3O6puKsRKBGISHddd911XHTRRdx8880cf/zxvXLNQw89lIsvvpg5c+YAcOmllzJr1iyeffZZrr32WhISEkhOTuZ3v/sdlZWVnHXWWdTV1eHu/OIXv+iVGPrdmsWzZ8/27ixM8/Lq7Vx819s88o15HDpmcBQiE5GeWrVqFQcddFCsw+j3wv0dzWypu4cd4xpHTUMtfQT9K/GJiERb3CSClvsIlAdERPYWN4nAaOkjiHEgItIp1dp7pjt/v7hJBJ/WCPQ/mUhflZaWxo4dO/TvtJta1iNIS0vbp/PiZtSQmWoEIn1dYWEhxcXFlJSUxDqUfqtlhbJ9EUeJIPSsXxoifVdycvI+rawlvSOOmoZa5hoSEZHW4iYR6M5iEZHw4iYRaPioiEh4cZMITFNMiIiEFT+JIHhWHhAR2VvcJIIELUwjIhJW3CQC01KVIiJhxU0i0PBREZHw4iYRaPioiEh48ZMI0DTUIiLhxE0iSAhKqjwgIrK3uEkEmoZaRCS8uEkEe+4sVnexiMhe4iYRaBpqEZHwukwEZnaumQ0KXv+nmT1iZodGP7TepWmoRUTCi6RG8D13rzSzo4GTgT8Bv+vqJDMbbWYvmdkqM1tpZt8Kc4yZ2S1mtsbM3o1mgtlzH4HygIjIXiJJBE3B8+nA79z9cSAlgvMagf9w94OAucA3zWxqm2NOBSYHj8uJIMF0V8tcQ7qPQERkb5Ekgk1m9nvgPOBpM0uN5Dx33+Luy4LXlcAqYFSbw84C7vGQt4BcMxuxTyWIkGoEIiLhRZIIzgOeBU5x911AHnDtvnyImY0DZgGL2+waBWxs9b6Y9skCM7vczIrMrKi7a5nqzmIRkfAiSQQjgL+7+0dmNh84F1gS6QeYWRbwMPDv7l7RdneYU9p9U7v77e4+291nFxQURPrRbeJouVa3ThcRGbAiSQQPA01mNgn4IzAeuC+Si5tZcnD+ve7+SJhDioHRrd4XApsjufa+0jTUIiLhRZIImt29ETgH+KW7f5tQLaFTFhq4/0dglbvf3MFhTwBfCUYPzQXK3X1LhLHvk0+bhqJxdRGR/ispgmMazOwLwFeAM4NtyRGcdxTwZeA9M1sebPsOMAbA3W8DngZOA9YANcAlEUe+j9RZLCISXiSJ4BLgX4Efu/s6MxsP/KWrk9z9NcL3AbQ+xoFvRhJoT6mzWEQkvEiGgb4PXEPol/10oNjdfxr1yHqZpqEWEQmvyxpBMFLoT8B6Qr/wR5vZRe7+alQj62WfTjonIiKtRdI09HPgJHdfDWBmU4D7gcOiGVhv2zPpnHqLRUT2EsmooeSWJADg7h8SWWdxn6IagYhIeJHUCIrM7I/An4P3XwKWRi+k6NA01CIi4UWSCK4gNLLnKkJ9BK8Cv4lmUNGgaahFRMLrMhG4+27g5uABgJm9Tug+gX5D9xGIiITX3RXKxvRqFPuBpqEWEQmvu4mg332bfjrXkIiItNZh05CZndPRLiA9OuFEj+4sFhEJr7M+gjM72fdUbwcSbZqGWkQkvA4TgbtHbQK4WPi0s1iZQESkte72EfQ7n3YWxzQMEZE+J24SgYaPioiEFzeJQJ3FIiLhdWfUEAAdLD3ZZ5n6CEREwopk1NBQYB7wYvB+AfAy0K8SAYQmnlMaEBHZW5ejhszsKWBqy1rCZjaCfjjXEIRqBWoaEhHZWyR9BOPaLCi/DZgSpXiiKsHUWSwi0lYks4++bGbPElqMxoELgJeiGlWUhGoEsY5CRKRviWT20SvN7Gzg2GDT7e7+aHTDig5DncUiIm1FUiMAeANoJFQjWBK9cKIrwUydxSIibXTZR2Bm5xH68v88cB6w2Mw+H+3AosFMaxaLiLQVSY3gu8Dh7r4dwMwKgIXA36IZWDSoRiAi0l4ko4YSWpJAYEeE5/U5ZrqzWESkrUhqBM+0GjUEcD7wdPRCip5QZ3GsoxAR6VsiGTV0bTDdxNGEvkv77aihhATTqCERkTYiHTX0OtBAPx81ZGgaahGRtuJq1FCos1iZQESktbgaNaQ7i0VE2ou7UUPqIxAR2VtcjRrSpHMiIu1FOmroc8BR9PNRQ4amoRYRaSuiUUPu/jDwcJRjiTrVCERE2otk1NA5ZvaRmZWbWYWZVZpZxf4Irreps1hEpL1IagQ/A85091XRDiba1FksItJeJKN/tg2EJACadE5EJJwOawTBtBIARWb2IPAYsLtlv7t3uni9md0JnAFsd/fpYfbPBx4H1gWbHnH3H+5D7PtMk86JiLTXWdPQma1e1wAntXrvQKeJALgbuBW4p5NjFrn7GV1cp9ckmKmzWESkjQ4Tgbtf0pMLu/urZjauJ9fobaG5hpQJRERa66xp6Dp3/5mZ/RraN627+1W98PlHmtk7wGbgGndf2UEslwOXA4wZM6bbH2YaPioi0k5nTUMtHcRFUfrsZcBYd68ys9MI9UFMDnegu98O3A4we/bsbn+Va9I5EZH2OmsaejJ4/lM0PtjdK1q9ftrMfmtm+e5eGo3Pg5Y1i6N1dRGR/qmzpqEnCdMk1MLdP9uTDzaz4YSGprqZzSE0lHVHT67ZFdUIRETa66xp6P96cmEzux+YD+SbWTHwX0AygLvfRmh9gyvMrBGoBS7w/XC3l+4sFhHZW2dNQ6+0vDazdGCMu6+O9MLu/oUu9t9KaHjpfhMaPqpMICLSWiRzDZ0JLAeeCd7PNLMnohxXVCQkaNSQiEhbkUwxcRMwB9gF4O7LgXHRCiiaNA21iEh7kSSCRncvj3ok+0GCddL7LSISpyKZfXSFmX0RSDSzycBVwBvRDStKNA21iEg7kdQI/g2YRmjCufuACuBb0QwqWhI0DbWISDuRJIIvuPt33f3w4PFd4AfRDiwaNOmciEh7kTQNfd7M6tz9XgAz+w2QFt2wokOTzomItBdJIjgHeMLMmoFTgTJ3/2Z0w4oO1QhERNrrbIqJvFZvLyU0KdzrwA/NLM/dy6IcW+/TwjQiIu10ViNYSmi0pbV6Pj14ODAh6tH1sgTTFBMiIm11NsXE+P0ZyP6QYEaTph8VEdlLZ01Dx7v7i63WLt5LV2sW90WmGoGISDudNQ0dB7zI3msXt4hkzeI+J8E0xYSISFudNQ39V/Dcbu1iM/tcNIOKltSkREqr6mMdhohInxLJDWXh/KJXo9hPslITqd7dGOswRET6lO4mAuvVKPaTjNQkauqVCEREWutuIuiXDe1ZqUlUqUYgIrKXzkYNvUf4L3wDhkUtoijKSEmkrqGZpmYnMaFfVmpERHpdZ6OGzthvUewnWamh4lbXN5KdlhzjaERE+obORg1t2J+B7A8ZKaHi1uxuUiIQEQl0t4+gX8pMTQRQP4GISCvxlQhaagQaOSQiskd8JYKgj0A1AhGRT3W5HkEHo4fKgSLgR+6+IxqBRUNL01DN7qYYRyIi0ndEsjDNP4AmQusVA1wQPFcAdxN+LqI+KbPVqCEREQmJJBEc5e5HtXr/npm97u5HmdmF0QosGvIzUzGDdaXVsQ5FRKTPiKSPIMvMjmh5Y2ZzgKzgbb/6aZ2Tkcys0bk8//42XLOQiogAkSWCS4E7zGydma0H7gAuNbNM4CfRDC4azp41ipWbK7jqgeW8tHo7dQ3qLxCR+NZl05C7vw3MMLMcwNx9V6vdf41WYNFy4dyxlFbV87tXPubJdzaTmZLICQcN4+jJ+UwsyGTGqFxSkuJqMJWIxDnrqokkSAD/BRwbbHoF+KG7l0c5trBmz57tRUVFPb5ObX0TS9aX8cyKLTy7chtl1Z+uU1AwKJWJBZlMLMgKPYZmMSE/k5G56ZqjSET6JTNb6u6zw+6LIBE8DKwA/hRs+jJwiLuHXcIy2norEbTW2NRM8c5aVm2p4MNtVRTvrGFtaTVrtldRXtuw57jkRGNkbjqFg9MZPTiD0XkZFA5OZ9yQTHIzkhmVm05SomoTItL39DQRLHf3mV1t21+ikQg64u6UVdfzcUk1H5dU8UlZDcU7a9lYVkPxzpp2q50lJxp5mSksOGAo4/MzGTskg7FDMhmfn0lacuJ+iVlEJJzOEkEkw0drzexod38tuNhRQG1vBthXmRlDslIZkpXKnPF57fbX1DfySVkN60trqKhtCGoRlTyzciu7ahpaXQey05I5ZHQueRnJFA7OYPqoHCYPy2L04Az1SYhITEWSCP4VuCfoKwDYCVwUvZD6j4yUJA4cns2Bw7Pb7SuvbeCTHTWs3xGqTbQ0Pa0rreLJd7fQ1ByqiSUmGLPHDubEqcOYNWYwU0dkk56i2oOI7D+RjBp6BzjEzLKD9xVm9u/Au1GOrV/LSU9mRmEOMwpz2u2ra2gKkkI1H26r4uXV2/nR31cBkGAwaWgWcycM4StHjmPS0Kx254uI9KYu+wjCnmT2ibuPiUI8XdqffQT709byOt7bVM57m8pZsamc19aUUt/YzBHj87hgzmhOnT5C/Qwi0m096izu4IIb3X10F8fcSWiVs+3uPj3MfgN+BZwG1AAXu/uyrj57oCaCtkqrdvPg2xv5a9FGNuyoYVBaEmfPGsWFc8cyZdigWIcnIv1MNBJBlzUCMzsWqALu6SARnAb8G6FEcATwK3c/ou1xbcVLImjR3OwsXlfGg29/wtMrtu6pJZw3ezSnzhi+Z9U1EZHOdCsRmFklHS9en+7ukUxhPQ54qoNE8HvgZXe/P3i/Gpjv7ls6u2a8JYLWyqrruX/JJ3tqCZkpiZxx8Ei+fORYpo9q3xchItKiW8NH3T3a7Q+jgI2t3hcH29olAjO7HLgcYMyYmHRN9Al5mSl8c8EkvjF/IkvWlfHwsmKeeGczDxZtpGBQKl84fDRHTsxn9rjBJOvGNhGJULeahiK+eOc1gr8DP2l1f8ILwHXuvrSza8ZzjSCciroGHl22iWdXbuWttTtodhiVm86Fc8dyzqGjGJadFusQRaQP6OkNZdFSDLTucC4ENscoln4rOy2Zi+aN46J54yivbeCNNaXc/cZ6/ueZD/jfZz/g6MkFnDe7kKMn5ZObkRLrcEWkD4plIngCuNLMHiDUWVzeVf+AdC4nPZlTZ4zg1BkjWFdazSPLinlk2SauvO+fJCUYJ08fzmcPGclxUwo0FFVE9oioacjMxgKT3X2hmaUDSe5e2cU59wPzgXxgG6EZTJMB3P22YPjorcAphIaPXuLuXbb5qGlo3zQ1O0vWlbFw1TYeXlbMrpoGctKTOXvWKI6cOISTpg4j9J9CRAaynk46dxmhjto8d59oZpOB29z9hN4PtWtKBN3X0NTM4rVl/PG1tby1tozahiYOHD6Is2eN4otHjGFQWnKsQxSRKOnx7KPAHGCxu88Ktr3n7jN6O9BIKBH0jqZm5+Glxdzz1npWbKogKcH4/GGFfP24iYzPz4x1eCLSy3raWbzb3etbmg/MLInw9xdIP5KYYJx3+GjOO3w0SzeU8dg/N3P/kk944O2NHDM5ny/PHcvxBw7V+goicSCSRPCKmX0HSDezE4FvAE9GNyzZnw4bm8dhY/P4t+Mn8cDbG7lv8Sdc/ueljMxJ4wtzxnD+nNEMHaRhqCIDVSRNQwnA14CTCN1V/Cxwh0fzBoROqGko+hqbmlm4ajv3Lt7Aoo9KSUowTpk+nC/PHcuc8XnqXBbph3raR3A28LS7745GcPtKiWD/WltSxb2LP+Ghoo1U1DUybWQ25x8+ms8fVqh5jkT6kZ4mgruA44FXgQeAZ929sdejjJASQWzU1jfx+PJN3PX6elZvqyQ3I5mvzB3LhUeOVbORSD/Q49lHzSwZOBU4HzgaeN7dL+3VKCOkRBB7SzeU8ftX1vL8qm0kJRhnHDySC+eO5dAxuWo2EumjemUa6iAZnAJcAhzj7gW9F2LklAj6jrUlVdzz5gb+trSYqt2NTB2RzX+efhBHThyihCDSx/S0aegU4AJgAfAy8CDwXKyah5QI+p6q3Y08+c5mfvPSGop31jJjVA5XnziF+QcUKCGI9BE9TQQPEOob+Edf6DBWIui76hqaeKhoI7cvWsvGsloOGzuY/zhpCvMm5sc6NJG41+srlMWSEkHf19DUzF+LNvLrF9awtaKOeROH8J3TDtLiOSIx1Fki6PC2UTNrWSeg0swqWj0qzawiWsFK/5ecmMCXjhjLy9fO53tnTOWDrZWceetrfPPeZeyoinmlUkTaUI1Aoq68toE7Fq3l96+uJTstmZ+fdwjHTYnJWAORuNWtGkGrk/8cyTaRjuSkJ/MfJx3AE1ceRV5mMhfduYQL71hMSaVqByJ9QSQzik1r/SaYdO6w6IQjA9mBw7N54sqjufbkAyjaUMZnbn6Fv7y1gf5WKxUZaDrrI7jRzCqBg1v3DxBaZObx/RahDChpyYl8c8EknrzyaKaNzOY/H1vBRXe9zcaymliHJhK3Ihk++hN3v3E/xdMl9REMHO7OX97awE/+8QHN7lx1wmQumTee9BQtoynS23pjionBwGRgz6Qy7v5qr0W4D5QIBp7Nu2r54ZPv88zKreSkJ/ObLx7K0ZN174FIb+ppZ/GlhCacexb4QfB8U28GKPFtZG46t335MB64fC4Fg1K58I+LueHhdymvaYh1aCJxIZLO4m8BhwMb3H0BMAsoiWpUEpfmThjCE1cexdePncBDS4s54eZXeGNNaazDEhnwIkkEde5eB2Bmqe7+AXBAdMOSeJWRksSNpx3E4988ipz0JC7842J++/Iamps1skgkWiJJBMVmlgs8BjxvZo8Dm6MZlMj0UTk8fuXRnDZjBD97ZjWX3VPExyVVsQ5LZEDapzuLzew4IAd4xt3roxZVJ9RZHF/cnbvfWM9P//EBZvDfZ03nnEMLSUzQrKYi+6KnncV5LQ/gPeA1QPV02S/MjEuOGs+i6xdw8Khcrv3bu3zlzsVsKa+NdWgiA0YkTUPLCHUOfwh8FLxeZ2bLzEx3GMt+MXRQGvdddgQ/PGsaS9aV8Zmfv8K9izeo70CkF0SSCJ4BTnP3fHcfQmjJyr8C3wB+G83gRFpLSkzgK0eO48X/mM+sMYP57qMr+OIdb+muZJEeiiQRzHb3Z1veuPtzwLHu/haQGrXIRDowOi+DP39tDj89ZwYrNlVw+i2L+NvSYtUORLopkkRQZmbXm9nY4HEdsNPMEoHmKMcnEpaZccGcMTx91TFMGprFNQ+9w7m/f5OVm8tjHZpIvxNJIvgiUEho+OhjwOhgWyJwXrQCE4nEmCEZ/O1f5/Gzzx3MutJqzvz1a/znY+9RvTsmS2qL9EsRDx81syx3j/lAbg0flY6U1zTwi4Ufcs+b68nLTOHbJ07hi3PGYKahpiI9HT46z8zeB94P3h9iZuoklj4nJyOZmz47jfsvm8ukoVl899EVnP3bN3hh1TbqGppiHZ5InxVJ09AvgJOBHQDu/g5wbDSDEumJIyYM4f7L5vKzzx1MSeVuvvanIub+5AVWbFL/gUg4kSQC3H1jm036eSV9mplx3uGjefna+fzqgpkkJSTw2Vtf498f+CfrSqtjHZ5In5IUwTEbzWwe4GaWAlwFrIpuWCK9IzkxgbNmjuKoSfn84dW13PPmBp56dwvnzi7ksmMmMKEgK9YhisRcJCuU5QO/Aj4DGPAc8C133xH98NpTZ7H0REnlbm554SMeLNoIDpceM54rj59ERkokv4lE+q8er1DWlygRSG8oqdzND596n6fe3Ux+VioXzxvHl44YQ25GSqxDE4mKbiUCM/t+J9d0d//vCD74FEK1iUTgDnf/aZv984HHgXXBpkfc/YedXVOJQHpT0foybnlxDa9+WEJ6ciKnTB/OFfMnMmXYoFiHJtKrOksEndWHw/WoZQJfA4YAnSaC4M7j3wAnAsXA22b2hLu/3+bQRe5+RmfXEomW2ePyuOerc1i1pYI/vraOZ1Zs5cl3NnPmISO58vhJTFQfgsSBDhOBu/+85bWZDSK0ZOUlwAPAzzs6r5U5wBp3Xxtc4wHgLIL7EUT6koNGZPN/5x7CNScdwG2vfMyDb2/kseWbOOHAYVx2zHjmjM/TjWkyYHU6fDRYh+BHwLuEksah7n69u2+P4NqjgNbDTouDbW0daWbvmNk/zGxaB3FcbmZFZlZUUqLlkiV6huekcdNnp/HqdQv4twWTWLqhjPNvf4vP3vo6jy/fREOTpteSgafDRGBm/wu8DVQCM9z9JnffuQ/XDvfzqW2HxDJgrLsfAvya0FxG7U9yv93dZ7v77IKCgn0IQaR7CgalcvVJB/DGDSfw47OnU13fyLceWM6xP3uJ37/yMeW1DbEOUaTXdNZZ3AzsBhrZ+wvcCHUWZ3d6YbMjgZvc/eTg/Y2ETvxJJ+esJzTtdWlHx6izWGKhudl5afV27li0jjfX7iAzJZEjJ+bzjQUTOXTM4FiHJ9KlbnUWu3tEdx134m1gspmNBzYBFxCatbR1YMOBbe7uZjaHUA0lJvcniHQmIcE44aBhnHDQMFZsKufO19fx/PvbeGn1do6cMIQr5k9k7oQhWktZ+qWo3UXj7o1mdiXwLKHho3e6+0oz+9dg/23A54ErzKwRqAUu8P52Y4PEnemjcrj5vJmUVdfzh0VreaiomC/dsZgROWmce1ghJxw0jENG58Y6TJGI6YYykR4qr23glQ9LeKhoI4s+CrVqFg5O5/JjJ/C5QwvJTNVdyxJ7urNYZD8prdrNn9/cwH1LPqGkcjf5Wamcf3ghx0wu4LCxg0lO7GmLq0j3KBGI7Gf1jc0sWVfGXa+v48XV23GHgwtzOG/2aE6aOoyh2WmxDlHijBKBSAxt3lXLsyu3cs+bG1hXWo0ZHFyYy9wJeVx05DhG5qbHOkSJA0oEIn2Au/PR9iqeXbGVVz4sYfnGXTQ2OydPG8alx0xg5uhcNR1J1CgRiPRBG8tqeODtT/jTGxuo2t1IdloSJ04dzmkzhnP05HxSkxJjHaIMIEoEIn1YeW0Dr68pZeGqbTz//jYq6xrJSk1ixqgczp1dyHFTChiSlRrrMKWfUyIQ6SfqG5t5/eNSnlu5jcXrdrC2JDQJ8JRhWXzmoGEcO6WAmaNzSUtWbUH2jRKBSD/k7ry9fifLN+7k2ZXbWL5xF03NzqC0JGaOzmVEThqnHzySqSOyKRikGoN0TolAZACorGvg9TU7eOmD7SxctY0d1fUApCQlcMykfEbnZTBrTC7zJuYrMUg7SgQiA9DO6npWbC7nqXe2ULShjI1ltdQ3NZOYYEwemsVxUwrIzUhh5uhcDi7M0R3Oca67K5SJSB82ODOFYyYXcMzk0NTsdQ1NrNlexXMrt/Lc+9v4/atr9xybkpjAidOGMW1kNgcNDzUlTRuZrcV2BFCNQGRAamhqZmNZDXmZKRSt38mij0p46t0te5qTADJTEjnugALmjMtjyvBBbNpZy5mHjFRH9AClpiERAUJzIX24rZJNO2tZumEnT7yzmZr6pj37R+elc8CwbA4bO5hJQ7PIzUhm2shsMlLUeNDfKRGISFjltQ1s2lnLh9sq2bCjhvc27WJtSTVrS6v3HJOUYAzLTmNCQSbTR+UwfWQO00dlMyYvQ01L/Yj6CEQkrJz0ZHLSk5k6cu8FBzeW1bBhRw3V9Y2s2FRO8c5aVm+t5A+vrqWxOfTjMT05keREo3BwBmceMpKRuWnkZaYwoSCLkTlpShL9iGoEIhKx3Y1NfLi1ipWby/lgayXVuxt5c+0OinfW7nXcsOxUhmWnMXnoIJqamxmdl8HwnDSOnpSvmkSMqEYgIr0iNSmRGYU5zCjM2bOtqdnZWlFHSeVuymsbWF9azbJPdrK+tJrn39+KA5V1jXuOHzooFQfG52cyIieNETnpDEpL4uDCHA4cns3gjGSaHRITjMQEw92VOKJMNQIRiaqmZqexuZmPtlWxfOMu3vx4B0mJRtH6nZjBtoo6Gpo+/R5KMEgwIy05kbTkRNydk6YNIzs9mcPGDAbgwOHZDM9Jw3FKKnczKjddyaIL6iwWkT6rsamZ2oYmitbvZOPOGraW11FR14BhvLupnHeLdxHuayopwUhOTKC2oYnReenMHT+EycOySEpIIC8zhZG56ZjBAcMHkZWSRLM7CWYkJMRnwlDTkIj0WUmJCQxKTGDBgUPD7m/5sbq2tJqK2gZ2NzazrrSa4p017KxpYGROGss37uKl1SU8tLQ4/GckGM3uDM5IYcqwQaSnJNLQ1ExGSiLHTikgJz2Z0srdDM9JZ0ROGoWD06ltaCI/K5WUxASa3UkawGtFKBGISJ/W0uQzsSBrz7a5E4aEPXZXTT3NDmXV9Xy4rZLy2gbKquv5ZEcNCQlGSeVudtXUs6u8gdr6RhqanGdXbusyhpSkBGaMyiEpwUhJSuDYyQVU1DWwvWI3O6rrOf7AoeRnpVDb0MTQQWkcOHwQiYnGll11FA5O32t6j77Y56FEICIDRm5GCgB5mSlMGprVxdGhL+WVmytISjTyMlPYWl7H5l11fFxSRVOzs2lnLbUNTZTXNlDf2EyTO2tLqln0USmJCcaQzBR21TawcFX7ZGLGniat7LQkZo0ZTFl1PR9tr+SwsYNZX1rDtJHZlFTtJi0pkXNnF5KcmMCmXbW8+fEOZo3JJSnBOGX6CEbnhZYzjdZiReojEBHZB+5OeW0DZkZOejJ1DU18tK2KyrrQtoamZlZtqaCsup70lETqG5tZs72KlZsryM1I5oBhg3ineBcfl1R3+BmDUpOoqm/Eg9FTTc1ORkoiV584hUuPmdCtuNVHICLSS8xsT80DIC05ca/htADHTino8jqNTc1U1DUyOCOZuoZmtpTXUtfQzNINZZw4dTipSQmsKali4aptZKUk8UlZDSNz03u9PKBEICISE0mJodFNAOkpiUwI+kBa3+V9eGYeh4/Li3osA7cbXEREIqJEICIS55QIRETinBKBiEicUyIQEYlzSgQiInFOiUBEJM4pEYiIxLl+N8WEmZUAG7p5ej5Q2ovh9Acqc3xQmeNDT8o81t3D3vLc7xJBT5hZUUdzbQxUKnN8UJnjQ7TKrKYhEZE4p0QgIhLn4i0R3B7rAGJAZY4PKnN8iEqZ46qPQERE2ou3GoGIiLShRCAiEufiJhGY2SlmttrM1pjZDbGOp7eY2Z1mtt3MVrTalmdmz5vZR8Hz4Fb7bgz+BqvN7OTYRN0zZjbazF4ys1VmttLMvhVsH7DlNrM0M1tiZu8EZf5BsH3AlhnAzBLN7J9m9lTwfkCXF8DM1pvZe2a23MyKgm3RLbe7D/gHkAh8DEwAUoB3gKmxjquXynYscCiwotW2nwE3BK9vAP4neD01KHsqMD74myTGugzdKPMI4NDg9SDgw6BsA7bcgAFZwetkYDEwdyCXOSjH1cB9wFPB+wFd3qAs64H8NtuiWu54qRHMAda4+1p3rwceAM6KcUy9wt1fBcrabD4L+FPw+k/Av7Ta/oC773b3dcAaQn+bfsXdt7j7suB1JbAKGMUALreHVAVvk4OHM4DLbGaFwOnAHa02D9jydiGq5Y6XRDAK2NjqfXGwbaAa5u5bIPSlCQwNtg+4v4OZjQNmEfqFPKDLHTSTLAe2A8+7+0Av8y+B64DmVtsGcnlbOPCcmS01s8uDbVEtd7wsXm9htsXjuNkB9XcwsyzgYeDf3b3CLFzxQoeG2dbvyu3uTcBMM8sFHjWz6Z0c3q/LbGZnANvdfamZzY/klDDb+k152zjK3Teb2VDgeTP7oJNje6Xc8VIjKAZGt3pfCGyOUSz7wzYzGwEQPG8Ptg+Yv4OZJRNKAve6+yPB5gFfbgB33wW8DJzCwC3zUcBnzWw9oabc483sLwzc8u7h7puD5+3Ao4SaeqJa7nhJBG8Dk81svJmlABcAT8Q4pmh6ArgoeH0R8Hir7ReYWaqZjQcmA0tiEF+PWOin/x+BVe5+c6tdA7bcZlYQ1AQws3TgM8AHDNAyu/uN7l7o7uMI/Xt90d0vZICWt4WZZZrZoJbXwEnACqJd7lj3kO/HnvjTCI0u+Rj4bqzj6cVy3Q9sARoI/Tr4GjAEeAH4KHjOa3X8d4O/wWrg1FjH380yH02o+vsusDx4nDaQyw0cDPwzKPMK4PvB9gFb5lblmM+no4YGdHkJjWx8J3isbPmuina5NcWEiEici5emIRER6YASgYhInFMiEBGJc0oEIiJxTolARCTOKRGItGFmTcHMjy2PXput1szGtZ4pVqQviJcpJkT2Ra27z4x1ECL7i2oEIhEK5on/n2BdgCVmNinYPtbMXjCzd4PnMcH2YWb2aLCGwDtmNi+4VKKZ/SFYV+C54E5hkZhRIhBpL71N09D5rfZVuPsc4FZCs2MSvL7H3Q8G7gVuCbbfArzi7ocQWjNiZbB9MvAbd58G7AI+F9XSiHRBdxaLtGFmVe6eFWb7euB4d18bTHq31d2HmFkpMMLdG4LtW9w938xKgEJ3393qGuMITSE9OXh/PZDs7j/aD0UTCUs1ApF94x287uiYcHa3et2E+uokxpQIRPbN+a2e3wxev0FohkyALwGvBa9fAK6APYvKZO+vIEX2hX6JiLSXHqwE1uIZd28ZQppqZosJ/Yj6QrDtKuBOM7sWKAEuCbZ/C7jdzL5G6Jf/FYRmihXpU9RHIBKhoI9gtruXxjoWkd6kpiERkTinGoGISJxTjUBEJM4pEYiIxDklAhGROKdEICIS55QIRETi3P8HstKMmYyYqf8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train NLL loss: 0.18756346192770024\n",
      "Accuracy ratio: 475/500\n",
      "Accuracy: 0.95\n",
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "\n",
    "def TEST_neural_network():\n",
    "\n",
    "    def onehot(y):\n",
    "        n_values = np.max(y) + 1\n",
    "        oh = np.eye(n_values)[y.flatten()]\n",
    "        return oh\n",
    "\n",
    "    nn_clf = NeuralNetwork(\n",
    "        neurons_per_layer=[10, 10, 10, 3],\n",
    "        g_hidden=ReLU,\n",
    "        g_output=Softmax,\n",
    "        batch_size=64,\n",
    "        epochs=500,\n",
    "        alpha=.01,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    X, y = make_gaussian_quantiles(\n",
    "        cov=1,\n",
    "       n_samples=500,\n",
    "       n_features=3,\n",
    "       n_classes=3,\n",
    "       random_state=42\n",
    "    )\n",
    "\n",
    "    y = onehot(y)\n",
    "    ss = Standardization()\n",
    "    X = ss.fit_transform(X)\n",
    "\n",
    "    nn_clf.fit(X, y)\n",
    "    y_hat = nn_clf.predict(X)\n",
    "\n",
    "\n",
    "    plt.plot(nn_clf.avg_trn_loss_tracker, label='Train loss')\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.ylabel(\"Negative Log Likelihood Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    y_hat_probs = nn_clf.predict_proba(X)\n",
    "    nll_loss = nll(y=y, y_hat_probs=y_hat_probs) / len(y)\n",
    "    print(f\"Final train NLL loss: {nll_loss}\")\n",
    "    acc = accuracy(y=y, y_hat=y_hat)\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "\n",
    "    todo_check([\n",
    "        (np.all(np.isclose(nn_clf.nn[0].W[0, :3],np.array([0.04373074, 0.60476897, 0.14050529]),rtol=.01)), \"Weight values for the 1st hidden layer are incorrect\"),\n",
    "        (np.isclose(nll_loss, 0.1875, rtol=.01), 'nll_loss is incorrect'),\n",
    "    ])\n",
    "\n",
    "\n",
    "TEST_neural_network()\n",
    "garbage_collect(['TEST_neural_network'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7a1e11",
   "metadata": {
    "id": "8d7a1e11"
   },
   "source": [
    "# Non-linear Multi-Class Classification\n",
    "\n",
    "Finally, onto training! Let's now use the full Sign Language MNIST dataset to test the our neural network implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd91229",
   "metadata": {
    "id": "8bd91229"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8896dc69",
   "metadata": {
    "id": "8896dc69"
   },
   "source": [
    "###  Cross validation and Hyper-parameter Tuning\n",
    "\n",
    "Training this time around is going to be different. In this lab, we are going to be using *cross validation*. Doing so will help us to find the optimal hyper-parameters for our neural network.\n",
    "\n",
    "#### K-cross validation\n",
    "\n",
    "To understand how cross validation with hyper-parameter tuning works, we first need to understand *[K-fold cross validation](https://machinelearningmastery.com/k-fold-cross-validation/)* which acts as the core concept that cross validation is built upon.\n",
    "\n",
    "The idea of k-fold validation is to split your dataset into $k$ folds. Each fold then takes turns acting as the test dataset while the rest are combined to act as the training dataset. We do this for all combinations of folds. This means each fold acts as the test dataset once. We refer to the different combinations of training and test datasets as *splits*.\n",
    "\n",
    "The below image is an example of using $k=5$ such that we split the training data into 5 folds. Thus, we train the model using 5 different splits. Meaning, we train the model 5 times, once for each unique test dataset.\n",
    "\n",
    "Once k-fold validation is done, we compute the average over ALL test metrics. Doing so gives us a good idea how of our model (i.e., neural network in this case) will perform when using different training and test data.\n",
    "\n",
    "\n",
    "#### Cross validation with Hyper-parameter Tuning\n",
    "[*Hyper-parameter cross validation*](https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/) uses k-cross validation with hyper-parameter tuning. Hyper-parameter tuning entails training using different combinations of hyper-parameters where the goal is to find \"optimal\" hyper-parameters for the current problem.\n",
    "\n",
    "One form of cross validation modifies k-cross validation using *grid search*. Grid search works by training a model multiple times using all combinations of any specified hyper-parameters. Thus, cross validation with grid search performs a grid search over hyper-parameters for each split (see the below picture).\n",
    "\n",
    "For example, say we had $k=5$ and we wanted to choose the best learning rate using the values {.01, .001} and best mini-batch size using the values {32, 64, 128}. Each split would need to train the model 6 times as we have 2 learning rates values and 3 mini-batch size values, leading to $2*3=6$ combinations of hyper-parameters to search over. Further, we would have to train the model 30 times in total as there are 5 splits and 6 different hyper-parameter combinations such that $5*6=30$.\n",
    "\n",
    "While cross validation can help us find the optimal hyper-parameters, it can be very expensive as you add more and more hyper-parameters to test. Doing so, means the number of hyper-parameter combinations to search over grows as well. As you might imagine, this becomes very computationally expensive.\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fscikit-learn.org%2Fstable%2F_images%2Fgrid_search_cross_validation.png&f=1&nofb=1\" width=400 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e944d2b",
   "metadata": {
    "id": "1e944d2b"
   },
   "source": [
    "#### TODO 14\n",
    "Complete this TODO by finishing the `run_neural_network_cv()` which performs cross validation with grid search by utilizing Sklearn's `GridSearchCV` class.\n",
    "\n",
    "**Initialize a dummy `NeuralNetwork` class instance**\n",
    "\n",
    "Before we can create the `GridSearchCV` class, the `GridSearchCV` requires that we initialize a dummy instance of our model/estimator. In this case, our model is a neural network which we defined in the `NeuralNetwork` class above. To make a dummy instance we simply initialize the `NeuralNetwork` class as we would normally, however the values for the arguments we pass don't matter as the `GridSearchCV` class will override them later on. Thus, we can simply pass `None` or any random argument values to any required arguments for our `NeuralNetwork` class.\n",
    "\n",
    "1. Initialize a dummy/placeholder instance of the `NeuralNetwork` class by passing `None` to all required arguments. Store the output into `nn_clf`.\n",
    "\n",
    "\n",
    "**Initialize `Scorer` class instance**\n",
    "\n",
    "A `Scorer` is a Sklearn specific class that wraps performance metrics like accuracy or a loss function so that they are compatible with `GridSearchCV`. `GridSearchCV` then uses these `Scorer` class instances to measure the performance of each split and hyper-parameter combination.\n",
    "\n",
    "2. Initialize an instance of Sklearn's `Scorer` class using the `make_scorer()` function ([docs](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html)). To do so, pass the arguments that meet the following description:\n",
    "    1. Pass a **reference** of the `mean_nll` function to the `score_func` argument. We will use the mean of the NLL loss computed by the `mean_nll()` function to measure the performance of the model for each split.\n",
    "    1. Pass the argument that indicates that `score_func` requires the use of the `predict_proba()` method. Recall, the  `predict_proba()` in the `NeuralNetwork` class will return the probabilities.\n",
    "    1. Pass the argument that indicates `score_func` is a loss function so that a lower score will indicate the model is performing better. Meaning, greater loss is NOT better, it is worse!\n",
    "        1. Note: By passing this argument the scores output by the `GridSearchCV` will actually be negative!\n",
    "    \n",
    "**Initialize `GridSearchCV` class instance**\n",
    "\n",
    "`GridSearchCV` performs an exhaustive search over specified parameter values for our estimator `NeuralNeworks`. The specified parameters are passed to the `param_grid` argument when initializing a `GridSearchCV` class instance.\n",
    "\n",
    "3. Finish the initialization of the instance for Sklearn's `GridSearchCV` class ([docs](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)). Store the output into `gscv`. To do so, pass the arguments that meet the following description:\n",
    "    1. Pass our placeholder/dummy `NeuralNetwork` class instance to the `estimator` argument which we defined in TODO 14.1.\n",
    "    2. Pass the `nn_param_grid` argument taken in by our `run_neural_network_cv()` function to the `param_grid` argument. `nn_param_grid` will contain the hyper-parameters to perform the grid search over.\n",
    "    3. Pass our `Scorer` class instance to the `scoring` argument.\n",
    "    4. Tell `GridSearchCV` to split our data into 2 folds or $k=2$ by setting the `cv` argument.\n",
    "    \n",
    "**Defining hyper-parameters to search**\n",
    "\n",
    "When defining `nn_param_grid`, which defines the hyper-parameters to search over, for our `NeuralNetwork` class you have to follow the rules given below:\n",
    "\n",
    "- First, `nn_param_grid` must be a dictionary where the keys correspond to the string names of the arguments taken in by the `NeuralNetwork.__init__()` method.\n",
    "- Second, the values of the dictionary must be a **list** as each element in the list will be a value used by the grid search. If you simply want `GridSearchCV` to use a single value for a particular hyper-parameter then pass a list with one element.\n",
    "\n",
    "Below is an example of how you could set `nn_param_grid`. Notice, each key corresponds to an argument in `NeuralNetwork.__init__()`. Further notice, all values are lists and some values such as  `neurons_per_layer`, `batch_size`, and `epochs` have multiple elements. Each element corresponds to a new value for the hyper-parameter that will be used in the grid search. Thus, each split would train our `NeuralNetwork` class 8 times as there is a total 8 hyper-parameter combinations ($2*2*2=8$).\n",
    "    \n",
    "```Python\n",
    "nn_param_grid = dict(\n",
    "    neurons_per_layer=[[5, 5, 24], [20, 24]],\n",
    "    g_hidden=[ReLU],\n",
    "    g_output=[Softmax],\n",
    "    batch_size=[64, 256],\n",
    "    epochs=[15, 40],\n",
    "    alpha=[.01]\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "4. Fill in the dictionary of keyword arguments that correspond to the `NeuralNetwork` class. Select hyper-parameters YOU want to try searching over. Store the output into `nn_param_grid`.\n",
    "    1. Hint: Try to limit the number of hyper-parameters you search over. The more values you try searching over the longer cross validation will take!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0640ae50",
   "metadata": {
    "id": "0640ae50"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def mean_nll(y, y_hat_probs):\n",
    "    \"\"\" Computes the mean loss of the NLL loss function\"\"\"\n",
    "    return nll(y=y, y_hat_probs=y_hat_probs) / len(y)\n",
    "\n",
    "def run_neural_network_cv(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    nn_param_grid: dict\n",
    ") -> GridSearchCV:\n",
    "    \"\"\" Perform cross validation with hyper-parameter tuning using the NeuralNetwork class\n",
    "\n",
    "        Args:\n",
    "            X: Data to be used for nest cross validation\n",
    "\n",
    "            y: Labels which correspond to the data\n",
    "\n",
    "            nn_param_grid:  Dictionary containg the hyper-parameters\n",
    "                to perform the grid search over. These correspond\n",
    "                to the arguments taken in by the\n",
    "                NeuralNetwork.__init__() method.\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO 14.1\n",
    "    nn_clf = NeuralNetwork(neurons_per_layer=None, g_hidden=None, g_output=None)\n",
    "    # TODO 14.2\n",
    "    mean_nll_scorer = make_scorer(score_func=mean_nll, response_method='predict_proba', greater_is_better=False)\n",
    "    # TODO 14.3\n",
    "    gscv = GridSearchCV(estimator=nn_clf, param_grid=nn_param_grid, scoring=mean_nll_scorer, cv=2)\n",
    "    gscv.fit(X_trn, y_trn)\n",
    "\n",
    "    return gscv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ce7ef1fa",
   "metadata": {
    "id": "ce7ef1fa",
    "outputId": "e9b35fef-69ad-4a47-98d2-80a9b6d81c10",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "# TODO 14.4\n",
    "nn_param_grid = dict(neurons_per_layer=[[10, 10, 24], [20, 24]], g_hidden=[ReLU], g_output=[Softmax], batch_size=[128], epochs=[20], alpha=[0.01])\n",
    "gscv = run_neural_network_cv(\n",
    "    X=X_trn,\n",
    "    y=y_trn,\n",
    "    nn_param_grid=nn_param_grid\n",
    ")\n",
    "\n",
    "todo_check([\n",
    "    (np.all([isinstance(v, (list, tuple)) for v in nn_param_grid.values()]), \"nn_param_grid keys whose values are lists or tuples.\"),\n",
    "    (isinstance(gscv.estimator, NeuralNetwork), \"gscv.estimator does not contain an instance of our NeuralNetwork class. \"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec45060a",
   "metadata": {
    "id": "ec45060a"
   },
   "source": [
    "The results of the Sklearn's `GridSearchCV` are stored in the `cv_results_` variable as a dictionary. You can read about how properly understand thee `cv_results_` variable output by looking at the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "95ddb2d7",
   "metadata": {
    "id": "95ddb2d7",
    "outputId": "b6500b73-a6aa-4aab-c172-0b690c1a5a18"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([4.45810997, 5.41112447]),\n",
       " 'std_fit_time': array([1.09363425, 1.44068766]),\n",
       " 'mean_score_time': array([0.00233424, 0.00234926]),\n",
       " 'std_score_time': array([0.00046217, 0.00059998]),\n",
       " 'param_alpha': masked_array(data=[0.01, 0.01],\n",
       "              mask=[False, False],\n",
       "        fill_value=1e+20),\n",
       " 'param_batch_size': masked_array(data=[128, 128],\n",
       "              mask=[False, False],\n",
       "        fill_value=999999),\n",
       " 'param_epochs': masked_array(data=[20, 20],\n",
       "              mask=[False, False],\n",
       "        fill_value=999999),\n",
       " 'param_g_hidden': masked_array(data=[<class '__main__.ReLU'>, <class '__main__.ReLU'>],\n",
       "              mask=[False, False],\n",
       "        fill_value=np.str_('?'),\n",
       "             dtype=object),\n",
       " 'param_g_output': masked_array(data=[<class '__main__.Softmax'>, <class '__main__.Softmax'>],\n",
       "              mask=[False, False],\n",
       "        fill_value=np.str_('?'),\n",
       "             dtype=object),\n",
       " 'param_neurons_per_layer': masked_array(data=[list([10, 10, 24]), list([20, 24])],\n",
       "              mask=[False, False],\n",
       "        fill_value=np.str_('?'),\n",
       "             dtype=object),\n",
       " 'params': [{'alpha': 0.01,\n",
       "   'batch_size': 128,\n",
       "   'epochs': 20,\n",
       "   'g_hidden': __main__.ReLU,\n",
       "   'g_output': __main__.Softmax,\n",
       "   'neurons_per_layer': [10, 10, 24]},\n",
       "  {'alpha': 0.01,\n",
       "   'batch_size': 128,\n",
       "   'epochs': 20,\n",
       "   'g_hidden': __main__.ReLU,\n",
       "   'g_output': __main__.Softmax,\n",
       "   'neurons_per_layer': [20, 24]}],\n",
       " 'split0_test_score': array([nan, nan]),\n",
       " 'split1_test_score': array([nan, nan]),\n",
       " 'mean_test_score': array([nan, nan]),\n",
       " 'std_test_score': array([nan, nan]),\n",
       " 'rank_test_score': array([1, 1], dtype=int32)}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a635e99",
   "metadata": {
    "id": "1a635e99"
   },
   "source": [
    "However, notice reading these results is really hard. We can format these results in a more digestible form by converting them into a Pandas DataFrame. The below function `format_results()` does exactly this. Further, it applies so additional formating steps for you to make results easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4c60f2a0",
   "metadata": {
    "id": "4c60f2a0"
   },
   "outputs": [],
   "source": [
    "def format_results(search_results):\n",
    "    def get_name(obj):\n",
    "        try:\n",
    "            if hasattr(obj, '__name__'):\n",
    "                return obj.__name__\n",
    "            elif hasattr(obj, '_name'):\n",
    "                return obj._name\n",
    "            elif hasattr(obj, 'name'):\n",
    "                return obj.name\n",
    "            else:\n",
    "                return obj\n",
    "        except Exception as e:\n",
    "            return obj\n",
    "\n",
    "    def find_name(objs):\n",
    "        if isinstance(objs, (tuple, list)):\n",
    "            obj_names = []\n",
    "            for obj in objs:\n",
    "                name = get_name(obj)\n",
    "                obj_names.append(name)\n",
    "            return obj_names\n",
    "        else:\n",
    "            return get_name(objs)\n",
    "\n",
    "    df = pd.DataFrame(search_results)\n",
    "    # Remove train related scores\n",
    "    df.drop(list(df.filter(regex='train')), axis=1, inplace=True)\n",
    "    # Sort results performance rank\n",
    "    df.sort_values('rank_test_score', axis=0, inplace=True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    # Convert object references to readable string names\n",
    "    df = df.applymap(find_name)\n",
    "    # Remove params column\n",
    "    df.drop('params', axis=1, inplace=True)\n",
    "    # Move rank_test_score to the first column\n",
    "    rts = df.pop('rank_test_score')\n",
    "    df.insert(0, rts.name, rts)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add2b09f",
   "metadata": {
    "id": "add2b09f"
   },
   "source": [
    "Below we format `gscv.cv_results_` using the `format_results()` function and store the output into `search_results`. Recall, `search_results` is a DataFrame and can be read as follows:\n",
    "\n",
    "- The `rank_test_score` column gives the rank for the performance of the hyper-parameter combination. Here the value of '1' indicates the hyper-parameter combination with the lowest mean test NLL loss computed over all splits. Further notice, the `search_results` DataFrame is sorted by rank.\n",
    "\n",
    "- Any column with the `_time` postfix provides statistics about how long it took to train and test using ALL splits for a given hyper-parameter combination.\n",
    "\n",
    "- Any column with the `param_` prefix denotes the hyper-parameter value for a given hyper-parameter combination.\n",
    "\n",
    "- Any column with the `split` prefix provides statistics regarding the performance of a given hyper-parameter combination for a particular split.\n",
    "\n",
    "- The `mean_test_score` reports the average test score over ALL splits for a given hyper-parameter combination. This is score that rank is determined by and the score you want to pay the most attention to!\n",
    "    - Recall test scores correspond to the metric passed to the `scoring` argument when initializing a `GridSearchCV` class instance. In our case, our test score is the mean NLL loss.\n",
    "\n",
    "- The `std_test_score` reports the standard deviation for `mean_test_score`.\n",
    "\n",
    "\n",
    "Notice, the mean test scores are all negative! This is because we specified the `greater_is_better=False` argument when defining our `scorer` variable. By doing so, Sklearn changes all scores to negative, This is just a weird and confusing thing Sklearn does that relates to making generalizable code. However, by making all the scores negative it just means when you take the max, the largest negative, in turn smallest loss, is the value closest to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "aa2178cb",
   "metadata": {
    "id": "aa2178cb",
    "outputId": "42d66333-6c7f-4125-9bd8-8257833f661e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JN\\AppData\\Local\\Temp\\ipykernel_11812\\1620407533.py:32: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(find_name)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>param_epochs</th>\n",
       "      <th>param_g_hidden</th>\n",
       "      <th>param_g_output</th>\n",
       "      <th>param_neurons_per_layer</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4.458110</td>\n",
       "      <td>1.093634</td>\n",
       "      <td>0.002334</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.01</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>Softmax</td>\n",
       "      <td>[10, 10, 24]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.411124</td>\n",
       "      <td>1.440688</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.01</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>Softmax</td>\n",
       "      <td>[20, 24]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank_test_score  mean_fit_time  std_fit_time  mean_score_time  \\\n",
       "0                1       4.458110      1.093634         0.002334   \n",
       "1                1       5.411124      1.440688         0.002349   \n",
       "\n",
       "   std_score_time  param_alpha  param_batch_size  param_epochs param_g_hidden  \\\n",
       "0        0.000462         0.01               128            20           ReLU   \n",
       "1        0.000600         0.01               128            20           ReLU   \n",
       "\n",
       "  param_g_output param_neurons_per_layer  split0_test_score  \\\n",
       "0        Softmax            [10, 10, 24]                NaN   \n",
       "1        Softmax                [20, 24]                NaN   \n",
       "\n",
       "   split1_test_score  mean_test_score  std_test_score  \n",
       "0                NaN              NaN             NaN  \n",
       "1                NaN              NaN             NaN  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results = format_results(gscv.cv_results_)\n",
    "search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84b5459",
   "metadata": {
    "id": "a84b5459"
   },
   "source": [
    "## Validation Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff6cf69",
   "metadata": {
    "id": "2ff6cf69"
   },
   "source": [
    "#### TODO 15\n",
    "Using the nest cross validation above, complete this TODO by selecting the hyper-parameters and training a neural network that produces a validation NLL loss below 0.25.\n",
    "\n",
    "1. Fill in the `nn_hyper_params` dictionary with the hyper-parameters for our `NeuralNetwork` class that YOU think will allow achieve a validation NLL loss below 0.25. Each key of the dictionary should match the keyword argument in the `NeuralNetwork.__init__()` method.\n",
    "    1. Hint: Use the learning curve plot that is plotted at the end of the training to help select how many epochs you need to prevent overfitting. Refer to this [post](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/).\n",
    "    1. Note: Recall, a dictionary's keys and values can be \"unloaded\" into class or function to act as keyword arguments (see this [post](https://www.pythontutorial.net/python-basics/python-kwargs/) for more details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "593edb1b",
   "metadata": {
    "id": "593edb1b",
    "outputId": "b3fe8420-d054-425d-aa28-68bf112f5d79",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\tTraining loss: 5.179492297257623\n",
      "\tValidation loss: 2.8719202565727664\n",
      "Epoch: 2\n",
      "\tTraining loss: 1.8897999573653248\n",
      "\tValidation loss: 1.4886420237256368\n",
      "Epoch: 3\n",
      "\tTraining loss: 1.0517345955309547\n",
      "\tValidation loss: 0.954198773389529\n",
      "Epoch: 4\n",
      "\tTraining loss: 0.6769463057129862\n",
      "\tValidation loss: 0.6703681244231777\n",
      "Epoch: 5\n",
      "\tTraining loss: 0.4716972847268762\n",
      "\tValidation loss: 0.4938235786551354\n",
      "Epoch: 6\n",
      "\tTraining loss: 0.3447267755992988\n",
      "\tValidation loss: 0.37330274562818977\n",
      "Epoch: 7\n",
      "\tTraining loss: 0.2570484246797157\n",
      "\tValidation loss: 0.28874346697669107\n",
      "Epoch: 8\n",
      "\tTraining loss: 0.1976260240185804\n",
      "\tValidation loss: 0.2289389063575772\n",
      "Epoch: 9\n",
      "\tTraining loss: 0.1543674793696976\n",
      "\tValidation loss: 0.18396396959257483\n",
      "Epoch: 10\n",
      "\tTraining loss: 0.12266912975061048\n",
      "\tValidation loss: 0.14896642089645343\n",
      "Epoch: 11\n",
      "\tTraining loss: 0.09878286309486214\n",
      "\tValidation loss: 0.12039199383520349\n",
      "Epoch: 12\n",
      "\tTraining loss: 0.08001890706257328\n",
      "\tValidation loss: 0.10076642301027237\n",
      "Epoch: 13\n",
      "\tTraining loss: 0.06565461907265506\n",
      "\tValidation loss: 0.08293409601183263\n",
      "Epoch: 14\n",
      "\tTraining loss: 0.054530087693644876\n",
      "\tValidation loss: 0.07134175817787171\n",
      "Epoch: 15\n",
      "\tTraining loss: 0.04603977215319549\n",
      "\tValidation loss: 0.061336681486392194\n",
      "Epoch: 16\n",
      "\tTraining loss: 0.03950166224351769\n",
      "\tValidation loss: 0.053636162624847085\n",
      "Epoch: 17\n",
      "\tTraining loss: 0.0341772488444872\n",
      "\tValidation loss: 0.04757951730392905\n",
      "Epoch: 18\n",
      "\tTraining loss: 0.030141173725035778\n",
      "\tValidation loss: 0.04296912765115658\n",
      "Epoch: 19\n",
      "\tTraining loss: 0.026804936466097756\n",
      "\tValidation loss: 0.039276617455836056\n",
      "Epoch: 20\n",
      "\tTraining loss: 0.023973324496843217\n",
      "\tValidation loss: 0.035705656743511976\n",
      "Epoch: 21\n",
      "\tTraining loss: 0.021683598699622438\n",
      "\tValidation loss: 0.03310276963157169\n",
      "Epoch: 22\n",
      "\tTraining loss: 0.019749971583170337\n",
      "\tValidation loss: 0.0302624235737924\n",
      "Epoch: 23\n",
      "\tTraining loss: 0.01806643805005998\n",
      "\tValidation loss: 0.02813686923947446\n",
      "Epoch: 24\n",
      "\tTraining loss: 0.016651651981618894\n",
      "\tValidation loss: 0.026420241550037114\n",
      "Epoch: 25\n",
      "\tTraining loss: 0.01540388646462808\n",
      "\tValidation loss: 0.024672484579258948\n",
      "Epoch: 26\n",
      "\tTraining loss: 0.01433413405838649\n",
      "\tValidation loss: 0.02330424016845598\n",
      "Epoch: 27\n",
      "\tTraining loss: 0.01337562720443383\n",
      "\tValidation loss: 0.02194064994563819\n",
      "Epoch: 28\n",
      "\tTraining loss: 0.012538491092706478\n",
      "\tValidation loss: 0.02098633777302357\n",
      "Epoch: 29\n",
      "\tTraining loss: 0.01178556981680867\n",
      "\tValidation loss: 0.020011833070553136\n",
      "Epoch: 30\n",
      "\tTraining loss: 0.011089575059100769\n",
      "\tValidation loss: 0.019134682121599664\n",
      "Epoch: 31\n",
      "\tTraining loss: 0.010508523950929752\n",
      "\tValidation loss: 0.018126403782591453\n",
      "Epoch: 32\n",
      "\tTraining loss: 0.009951776872286319\n",
      "\tValidation loss: 0.017423672006734867\n",
      "Epoch: 33\n",
      "\tTraining loss: 0.009456801636293923\n",
      "\tValidation loss: 0.016800284100646178\n",
      "Epoch: 34\n",
      "\tTraining loss: 0.00900026855764106\n",
      "\tValidation loss: 0.016090296173129553\n",
      "Epoch: 35\n",
      "\tTraining loss: 0.008580881379562014\n",
      "\tValidation loss: 0.015575553387560871\n",
      "Epoch: 36\n",
      "\tTraining loss: 0.008210899981547296\n",
      "\tValidation loss: 0.015035003892692574\n",
      "Epoch: 37\n",
      "\tTraining loss: 0.0078663635770123\n",
      "\tValidation loss: 0.01453592962372011\n",
      "Epoch: 38\n",
      "\tTraining loss: 0.007539929061418975\n",
      "\tValidation loss: 0.014130841896321723\n",
      "Epoch: 39\n",
      "\tTraining loss: 0.007247987939648732\n",
      "\tValidation loss: 0.013647451930776466\n",
      "Epoch: 40\n",
      "\tTraining loss: 0.006968617205581126\n",
      "\tValidation loss: 0.013326563406702022\n",
      "Epoch: 41\n",
      "\tTraining loss: 0.006717960318604099\n",
      "\tValidation loss: 0.012976901125950945\n",
      "Epoch: 42\n",
      "\tTraining loss: 0.00648289337003324\n",
      "\tValidation loss: 0.012590943348985011\n",
      "Epoch: 43\n",
      "\tTraining loss: 0.00625934201566907\n",
      "\tValidation loss: 0.012295954523192851\n",
      "Epoch: 44\n",
      "\tTraining loss: 0.006053121908684483\n",
      "\tValidation loss: 0.011960183697899198\n",
      "Epoch: 45\n",
      "\tTraining loss: 0.00585226547802198\n",
      "\tValidation loss: 0.011661037496716873\n",
      "Epoch: 46\n",
      "\tTraining loss: 0.005673833848311703\n",
      "\tValidation loss: 0.011408398596843922\n",
      "Epoch: 47\n",
      "\tTraining loss: 0.005504954485132215\n",
      "\tValidation loss: 0.011149577867465973\n",
      "Epoch: 48\n",
      "\tTraining loss: 0.005336529490582918\n",
      "\tValidation loss: 0.010900035186989693\n",
      "Epoch: 49\n",
      "\tTraining loss: 0.005184437012094469\n",
      "\tValidation loss: 0.010692072714878015\n",
      "Epoch: 50\n",
      "\tTraining loss: 0.005040825002803515\n",
      "\tValidation loss: 0.010485192809699149\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Valid NLL loss: 0.010485192809699149\n",
      "Your code PASSED the code check!\n"
     ]
    }
   ],
   "source": [
    "# TODO 15.1\n",
    "nn_hyper_params = dict(\n",
    "    neurons_per_layer=[128, 64, 24],\n",
    "    g_hidden=ReLU,\n",
    "    g_output=Softmax,\n",
    "    alpha=0.01,\n",
    "    epochs=50,\n",
    "    batch_size=128,                   \n",
    "    seed=0,\n",
    "    verbose=True\n",
    ")\n",
    "nn_clf = NeuralNetwork(**nn_hyper_params)\n",
    "\n",
    "nn_clf.fit(X_trn, y_trn, X_vld=X_vld, y_vld=y_vld)\n",
    "\n",
    "plt.plot(nn_clf.avg_trn_loss_tracker, label='Train loss')\n",
    "plt.plot(nn_clf.avg_vld_loss_tracker, label='Valid loss')\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.ylabel(\"Negative Log Likelihood Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_hat_probs_vld = nn_clf.predict_proba(X_vld)\n",
    "vld_nll = mean_nll(y=y_vld, y_hat_probs=y_hat_probs_vld)\n",
    "print(\"-\"*50)\n",
    "print(f\"Valid NLL loss: {vld_nll}\")\n",
    "\n",
    "todo_check([\n",
    "    (vld_nll <= 0.25, \"vld_nll loss is not below .25\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c4e1d91a",
   "metadata": {
    "id": "c4e1d91a",
    "outputId": "2e6d2942-85f2-41a3-b811-92c1548f1995"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24],\n",
       "       [ 8],\n",
       "       [18],\n",
       "       ...,\n",
       "       [ 0],\n",
       "       [20],\n",
       "       [ 3]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_vld= nn_clf.predict(X_vld)\n",
    "class_labels[y_hat_vld]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "25b9c269",
   "metadata": {
    "id": "25b9c269",
    "outputId": "ceef2d92-a42b-440c-cc6f-9dffd6dc0b1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ratio: 5483/5491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.9985430704789656)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y=y_vld, y_hat=y_hat_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "50b6a896",
   "metadata": {
    "id": "50b6a896",
    "outputId": "12f72d1d-8089-4cd7-e692-5f4eaab6d7b0"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    y_hat=class_labels[y_hat_vld],\n",
    "    y=class_labels[np.argmax(y_vld, axis=1)],\n",
    "    class_names=class_names,\n",
    "    figsize=(10,10)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f6399b",
   "metadata": {
    "id": "79f6399b"
   },
   "source": [
    "## Testing Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a61f789",
   "metadata": {
    "id": "3a61f789"
   },
   "source": [
    "Let's now see how well the neural network trained using your selected hyper-parameters, stored in `nn_hyper_params`, performs on the testing set. To do so, we will retrain a neural network using a combined dataset consisting of the train and validation datasets. Further, we'll track the loss over each epoch for the new combined training dataset and the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4d8fcbc4",
   "metadata": {
    "id": "4d8fcbc4",
    "outputId": "913d3d08-05c3-4f97-9d5b-80897e8ee654",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\tTraining loss: 4.669893305006708\n",
      "\tValidation loss: 4.012432233888472\n",
      "Epoch: 2\n",
      "\tTraining loss: 1.5385940472307817\n",
      "\tValidation loss: 3.167729069826951\n",
      "Epoch: 3\n",
      "\tTraining loss: 0.8304587293950371\n",
      "\tValidation loss: 2.838786932774\n",
      "Epoch: 4\n",
      "\tTraining loss: 0.5163773873282873\n",
      "\tValidation loss: 2.7403407448258514\n",
      "Epoch: 5\n",
      "\tTraining loss: 0.3456821906805279\n",
      "\tValidation loss: 2.655998952963251\n",
      "Epoch: 6\n",
      "\tTraining loss: 0.24055362084300844\n",
      "\tValidation loss: 2.598408951030176\n",
      "Epoch: 7\n",
      "\tTraining loss: 0.1732354167845527\n",
      "\tValidation loss: 2.608973338009391\n",
      "Epoch: 8\n",
      "\tTraining loss: 0.12687264551598973\n",
      "\tValidation loss: 2.5866472547151544\n",
      "Epoch: 9\n",
      "\tTraining loss: 0.09441646402682045\n",
      "\tValidation loss: 2.5691453920130205\n",
      "Epoch: 10\n",
      "\tTraining loss: 0.07217957727414427\n",
      "\tValidation loss: 2.5754799546291784\n",
      "Epoch: 11\n",
      "\tTraining loss: 0.0570914733814548\n",
      "\tValidation loss: 2.5822572058744497\n",
      "Epoch: 12\n",
      "\tTraining loss: 0.04606251480534337\n",
      "\tValidation loss: 2.5896268426500733\n",
      "Epoch: 13\n",
      "\tTraining loss: 0.03810944838343735\n",
      "\tValidation loss: 2.5834977636568026\n",
      "Epoch: 14\n",
      "\tTraining loss: 0.03224516796108758\n",
      "\tValidation loss: 2.5848117573668477\n",
      "Epoch: 15\n",
      "\tTraining loss: 0.02776119002024023\n",
      "\tValidation loss: 2.5759064230202595\n",
      "Epoch: 16\n",
      "\tTraining loss: 0.02427440144730813\n",
      "\tValidation loss: 2.5816782900416113\n",
      "Epoch: 17\n",
      "\tTraining loss: 0.02145658074309699\n",
      "\tValidation loss: 2.58000218860092\n",
      "Epoch: 18\n",
      "\tTraining loss: 0.019157316487241866\n",
      "\tValidation loss: 2.581551659654304\n",
      "Epoch: 19\n",
      "\tTraining loss: 0.01728450691887932\n",
      "\tValidation loss: 2.575911417741908\n",
      "Epoch: 20\n",
      "\tTraining loss: 0.01568511602518779\n",
      "\tValidation loss: 2.5759722809040815\n",
      "Epoch: 21\n",
      "\tTraining loss: 0.01437100729956601\n",
      "\tValidation loss: 2.573622452749004\n",
      "Epoch: 22\n",
      "\tTraining loss: 0.013237981941533557\n",
      "\tValidation loss: 2.5764142058184767\n",
      "Epoch: 23\n",
      "\tTraining loss: 0.012259329223281467\n",
      "\tValidation loss: 2.5817732655004795\n",
      "Epoch: 24\n",
      "\tTraining loss: 0.01140313573153518\n",
      "\tValidation loss: 2.579001525230099\n",
      "Epoch: 25\n",
      "\tTraining loss: 0.010662623264079936\n",
      "\tValidation loss: 2.579109137040587\n",
      "Epoch: 26\n",
      "\tTraining loss: 0.010001008550385172\n",
      "\tValidation loss: 2.5787900292050008\n",
      "Epoch: 27\n",
      "\tTraining loss: 0.009416716115197704\n",
      "\tValidation loss: 2.5760201084571746\n",
      "Epoch: 28\n",
      "\tTraining loss: 0.008897098633547347\n",
      "\tValidation loss: 2.5806056176640295\n",
      "Epoch: 29\n",
      "\tTraining loss: 0.00841709575486877\n",
      "\tValidation loss: 2.579844467726391\n",
      "Epoch: 30\n",
      "\tTraining loss: 0.007981876323479941\n",
      "\tValidation loss: 2.580336852603593\n",
      "Epoch: 31\n",
      "\tTraining loss: 0.007609106394018399\n",
      "\tValidation loss: 2.5813594770659973\n",
      "Epoch: 32\n",
      "\tTraining loss: 0.007249039169208609\n",
      "\tValidation loss: 2.5769136086148188\n",
      "Epoch: 33\n",
      "\tTraining loss: 0.006926353417216499\n",
      "\tValidation loss: 2.5810816933448897\n",
      "Epoch: 34\n",
      "\tTraining loss: 0.006628666340063931\n",
      "\tValidation loss: 2.57903917311553\n",
      "Epoch: 35\n",
      "\tTraining loss: 0.006350272592712695\n",
      "\tValidation loss: 2.580909204529866\n",
      "Epoch: 36\n",
      "\tTraining loss: 0.006093809953032018\n",
      "\tValidation loss: 2.5787220386571135\n",
      "Epoch: 37\n",
      "\tTraining loss: 0.005857432974963124\n",
      "\tValidation loss: 2.581684545589034\n",
      "Epoch: 38\n",
      "\tTraining loss: 0.0056405714917534735\n",
      "\tValidation loss: 2.581243790223658\n",
      "Epoch: 39\n",
      "\tTraining loss: 0.005436443683484067\n",
      "\tValidation loss: 2.579381248350818\n",
      "Epoch: 40\n",
      "\tTraining loss: 0.005244995958514297\n",
      "\tValidation loss: 2.580334918520399\n",
      "Epoch: 41\n",
      "\tTraining loss: 0.005067218359799317\n",
      "\tValidation loss: 2.580850799428421\n",
      "Epoch: 42\n",
      "\tTraining loss: 0.004899693962315634\n",
      "\tValidation loss: 2.57907322149385\n",
      "Epoch: 43\n",
      "\tTraining loss: 0.00474106015585553\n",
      "\tValidation loss: 2.5789944781881653\n",
      "Epoch: 44\n",
      "\tTraining loss: 0.004594155817593844\n",
      "\tValidation loss: 2.5800098601146884\n",
      "Epoch: 45\n",
      "\tTraining loss: 0.004453934031686711\n",
      "\tValidation loss: 2.5770355984517708\n",
      "Epoch: 46\n",
      "\tTraining loss: 0.0043233904604249055\n",
      "\tValidation loss: 2.5790503984094526\n",
      "Epoch: 47\n",
      "\tTraining loss: 0.00419917769627822\n",
      "\tValidation loss: 2.5762965835130354\n",
      "Epoch: 48\n",
      "\tTraining loss: 0.004083630433813034\n",
      "\tValidation loss: 2.577118750306978\n",
      "Epoch: 49\n",
      "\tTraining loss: 0.003971861685284606\n",
      "\tValidation loss: 2.5777392974251963\n",
      "Epoch: 50\n",
      "\tTraining loss: 0.0038652037061307444\n",
      "\tValidation loss: 2.5778864423290395\n"
     ]
    }
   ],
   "source": [
    "X_trn_all = np.vstack([X_trn, X_vld])\n",
    "y_trn_all = np.vstack([y_trn, y_vld])\n",
    "\n",
    "nn_clf = NeuralNetwork(**nn_hyper_params)\n",
    "\n",
    "nn_clf.fit(X_trn_all, y_trn_all, X_vld=X_tst, y_vld=y_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1440705a",
   "metadata": {
    "id": "1440705a"
   },
   "source": [
    "You might be noticing, depending on the hyper-parameters you picked, that you have vastly overfitted to the training and validation datasets! You can tell as your learning curve will show your test loss start to increase while the training loss continues to decrease and by having a very low test accuracy (if you want to learn more checkout this [post](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/) on interpreting learning curves). This is an example where picking the best hyper-parameters doesn't mean your model will generalize well to new unseen data.\n",
    "\n",
    "Overcoming this problem is non-trivial. However, to help alleviate this issue, we would likely need to diversify the training dataset to account for the variations in testing data that are not included in the training or validation datasets.\n",
    "\n",
    "That being said, a short term fix could be simply training less epochs and stopping training before the validation loss starts to increase. Doing so is what we call *[early stopping](https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "fc205a81",
   "metadata": {
    "id": "fc205a81",
    "outputId": "5437e96b-009e-4db3-ce89-c534a88a6842"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyEAAANaCAYAAACeC0PxAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzs3XdcU1cfBvAnCRBAhgxluHdFFPeeOHHVhVqrdbRqtXZZq1KtrV20trXDWd+6d7XOuupuFXHgrLPWLahsZAWS3PePQCSykhiSm/B8P28+kjvOc06S8vLLPfdeiSAIAoiIiIiIiMxEaukOEBERERFR6cIihIiIiIiIzIpFCBERERERmRWLECIiIiIiMisWIUREREREZFYsQoiIiIiIyKxYhBARERERkVmxCCEiIiIiIrNiEUJERERERGbFIoSIrN6KFSsgkUhw5swZS3fFYB07dkTHjh0tlq9Wq7F69Wp06dIF3t7esLe3R/ny5dG7d2/s3LkTarXaYn0jIiLbZWfpDhARlWYLFy60WHZmZib69euHP//8E0OHDsWiRYvg6+uL2NhY7N27F6Ghodi4cSNefvlli/WRiIhsE4sQIiITEQQBmZmZcHJy0nufgICAEuxR0SZPnox9+/Zh5cqVeO2113TWDRgwAB9++CEyMjJMkpWeng5nZ2eTtEVERNaP07GIqNT4999/MWzYMJQvXx5yuRx169bFggULdLbJzMzEBx98gIYNG8Ld3R2enp5o1aoVtm/fnq89iUSCSZMmYfHixahbty7kcjlWrlypnR52+PBhTJgwAd7e3vDy8sKAAQMQHR2t08bz07Hu3LkDiUSC7777DnPnzkW1atXg4uKCVq1aITIyMl8f/ve//6F27dqQy+UICAjAunXrMGrUKFStWrXI1+LRo0f49ddf0b1793wFSK5atWqhQYMGAJ5Nebtz547ONkeOHIFEIsGRI0d0xhQYGIi//voLrVu3hrOzM8aMGYN+/fqhSpUqBU7xatGiBRo3bqx9LggCFi5ciIYNG8LJyQkeHh4YNGgQbt26VeS4iIjIOrAIIaJS4cqVK2jWrBn++ecffP/99/jjjz/Qq1cvvPPOO5g9e7Z2O4VCgYSEBEyZMgXbtm3D+vXr0bZtWwwYMACrVq3K1+62bduwaNEizJo1C/v27UO7du2069544w3Y29tj3bp1mDNnDo4cOYLhw4fr1d8FCxZg//79+PHHH7F27VqkpaWhZ8+eSE5O1m6zZMkSjBs3Dg0aNMCWLVswc+ZMzJ49W6cgKMzhw4eRnZ2Nfv366dUfQ8XExGD48OEYNmwYdu/ejYkTJ2LMmDG4d+8eDh06pLPttWvXcOrUKYwePVq7bPz48XjvvffQpUsXbNu2DQsXLsTly5fRunVrPH78uET6TERE5sPpWERUKkyePBmurq44duwY3NzcAABdu3aFQqHA119/jXfeeQceHh5wd3fH8uXLtfupVCp07twZiYmJ+PHHH/MdNUhNTcWlS5fg4eGhXXb69GkAQI8ePfDzzz9rlyckJGDq1Kl49OgRfH19i+yvq6sr/vjjD8hkMgCAv78/mjdvjj179mDo0KFQq9X45JNP0KJFC2zevFm7X9u2bVGzZk34+/sX2f69e/cAANWqVStyO2MlJCRg06ZNCA4O1i5TKpXw8fHB8uXL0aVLF+3y5cuXw8HBAcOGDQMAREZG4n//+x++//57TJ48Wbtdu3btULt2bcydOxfffPNNifSbiIjMg0dCiMjmZWZm4uDBg+jfvz+cnZ2hVCq1j549eyIzM1NnqtOmTZvQpk0buLi4wM7ODvb29li6dCmuXr2ar+3g4GCdAiSvvn376jzPndp09+7dYvvcq1cvbQFS0L7Xr1/Ho0ePMHjwYJ39KleujDZt2hTbfknz8PDQKUAAwM7ODsOHD8eWLVu0R3RUKhVWr16Nl19+GV5eXgCAP/74AxKJBMOHD9d5r3x9fREUFKTXkR4iIhI3FiFEZPPi4+OhVCoxb9482Nvb6zx69uwJAIiLiwMAbNmyBYMHD0aFChWwZs0anDhxAqdPn8aYMWOQmZmZr20/P79Cc3P/qM4ll8sBQK+TvYvbNz4+HgDg4+OTb9+Clj2vcuXKAIDbt28Xu60xCntdcl/HDRs2AAD27duHmJgYnalYjx8/hiAI8PHxyfd+RUZGat8rIiKyXpyORUQ2z8PDAzKZDCNGjMBbb71V4Da505LWrFmDatWqYePGjZBIJNr1CoWiwP3ybmNOuUVKQedHPHr0qNj9O3XqBHt7e2zbtg1vvvlmsds7OjoCyP86FFYQFPa6BAQEoHnz5li+fDnGjx+P5cuXw9/fH926ddNu4+3tDYlEgr///ltbfOVV0DIiIrIuPBJCRDbP2dkZnTp1wrlz59CgQQM0bdo03yP3j3qJRAIHBwedP6IfPXpU4NWxLKlOnTrw9fXFb7/9prP83r17iIiIKHZ/X19fvPHGG9i3b1+BJ9wDwH///YeLFy8CgPZqW7nPc+3YscPgvo8ePRonT57EsWPHsHPnTowcOVJn6lnv3r0hCAIePnxY4HtVv359gzOJiEhceCSEiGzGoUOH8l1CFgB69uyJn376CW3btkW7du0wYcIEVK1aFU+fPsXNmzexc+dO7RWbevfujS1btmDixIkYNGgQ7t+/j88//xx+fn74999/zTyiwkmlUsyePRvjx4/HoEGDMGbMGCQlJWH27Nnw8/ODVFr8d0xz587FrVu3MGrUKOzbtw/9+/eHj48P4uLisH//fixfvhwbNmxAgwYN0KxZM9SpUwdTpkyBUqmEh4cHtm7dimPHjhnc91deeQWTJ0/GK6+8AoVCgVGjRumsb9OmDcaNG4fRo0fjzJkzaN++PcqUKYOYmBgcO3YM9evXx4QJEwzOJSIi8WARQkQ2Y9q0aQUuv337NgICAnD27Fl8/vnnmDlzJp48eYKyZcuiVq1a2vNCAM239E+ePMHixYuxbNkyVK9eHdOnT8eDBw90LuUrBuPGjYNEIsGcOXPQv39/VK1aFdOnT8f27du1V78qiqOjI3bt2oW1a9di5cqVGD9+PFJSUuDh4YGmTZti2bJl6NOnDwBAJpNh586dmDRpEt58803I5XIMHToU8+fPR69evQzqt7u7O/r3749169ahTZs2qF27dr5tfvnlF7Rs2RK//PILFi5cCLVaDX9/f7Rp0wbNmzc3KI+IiMRHIgiCYOlOEBGRaSQlJaF27dro168flixZYunuEBERFYhHQoiIrNSjR4/w5ZdfolOnTvDy8sLdu3fxww8/4OnTp3j33Xct3T0iIqJCsQghIrJScrkcd+7cwcSJE5GQkABnZ2e0bNkSixcvRr169SzdPSIiokJxOhYREREREZkVL9FLRERERERmxSKEiIiIiIjMikUIERERERGZFYsQIiIiIiIyq1Jzdawq7+w0S871uX3MkkNkSwRBgFItQKkSoFSroVQJyFaroVJplmer1Dnr1VCpNeuUqjzb56zL/TdbJUCl1m1LqV0mQK3W/KwScv5VC1Cp1VCpoflXgM5ztfBsP51/hcLbUwsC1GpAJQgQcpYLgua5Zp0AlQDtOrUAzXJBsx0RUWl052vDbn5qTk6NJlm6C4XKODff0l0wWKkpQoioYIIgIEupRnq2CplZKmRkax65PyuUKmQp1ZqHSvOvIs/zbO1ylc422n+VBTx/bptsldrSL4NVkkoAmVQCiUQCqQSQQAKJBJDmPpdItD9LJc/WSbTrcp8/+zl3P51t8Ww9NP/TWadpL2cZnu3/rF8A8vRRmrM9kCcv736azbVtQyfv2fbQGbNmOZ7bN3+7Em37miaejen5/XI3Kmhdbv+12z6Xg+f6n5ud+zOe3++5PuX+nHf9s7E9awuF7CcpYB3yrSto+4La1V2X7/XLs1GBuXn6nXdlUdvmXa/bfv7+Fdr/ArZ7kXags13hr5PuMp1R6PZFZ3kBeQVsr7uukMbw/JiK7sPzrRTWl4IWFLVv4dsUvw+VDixCiKyAIAjIyFbhaaYSqZnZeKpQIl2hQka2EhlZKmRmq7RFhPbnbBUycgqJIn/OVonym3fNH9hS2MsksJNKYCeTav6VSmEnk2geuT9Lc9bl/Gwvk0CWZ11uG7nLZFLNH+d2Ugmk0px/n38u1Wwnk+T8W8DzgvbNu41E+y8gyy0IpM/+oM/th7YAyFmn3Td3nXY73fVERETWikUIUQlTqQWkKpR4mpmNNIUSTzM1P6dmKrXLnyo0xUVqzvrUTCWeKp5tk6pQQqUu+UrBXiaFs4MMjvaah5O9DHJ7KRxkUjjY5Txkz/1rJ3vueUHbaH62z/lZbifLt01ukWGfUzhIpfwjm4iIRETCU6lNiUUIkQEys1VISMtCfJoCiWlZBfychaT0LE0RkVNUpGepTJYvk0rgIreDi9wOZeR2cHLQFAq5/zoW9XPO82c/2+Wsk8LJwQ6OOYUAERERUUljEUKlWm5RkZCWhcQ0BeJzfk5IU2iX533+IgWF3E4KV0d7lJHbwdXRDq5yO7g42sPVUVNU5P6cu9wld7s8+zjZyzgNh4iIiKweixCySWq1gNhUBR4mpuN+QjoeJGYgOikd8WlZSMw5emFsUWEvk8CzjByeZRzg4ewALxcH7fPcZS45xYOmoLCDi9weDnY8ykBERGS1+CWgSbEIIauVmpmN+4maAkNTaKRrio7EdDxMzIBCqd8Vl+ykkpwC4lkhkfe5l4sDPMo4wNNZDi8XB7jI7Xg0goiIiOgFsAgh0cpSqhGTnLfAyMgpOjRHN5IzsovcXyoBfN2dUMnDGRU9nFDBwxleLnJ4PVdkuDqyqCAiIiIyJxYhZFEZWSpcf5yC+/HpOgXGw8QMPErJQHEXhPJwdkBFDydU8nRGBQ/nnIJDU3T4lXWCPU+0JiIiIlPg1bFMqlS9mhO71sSOD9rh8pwQRH3ZDUveaIbq5cto19tJJZjety72Te+Aq9+G4NTnXTF3eEOUd5PrtPNK68rY8HYr/DOnB+7+3AduTsbVchvXr0VIt2A0a1QfQ0MH4GzUmRcan9hzVGoBNx4/xe9R9zFr2yX0X/A3mn6xD0N/icCHm8/j54M3sOXsA5y+k4DoZE0B4mgvRc3yLuhQpzyGt6yK6SF1MczrHiqe+A6uu6ajyrlFCGvuiLlDGuODbi9hcLPKaF3TG5W9yrxwASKW181aMpjDHHNlMIc55spgjvhzyHqVqiKkRU0vrPr7NvrN/RvDF0TCTirB6okt4eQgAwA4OcgQWNEdP++7gV7f/oXxS0+jWnkXLB3XXKcdJwcZjl6NxYI/bxrdl717dmPO1+EYO24CNm7ehsaNm2Di+LGIiY5+oTGKJadR4yaYMP4NbDh6Ht/tu4oRv55Asy/2oe+8vzBj60X8duYersakQKUWUM5FjubVPDGwcUW827k2vh3UEOvHtcbf0zrj3Kwe+OOdDvhlRDPM7F0PvinXsH3FAkycMNEmX7eSyLGlsTBH3Dm2NBbmiDvHlsbCHCqtJIIgxnslF+78+fNo2LChwftVeWdnvmWeLg4491V3hP50HKf+SyhwvwaV3bFzSnu0+uQAohMzdNa1rOmFje+0Rv1pe5CSoQQAXJ/bR6/+vDo0FHUDAjBz1mztsn59QtApuAveff8DfYclmpyhgwfBu1INBPYZi0sPk3DxQRKStsyG2j8Qqnq9tds5O8gQWMEdDSqW1T583Bz1PifD1l43c+TY0liYI+4cWxoLc8SdY0tjYY4uRxGfKODUbLKlu1CojNNzLd0Fg1nFkZDk5GQsXLgQjRs3RpMmTUzWrmvOJz0pvfATnF0d7aFWC0gp5iRoQ2RnZeHqlcto1bqtzvJWrdvgwvlzos/JUqpx6WES1p28g2mbzyNk7gFcuXIZhxLL4scD13Hw6mPEPlUAPnXgkvoAQ5pVxpf9G2DH2+1xemZ3rHq9FaZ0r4tu9fzg6+6kdwFi7a+bJXJsaSzMEXeOLY2FOeLOsaWxMIdKMxHXm8ChQ4ewbNkybNmyBVWqVMHAgQOxdOlSk7X/cf96OPVfPG7EPC1wvdxOiul962J71EOkZipNlpuYlAiVSgUvLy+d5V5e3oiLixVVjiAIuBufjksPknDxYRIu3k/ClZgUZKvyXP42IxlyQY1y3t5oUt8fDSpqjnSc3H0Xe/7Yjtkv1xfNeEpbji2NhTnizrGlsTBH3Dm2NBbmUGkmuiLkwYMHWLFiBZYtW4a0tDQMHjwY2dnZ+P333xEQEKBXGwqFAgqFQmeZoMqGRGavff55aCBe8nfDoJ+OF9iGnVSCeaOaQCqRYOamS8YPqAjPHwEQBKFELhVraE6WUoVD155g27kHOHcvscBL4bo72WunU1VxVuLjvcDPw5oiqGEj7TZRUokoxsMc2xoLc8SdY0tjYY64c2xpLMyxErw6lkmJqgjp2bMnjh07ht69e2PevHno0aMHZDIZFi9ebFA74eHhmD17ts4yt+ZDUbbFMADA7IGB6BLoi8E/HcejpMx8+9tJJVgwugkqeTnhlXknTHoUBAA8ynpAJpMhLi5OZ3lCQjy8vLwtlnPzyVNsjrqP7eceIjE9S7vcwU6KAD83NKhYFvUrlkVQxbKo5Oms/WWSnZWFT0U4HubY1liYI+4cWxoLc8SdY0tjYQ6VZqIq6f7880+88cYbmD17Nnr16gWZTGZUO2FhYUhOTtZ5uDcNBQB8NigQPYJ88cr8E7ifkJFv39wCpFq5Mnh1QWSR54sYy97BAXUD6iEyQvcoTGREhM6RBHPkpCmU+D3qPob+chy9f/4LK47fRmJ6Fsq7yjGhY01sntAWZ2Z2x4bxbfBRr3roE1QBlb3K6HybIabxMMf8GcxhjrkymMMcc2UwR/w5ZP1EdSTk77//xrJly9C0aVO89NJLGDFiBIYMGWJwO3K5HHK57r09JDJ7fBFaH32bVMDYX08jLVOJcq6abVIys6HIVkMmlWDR600RWNEdY345BZlEot0mKT0L2SrNhcTKucpRzk2OquU09xip4+eGNIUSDxPzFzWFGTFyNGZMn4qAwEAEBTXC75s2IiYmBqFDhho8XmNy6rfviY+3XcSui9FIz1IBAGRSCTrVKY9BTSuhbc1ysDPgPhuWHg9zLJvBHOaYK4M5zDFXBnPEn2N21j6dTGREVYS0atUKrVq1wk8//YQNGzZg2bJlmDx5MtRqNfbv349KlSrB1dXV6PZHtKsKAPjtndY6yz9Ycw6bTz2AX1lHdKvvCwDYO72DzjZDfo5A5M14AMCrbavg/ZA62nWb32ujbUdfPUJ6IjkpEUsWLURs7BPUrFUbCxYvgb9/BYPHZUiOh18VuHV9CxO33tJuU8WrDAY1qYR+jSqgnKujSXLMNR7miCODOcwxVwZzmGOuDOaIP4esm+jvE3L9+nUsXboUq1evRlJSErp27YodO3YY3E5B9wkpCfreJ8Rc1GoBJ2/HY9OZ+9h/5ZH2qlZyOyl6BPphUJNKaFrV0/pPFiMiIiKrJ+r7hLT40NJdKFTGyW8t3QWDifit1qhTpw7mzJmD8PBw7Ny5E8uWLbN0l6zC45RMbD17H5uj7uNBnmliAX5uGNS0Eno3qAA3J/siWiAiIiIiLV4dy6REX4Tkkslk6NevH/r162fprohWtkqNo9efYFPUffx94wnUOce4XB3t0CeoAgY2qYR6/u6W7SQRERERlXpWU4RQ4e7EpeH3qPvYeu4B4lKf3R+laVVPhDaphG71/ODkYNyVxoiIiIiITI1FiJXKyFLhz8sx2BR1H2fuJGiXe5VxQP/GFTGgcSVUL+diwR4SERER2RCeP2tSLEKsTJZSjZ8OXMdvZ+7hac5NFKUSoF2tchjUtDI61ikPewMurUtEREREZG4sQqzI08xsvLMuCiduaS4VXKGsEwY1qYT+jSvC193Jwr0jIiIiItIPixAr8Sg5A+NWncaNx0/h7CDDVwOC0C3AF1IpDw0SERERlTheHcukWIRYgeuPUjBu1Wk8TslEORc5Fr/WjFe5IiIiIiKrxSJE5E78F4e310UhVaFEjXIu+OW1Zqjo4WzpbhERERERGY1FiIhtP/cAM7ddRLZKQNOqnljwalO48waDRERERObHq2OZFIsQERIEAb8cvYkfD9wAAPSs74fwAUGQ2/NeH0RERERk/SSCIAiW7oQ55FzNtsR1/enYC+0vCAISUzOQlpkNAHB1coB7GUdInqu+97/b9oVyiIiIiMTGUcRfjzu1mWHpLhQq4/iXlu6CwUT8Vpc+akFAfEo6MrM0FVNZF0e4Oskt3CsiIiIi4tWxTItFiEio1GrEJqcjW6mCBICnmzOc5Tz/g4iIiIhsD4sQEchWqhCbnAaVWoBUIoG3uzPk9nxriIiIiMg28S9dC1NkKxGXnA61IMBOKoV3WWfYy3gCOhEREZGo8OpYJsUixILSFdmIT0kHADjYyeDt7gyZlPMNiYiIiMi2sQixkKfpCiSlZQIAnBzs4OnmDCkrbCIiIiIqBViEmJkgCEhKy0RqRhYAwMXRAWVd8l+Cl4iIiIhEhFfHMikWIWakFgQkpKQjI+cSvO5lHOHq5MAChIiIiIhKFRYhZqJSqxGXnI4spQoA4OXqBGdHBwv3ioiIiIjI/HhcqRAb169FSLdgNGtUH0NDB+Bs1Bm99w2q4Iav+wVg6/hm+PuDtmhdzQNPktKQpVRBIgEqe7vho54v4fdxzXDgnVZYPaox+gX56rTh6WyPmSG1se3N5vjznVZYOrwhOtbyMvtYmGMbObY0FuaIO8eWxsIccefY0liYYyUkUvE+rJB19rqE7d2zG3O+DsfYcROwcfM2NG7cBBPHj0VMdLRe+zvay3AzNhU/HLwFAEhKy4RSpYZMKoFPWRd82K0WWlT1wOe7b2D4irP4Leoh3g2ugbY1PLVtzOxZG5U8nBC27QpGrjyLo//G49PeL6FW+TJmHQtzrD/HlsbCHHHn2NJYmCPuHFsaC3OotGIRUoDVK5ej/8CBGDAoFNVr1MDUsBnw9fPFbxvX67X/yTuJ+PX4Pey7/AiA5mR0ezspfMq6wN5Ohnr+rth75QnOP0jGoxQFdl56jP9i01DHx0XbRj0/N2w5F42rj1IRk6zAqpP3kapQonZ5l8JiS2QszLH+HFsaC3PEnWNLY2GOuHNsaSzModKKRchzsrOycPXKZbRq3VZneavWbXDh/Dm920nNUCAu5x4g9nYylHd3gUymebkvPkxBmxqe8HbRnBPSqJI7Knk44tTdJO3+lx6mILhOObg62kECoHMdb9jLpDh3P9nsY2GO9ebY0liYI+4cWxoLc8SdY0tjYY6VkUrE+7BCoj0xPT4+Hl5emnMg7t+/j//973/IyMhA37590a5duxLLTUxKhEql0mbn8vLyRlxcbLH7C4KA5DQFnmYotMvcnOWQStO0z386dAtTu9XE1vHNoVSpoRaAOX/+i0sPU7TbfPLHNczu/RJ2v9USSpUamUo1Zuy4iujkTLONhTnWn2NLY2GOuHNsaSzMEXeOLY2FOVSaia4IuXTpEvr06YP79++jVq1a2LBhA3r06IG0tDRIpVL88MMP2Lx5M/r161doGwqFAgqFQmeZIJNDLpfr3Y/nL5srCEKxl9LNUqqQ8DQD6YpsAJriQ9OW7naDGvujnp8rpm29gscpmQiq6I7JXWogLi0LUfc0RzrGtq0CV0c7vLfpEpIylGhX0xOf9X4JkzZexK24dL3HYexYjMEc8ebY0liYI+4cWxoLc8SdY0tjYQ6VRqKbjjV16lTUr18fR48eRceOHdG7d2/07NkTycnJSExMxPjx4/H1118X2UZ4eDjc3d11Ht9+E65XvkdZD8hkMsTFxeksT0iIh5eXd5H7Ttl0XluAeLo6wb2MY75tHOykGNe2CuYfuY2IWwn4Ly4dW87H4ND1OLzStCIAwN/dEQMb+SN837+IupeM/2LTsOLEfVx/nIr+Df30GseLjsUQzBFvji2NhTnizrGlsTBH3Dm2NBbmWBlLXwGLV8cqWadPn8aXX36Jtm3b4rvvvkN0dDQmTpwIqVQKqVSKt99+G9euXSuyjbCwMCQnJ+s8PpwWple+vYMD6gbUQ2TEcZ3lkRERCGrYqND9EtIU+DPnRPRy7s4oU8g9QOykEtjLpFALustVakF7xMTRXvO2CILuRmpBgNSAbxGMHYuhmCPeHFsaC3PEnWNLY2GOuHNsaSzModJMdNOxEhIS4OuruWeGi4sLypQpA0/PZ5eu9fDwwNOnT4tsQy7PP/UqU6l/H0aMHI0Z06ciIDAQQUGN8PumjYiJiUHokKGF7nPqdgIAwF4mhUcZOSqUddKu83NzRM1yZZCSqcSTpwqcu5+MiR2qQqFU43FKJhpWckePgPKYf/Q2AOBuQgbuJ2ZgSteaWHj0NpIzlGhX0wtNq5TFtK1X9B+IkWMxBnPEm2NLY2GOuHNsaSzMEXeOLY2FOVRaia4IAfLPIzT3HMIeIT2RnJSIJYsWIjb2CWrWqo0Fi5fA379CoftE3tIcdpQ72KGOjyvmDamvXfd2p+oAgD3/PMZX+/7Fp39cw/h2VTGrZ224Odrh0VMF/nf8LrZd0BxJUakFTN1yGePbVcXX/QLg5CDDw8RMfLXnBiJvJ5b4WIzBHPHm2NJYmCPuHFsaC3PEnWNLY2GOFeE5LSYlEZ6f82NhUqkUISEh2iMZO3fuRHBwMMqU0dykT6FQYO/evVCpVAa1a8iREGP0+PEI7sSlwdvNGU5y+5INA7D/3bbFb0RERERkRRxF+fW4hlPnryzdhUJlHPzI0l0wmOje6pEjR+o8Hz58eL5tXnvtNXN1Ry+PUzJxJy4NUgkgtxfdS0pEREREJCqi+4t5+fLllu6CwU7mTMUK8HdHcpaFO0NEREREpmelV6ESK76aJhB5Kx4A0LK6VzFbEhERERERi5AXJAgCIv/TFCEtqlv59a+JiIiIiMxAdNOxrM2DxAxEJ2fATipBkyoelu4OEREREZUEXh3LpHgk5AXlXpo3qFJZODuwpiMiIiIiKg6LkBd0Mud8kBbVeD4IEREREZE++NX9CxAE4VkRwvNBiIiIiGwXr45lUnw1X8Ct2FTEpiogt5OiYaWylu4OEREREZFVYBHyAnIvzdu4sgfk9jIL94aIiIiIyDpwOtYLiORULCIiIqLSgVfHMikWIUZSqwWcup1zk8Iaz05K3/9uW7Pkv/TBH2bJufZ9b7PkEBEREVHpwelYRrr+OAXJGdlwdpChnr+7pbtDRERERGQ1eCTESLlTsZpV9YS9jLUcERERkU3j1bFMiq+mkXg+CBERERGRcViEGCFbpcbp3PNBqvMmhUREREREhuB0LCNcjk5GepYK7k72eMnXzdLdISIiIqKSxqtjmRSPhBgh9y7pzat5QirlB5KIiIiIyBAsQoxwkueDEBEREREZjdOxDJSlVCHqbgIAng9CREREVGrw6lgmxVfTQOfvJ0GhVMPbRY4a5Vws3R0iIiIiIqvDIsRAzy7N6wUJT1AiIiIiIjIYi5BCbFy/FiHdgtGsUX0MDR2As1FnAAAnb8UBMM1UrMIy9DWxSw1s/6At/vmmB8580RVLXm+K6uXLaNfbSSWY3ucl7J3WHlfm9MDJz7rg+1cborybXKcdB5kUnw6sh7NfdsOVOT3wvzeawtfd0ezjYU7J5djSWJgj7hxbGgtzxJ1jS2NhjpWQSMT7sEIsQgqwd89uzPk6HGPHTcDGzdvQuHETTBw/Frfu3sPFB0kAgBbVXqwIKSwjJjpa7zZa1PTC6r/voP8PxzBiYSRkMglWTWgBJwcZAMDJQYZ6ldwxb9+/6P3d33hz6RlUL18Gv45tptPOrAEB6N7AF2+vPIvQnyJQRm6HZeOawZALf5liPMwpmRxbGgtzxJ1jS2NhjrhzbGkszKHSSiIIgmDpTuQ6dOgQJk2ahMjISLi56d5/Izk5Ga1bt8bixYvRrl07g9vOVOq/7atDQ1E3IAAzZ83WLuvXJwS1GrXGTnVj+Ls74eCUTi80HauwjE7BXfDu+x8Uu/9LH/yRb5lnGQec/aobBv8cgVP/JRS4X4PK7tjxQTu0/vQAohMz4epoh6gvu2HymnP441wMAKC8mxwnZnfB6F9OYcnYFmYZj76YI84M5jDHXBnMYY65MphjmRxHEV8yyanXz5buQqEydr1j6S4YTFRHQn788UeMHTs2XwECAO7u7hg/fjzmzp1bon3IzsrC1SuX0ap1W53lrVq3wYVz5wC8+PkgRWacP2d0u65Omv9yk9KzC9/G0R5qtYCUdE1VFljJHQ52Uvx1LU67zZMUBW7EPEWTah565ZbUeJhjHRnMYY65MpjDHHNlMEf8ORYhkYr3YYVE1esLFy6gR48eha7v1q0boqKiim1HoVAgJSVF56FQKPTqQ2JSIlQqFby8dKdbeXl5IyHBNOeDFJURFxdrdLsz+wXg1H/xuBHztMD1cjsppvV5CdvPPkSqQlOElHOTQ6FUISVDt3CJfapAOVd5Qc3kU1LjYY51ZDCHOebKYA5zzJXBHPHnkPUTVRHy+PFj2NvbF7rezs4OsbHFf4DDw8Ph7u6u8/j2m3CD+vL8kY7MLCWylJqZay1MdH+Q5zMEQTD6CMtngwJR198N76ws+FsGO6kE80Y2hlQiwce//aNH3wBD5+mZcjzMMW2OLY2FOeLOsaWxMEfcObY0FuZQaSSqmXcVKlTApUuXULNmzQLXX7x4EX5+fsW2ExYWhsmTJ+ssE2T6favvUdYDMpkMcXFxOsuv3Y2GIHdBVe8y8HV30qstQzMSEuLh5WX4Xdg/HVgPXQJ9MPjnCDxKzsy33k4qwYLRTVDJyxmvzD+hPQoCALEpCsjtZHBzstc5GuLtIsfZ24kWGU+pzVGrAGUmEp5Ew6usGyRJdwFVFqBUACoFoMyCRJmpWaZS5PybDYkqG1BnASqlZpk6O2d5FryzMiGTSpD457ewe+AFiVqzTdLJf+GtToP9+oF52sjWtpn7s0SlBCBoD/cKEkmBh4HLqyWQSYCkDePhEOWkXZ58LAne2dlwWBkCQSrLuYpHUYeU86yHFJDK8mRL4a0SNOPZ9w3sYipo1yWduwRv6VPYHZiZpy3dfXXaz9OudjtpTqZECm81IJNKkBC1FVLZv9p9E2+ehpeTBNLru57tL5UBeNauNktQQyKoAe1D0LyWeZZ5ZWVBJpUi4fwuSB1ua7dLvBEJL7kasgtrdbbXaUtQQ/ergpz/c9f5P3nNz94qtWY8keshU57Rbpd47S94O2RDFrW0kDZy/xUAQQWo1Zp/BbXm8yqocv5Vwzs7W/PeHFkIuwR/7fKkc+fhLUmB3Z7JefbJfW1Uz5YJqpzIPO//c+9bvs9AtF+ez8A/ms/A/o/yf47yPBdypi1ItKdD5vz73HPvbOWz8SRW0q5PunAG3rJU2B36pMD9UNBplvlez2fLvJWa9ybx8ALYJVTSLk+6eAbesjTYHfmi0H1135/nPiPaZULO+6N89rsg2ke7TVLUFXjjKex3TND9bAlqAIV91gpXTqmGTAok7ZwJ+5vPpvQmnbwLb3UG7DcPL74RoYD3JPe/HwDllCrN75utU2F/1V27PulEDLyVmbBf2y/fPrltSZ57fwRJ3s+8ROffcipBNydnXVLkQ3irMmH/2yvP3oNC25E++/e5z7Jmeym8VXj23jzy1+6j+3stt63nf09KctoScsaWd7yCzr/eypzPwKF5sIuv+Ox1O3/u2X87Ovvlf+31of1M7/8Odo+e/c2WdO4feEuSYbd3StEN9P5O7yyzs9JpT2IlqiKkZ8+emDVrFkJCQuDoqHuJ2IyMDHzyySfo3bt3se3I5XLI5bpFh74npts7OKBuQD1ERhxH5y5dtcsvnDkJtVftF74qVlEZkRER6Bjc2aC2Zg8MRPcGvhg6/wQeJGTkW59bgFQt54xX5kXmO1/kn/vJyFKq0a6ON3ad15yYXs5Njtp+rgjfcdXs49E7p2M7ID0BksxERB49gE5NX4Ls3EogIxGSzETtOmQkQZKRAElGIqBIyfOHo13OH6IyQGqn+SUu1fzsIJGhnrcEp5Z9iJB7npptJTJE7nmA4NousF83QLvtsz9o7Z4tA54VDjlFhCS3cFBmAcpMSFRZkKsUqOch4PSCceh5MhmSnD/CTu72RucKmZDPb/DCr5kdgHoenog8/jd6ZKVol5+4qsmQ/XfToPYK+w7LEUA9T0+cuBqD7i7PciJuanKk9+8a3vkCaMcTcRw9svOM54Imxy5ynkly7HNyTu5YjpCYZzmRxzQ5DpuGmSTHITdn6y8IeZAnJ0KTY79zoklytOPZsx4hcXlyInNy9kwufGcDMyIP70WP1ALem6jLL5wB6PEZOFn8kV6Dcp4fz7mcnIgfTZtzZB96pOXJOZuTc+xb0+Y8/7vgUs7vgovXTJLjlJNz4sxFdJcU8Dvn2r+myfD0xInz19DdPk/G9ZyM27deOKPInGs5OTcM+/1ZmELfGxP/XtPmHN2PHul5cs6X0H87x/5CD0UB4zlzpegGxFyEkEmJqgiZOXMmtmzZgtq1a2PSpEmoU6cOJBIJrl69igULFkClUmHGjBkl3o8RI0djxvSpCAgMRFBQI/y+aSOeJsRC1WiUSe4PUlhGTEwMQocM1buNz0MD8XLjChj762mkZSq153CkZGZDka35JmLRmCaoV9Edry85BZlUot0mKT0L2SoBTzOV+C3yHmb0C0BiehaS07LxUb8AXI9OwbHr+s/dNHo8ahWQFgtJRrymgMhIhCQj4VkxoX2eBElGIsaUj8e0TRcQdHsxGnlnYeN/zngU44RXVRdgv2u93v193vN/XI+u4Yipke6oL3/0LCfZCcPKPYDsVjG/QA0wurYmJ7CsgzYnJl2KIbUyINg5AXYOgEwOyOQQcn+2cwBkjhBk9oDMAZDZax5SzXMhz8+Q2eE1eQymrz6Buq27I6hmBWz++xqiFZcxaNw7yPbxhpDbRu4+Urs8y+y03+rrfEOa88j7Tf+IiqcQ9sNK1A15A0G1q2DTn8cRnRWJge/MQlY59/zf4ufdX61Cgd/mFvAYUeZfhC0/ioCWXRFUzQubjt1AdOZNhA7tD6WHU057he2vAgRBk5vzTf2z5brbjlQkYNq2+wioUwMN/eXYdC4JMRnJGNyuKtTu0jxHAwrJKuQb0OcfI1unY/quWATUqoqGFZ2w6XwKojOSENq5LlRlHQs/UpTbPgDdbynz/Jzn28yRynhM2/IfAurVQ8NKLth05gmiM2MR2qMxVO4O+T+cz3+jL809aiTTFt+CRPZsuVSG12QxmL7+HAKatkNQNW9sOnEb0Zm3EPpKKJTe7jlHw6TPvgDI+7NU+iy3gM9a3m/3cz8DdVt1RcOq3th07HrOZ2AglJ5OBXzW8raneva6PX9E4bnnr9lFY/q6KAQ0aYOgap7YFHEH0Zl3EDqoN5ReZZ7bt6B2dN8D3bfn2TfWr9k/wPS1pxHQtA2Cqnhi04mcnIEhms/0c9vne17gUUXkO6r0mtNdTF95HHVbdUVQdV9sPnYd0YprGPTaa8j2di+gDcmzfwv9CiK/ER5XEfbLLtTtMEDzO+fIeUQrLmLQ2Lc0OUUScrIkuu9H3j5IJBjhdQVhi7ahbvAQBNWqhE2HziJaEYWBEz5EVnkP6L4Xkjzdz9Ou9lv/nNznjx4AGOF9CWELt6JupyEIqlUBmw+dRbTinOb3p7ebbjt5jyDkbUvnyNTzn0fNdiOcbyJsxd8IaNFZ89/OsRuIzvwPoUP65fxee/5I6nP/jTx/BEZn7M/+fc3+Yc5nrR2Cqnph04lbiM68jdAhL0Pp6ZJn/yJefz285nQb01ce14ynejlsOvYvojP/Regrg6H0ciliT0Fcf5hSiRLVe+3j44OIiAhMmDABYWFhyL16sEQiQffu3bFw4UL4+PiUeD96hPREclIilixaiNjYJ6haoyYUrd4AnD3R3ARHQgrKqFmrNhYsXgJ//wp6tzGibVUAwMZ3Wussn7L2PDafegC/so7oWt8XALBnWgedbYbOO4HIm5q7v3++9QqUagELRjWBo70Mx2/EYcra81AbcFJI/vHUwoKff0AFh1RIbh0GnsZA8jQaktRHkOT+/DQGSH2s+YNQT73KA0mNnbDwsgueZEhRywNY3KcM/GpWg8rJA3DyhODkAcHRA3DO+dfJA4KTByDP+T+MvNNI1Epop5nk/CxRK9FFrcLUXUewYMt+xJ5NQc3KvlgwsyfK1a2CLLUSEm0bymd/jOa2JQiAnTynCJBDyP0551/BzlFbTHSWyfHh9n1YsG4TYs8loGbNGpi/ZDq8m7eEwkRzZ7t2ARIqrsXiZUsRuz5S81lbshw+TZtB9YJt5/2IdKveCYnyipqc3M/0L8vg27QZ9H+Hi9etDZDouxaLli1FbOwpTc7/VqB802Yw4ErcxeoyBJgakJPz50PUrFUb83+dj3JNmyHLlDkAPgzKyTkQrRnPr/NQvmkzFH6dOyNyBgJT6+Tk7L6W87p9b9KcriFAQtWcjN8vlth7k/sZWFzCn4GuPYCEKjnj2fJPTs5K0+d0AxIq536mSzCnE5Dgn/O6bch53ZasMMnvgry6NQESPRvlvD8HTfo7R5vREEgsG5iTsVebYfLfNw1eQaJ7vXw5Jn/N2gGJfjmfgd/OlNxnOu9nTfvfaAl81jpqPmvGjEdUf5g+j+e0mJSo7hOSV2JiIm7evAlBEFCrVi14eHgUv1MRDLlPyPP2/hOD9zacRW0fV+x4u/0L9cNUCrpPSEm49n0B099U2ZriIU8xIUmNgSQlBsh9/jQGkuw0vTIEiVRbPGj/dfSA4OyZU0SULXAdHFz5C4GIiMiGiPo+IX0XWboLhcrYMcHSXTCYaN9qDw8PNGvWrPgNzSDylmkuzWsN7KGErzQBFaWxqCiNg+zvq88KjdyjF2mxkOh5kpogd4fg6pfz8Adc/SC4+kJw9X+2rEy5nJN7iYiIiKg0EG0RIiYnb2mmLbWsbrorI1mKHFmoKI1DhZxHRWms5meZ5mcfSRKkkjwFxtGC2xGkdoCLHwS3ZwWG4OL7rNBw0zyHQ5mCGyAiIiKyJrw6lkmxCCnG45RM3I5Lg1QCNK3qaenuFMsF6ZriQlZAoSGNQzlpSrFtZAr2eKj2xkO1N1o3DQJyj1q45fzr4geU8eZ/jERERERkFBYhxTiZMxUrwN8dbk6F30jRnCSJt9Hd/rR2ylTeQsNdml7s/k8FR02RofLGA3U5bcGR+3Oc4Ibcq2Bc61P8JZGJiIiIiAzBIqQYkdqpWBY+H0SVDemNXZBFLYXszl/4pYgr3CWqXfAgp7DIW1w8VHvjvrocUgRnGHKpPSIiIqJSjxfDMSkWIUUQBAGR/2mKkBaWOh8k+T7szq2E7PwqSFIfa/olkeJCdlXcV5fPKTJ0j2qkw7GYRomIiIiILIdFSBEeJGYgOjkDdlIJmlR5sUsEG0StgvTWIciilkJ6c5/2PhpCmfJQNXwNysaj0O+zC+brDxERERGRCbEIKULu+SANKpaFs4MZXqq0WMjOr4bs7HJIk+9pF6uqtoeq8Rio6/TW3MUaAMAihIiIiMhseEEek2IRUgSznA8iCJDci4Bd1FJIr+2ARK25d7Hg6A5Vg1ehajIGgletkssnIiIiIjIzFiGFEARBe3+QEjkfJDMJsksbIYtaBmncNe1idYWmUDUeA1XAAMDeyfS5REREREQWxiKkELdiUxGbqoDcToqGlcqarF1J9FnIzi6D7PLvkGRrLqcr2JeBKjBUc9TDN0ivdq59b55L53oMWmKWnMTN48ySQ0RERGQUXh3LpFiEFCJ3Klbjyh6Q28terLHsdMgu/6450TzmnHaxulxdqJq8DlXgYMDR/cUyiIiIiIisBIuQQkSaYCqWJO46ZFHLILu4HhJFMgBAkDlAXbcflI3HQKjUklU1EREREZU6LEIKoFYLOHU7twgx8KR0VRak13bCLmoZpPeOPWuzbFWoGo+GKmg4UMZC9xwhIiIiIqNI+MWxSbEIKcD1xylIzsiGs4MMgRX0nyYlO7UYdse/gyQtFoDmpoLq2j2hajwa6urBvLQbERERERFYhBQodypWs6qesJfpVzjIjn8P+8OfAQAEVz+oGo6EstFrgFuFEusnEREREZE14lfzBTD0fBDZ6V+0BUh2h5lQTLoEZYcwFiBERERENkIikYj2oa/w8HA0a9YMrq6uKF++PPr164fr16/rbDNq1Kh87bds2VJnG4VCgbfffhve3t4oU6YM+vbtiwcPHhj0erIIeY5SpcaZOwkA9LtJoez8atjvm6rZt900qNp9mOeu5kRERERE4nD06FG89dZbiIyMxP79+6FUKtGtWzekpaXpbNejRw/ExMRoH7t379ZZ/95772Hr1q3YsGEDjh07htTUVPTu3RsqlUrvvnA61nMuRycjTaGEu5M9XvJ1K3Jb6eXNsPvjbQCAssUkKNuHmaOLREREREQG27t3r87z5cuXo3z58oiKikL79u21y+VyOXx9fQtsIzk5GUuXLsXq1avRpUsXAMCaNWtQqVIlHDhwAN27d9erLzwS8pzcqVjNq3lCKi388Jb0xm7Ybx8PCQQoG4+GsssXvNwuERERka2SiPehUCiQkpKi81AoFMUOKTlZcwsJT09PneVHjhxB+fLlUbt2bYwdOxZPnjzRrouKikJ2dja6deumXebv74/AwEBEREQUm5mLRchzTupxPoj01mHY/z4SErUSqsAhUIbMZQFCRERERBYRHh4Od3d3nUd4eHiR+wiCgMmTJ6Nt27YIDAzULg8JCcHatWtx6NAhfP/99zh9+jSCg4O1Rc2jR4/g4OAADw8PnfZ8fHzw6NEjvfvMIiSPLKUKUXc154MkXTyEkG7BaNaoPoaGDsDZqDMAAMm9E7DfNAwSVRZUL/VBdt+FRl96d+P6tQVmmNqL5EwZ2BDHvu2HJ+tH4e6KEfgtrBtq+etetnjJOx2QsW2czuPoNy/na6tFnfLY81kvxG0YjZi1I7Hvi95wdDD8bvTW8LqJLceWxsIccefY0liYI+4cWxoLc+hFhYWFITk5WecRFlb0aQKTJk3CxYsXsX79ep3lQ4YMQa9evRAYGIg+ffpgz549uHHjBnbt2lVke4IgGHSSPIuQPM7fT4JCqYZ73D9YvmAuxo6bgI2bt6Fx4yaYOH4sHp3/Ew4bQiHJToeqRldk918GSI07rWbvnt2Y83V4voyY6GiTjulFc9rV88PiPVfQYep29P50F2RSCf74tCec5brj3hd1D1VHrdY++n2uO+ewRZ3y2D6rJw6ef4B2H25D2ylbsXjXZajVglnHUxpzbGkszBF3ji2NhTnizrGlsTDHelj6ClhFPeRyOdzc3HQecrm80LG8/fbb2LFjBw4fPoyKFSsWOW4/Pz9UqVIF//77LwDA19cXWVlZSExM1NnuyZMn8PHx0fv1ZBGSR+75IA63jqL/wIEYMCgU1WvUwNSwGfAt54HN30yAJOspVFXaIXvQakDmYHTW6pXL82f4+eK3jeuL39mMOS9/tgdrDt3A1fuJuHQnAePnHUXl8q5oVEN3ulqWUo3HSRnaR2Kq7jzEOWNaYeGuf/Ddlgu4ej8R/8WkYOuJ28hSqs06ntKYY0tjYY64c2xpLMwRd44tjYU5ZE6CIGDSpEnYsmULDh06hGrVqhW7T3x8PO7fvw8/Pz8AQJMmTWBvb4/9+/drt4mJicE///yD1q1b690XURYharUay5YtQ+/evREYGIj69eujb9++WLVqFQTBsG/ODXHqVjygVuJpzG20at1Wu1wSfxNtXR/gfIwa6gpNkT14PWDvZHROdlYWrl65rJMBAK1at8GF8+eMbtccOW7OmsLr+SKjXaAf7q4YgYsLBmPBxHYo5+6oXVfO3RHN6/ggNjkDh7/uizsrhuPPL3qjdV39q+WSGo+t59jSWJgj7hxbGgtzxJ1jS2NhDpnbW2+9hTVr1mDdunVwdXXFo0eP8OjRI2RkZAAAUlNTMWXKFJw4cQJ37tzBkSNH0KdPH3h7e6N///4AAHd3d7z++uv44IMPcPDgQZw7dw7Dhw9H/fr1tVfL0ofoLtErCAL69u2L3bt3IygoCPXr14cgCLh69SpGjRqFLVu2YNu2bUW2oVAo8l0RQJDJizwslZ6lxIUHiYAiDYJaDS8vzT1CJEl34bCmL7xlqYhVlkXW0M2A3PWFxpiYlAiVSqXNyOXl5Y24uNgXarukc74Z0wrHr8Tgyr1nh+D+jLqPLcdv4V5sKqr6uGLWsKbY81lvtP5gC7KUalTz0VzqeMaQJghbEYmLt+Pxaqfa2P1ZbzR5ZxP+i0mx2HhsPceWxsIccefY0liYI+4cWxoLc6yLIec7iNWiRYsAAB07dtRZvnz5cowaNQoymQyXLl3CqlWrkJSUBD8/P3Tq1AkbN26Eq+uzv39/+OEH2NnZYfDgwcjIyEDnzp2xYsUKyGT6n+sruiJkxYoV+Ouvv3Dw4EF06tRJZ92hQ4fQr18/rFq1Cq+99lqhbYSHh2P27Nk6y2Z8/Almzvq00H3O3k1EtkqAr5sjEpHzQXsaA/u1L0Py9CHUzlUANx/AyaPQNgz1/IfZ0BN6zJ3zw7g2qF/VE53Ddugs33z8lvbnK/cScfZmLK4vGYaQppWxPfIOpDlZS/+8itWHbgAALtw+gY4N/DGycx3MWnPaIuMpTTm2NBbmiDvHlsbCHHHn2NJYmEPmUtyMIicnJ+zbt6/YdhwdHTFv3jzMmzfP6L6IbjrW+vXr8dFHH+UrQAAgODgY06dPx9q1a4tso6ArBHw4regrBGgvzftSFchkMsQ9uAWHtf0gTbwNddkqiK3cC17lDJs+VBiPsh6ajLg4neUJCfHw8ir80sCWzJk7tjV6N6+C7jP/wMP4tCK3fZSYgXuxqajpp7mKVkxiOgDg6n3dE5iuP0hCpXIuevfBGl83S+fY0liYI+4cWxoLc8SdY0tjYQ6VZqIrQi5evIgePXoUuj4kJAQXLlwosg1DrxAAAJG3NP+xtK7tg7ovvYRTaz+HNO4aBFd/ZL+6A5FnLiCoYSPDB1QAewcH1A2oh8iI47p9iIgwWYYpc34Y2wYvt6yGHh//gbtPnha7vaerHBW9y2iLj7tPniI6Pg21K5TV2a6mvzvuxabq3Q9re93EkGNLY2GOuHNsaSzMEXeOLY2FOdbF0lfAKuphjUQ3HSshIaHIy3v5+PjkuyTYi3qamY3L0Zo7Rras6Igy1R9h+q501Hcvj3pjF2DzL+sQExOD0CFDTZY5YuRozJg+FQGBgQgKaoTfN200eYYpcn4c3wZD2tdE6Fd/IjUjGz5lNSfkJ6dnITNLhTKOdpg5tAm2nbiNmMR0VCnvis+GN0N8SiZ2RN7RtvPDtguYObQpLt2Ox4Xb8RgeXBt1KpTFsDn7C0kumfGUxhxbGgtzxJ1jS2NhjrhzbGkszKHSSnRFiEqlgp1d4d2SyWRQKpUmzTx9JwFqAajlaYeKe0ehistlJDf3woIb/oh9/R3UrFUbCxYvgb9/BZNl9gjpieSkRCxZtBCxsU9KJMMUOeND6gEA9n/ZR2f52J+PYM2hG1CpBdSr4olhHWujbBkHPEpMx9F/ojHiu4NIzczWbj9/5z9wtJdhzuut4OEix6U78ej96S7cflT8kRVTjqc05tjSWJgj7hxbGgtzxJ1jS2NhDpVWEqEkr3lrBKlUipCQkEKnTykUCuzduxcqlcqgdjOLqFvCd1/Buoh/saPcYtR9GgHBwQVZr+6AUKGJQRm2yGPQErPkJG4eZ5YcIiIiEi9H0X09/oz7K6st3YVCJa8fYekuGEx0b/XIkSOL3aaoK2MZ49R/j/GD/QLUfXoSgp0Tsob8xgKEiIiIiKiEiK4IWb58uVnzElIz8HrCXPSWnYQgtUd26BoIVdqYtQ9ERERERKWJ6IoQsxIEZG6fjEGyv6CCFKoBy6Guof+dHomIiIiolLDOi1CJlugu0Ws2ggC7Q7NQ/fY6qAUJtlb5GOqX+hS/HxERERERvZBSW4TIjn0LuxM/AwA+Ur4Op6bDLNwjIiIiIqLSoVROx5JFzof90S8BAJ9lj8Bv6mBMrupl4V4RERERkVhZ600BxarUHQmRnV0O+wMzAACX6ryNZaoQBPi7w83J3sI9IyIiIiIqHUpVESK9tBF2u98HAChbv4fl0kEAgJbVeRSEiIiIiMhcSk0RIr22A/Y7JkACAcqmY6Hs9CkibycAAFpU97Zw74iIiIhIzCQSiWgf1qjUFCH2W8ZAIqigbPAqlN3n4EFiBqKTMmAnlaBJFQ9Ld4+IiIiIqNQoNSemS9TZUAUMgLL3PEAiReStOABAg4pl4exQal4GgyVuHmeWnNZfHSrxjIiPgks8g4iIiIiKV2r++lbVH4rs3vMBqQwAEHkrHgDPByEiIiKi4lnrtCexKjVFSPbLv2h/FgQBJ3OKEJ4PQkRERERkXqXmnJC8bsWmIjZVAbmdFA0rlbV0d4iIiIiISpVScyQkr9ypWI0re0BuL7Nwb4iIiIhI7Dgdy7RK5ZGQk7c5FYuIiIiIyFJKXRGiVuc9H4QnpRMRERERmVupm451/XEKkjOy4ewgQ2AFd0t3h4iIiIisAWdjmVSpOxKSez5Is6qesJeVuuETEREREVlcqfsrPJKX5iUiIiIisqhSNR1LqVLjzJ0EALxJIRERERHpj1fHMq1SdSTkcnQy0hRKuDvZ4yVftyK33bh+LUK6BaNZo/oYGjoAZ6POmLw/5siwlpzRbapg9etN8fe09jjwQVt8P7g+qng559tufIdq2Pd+G0SEdcCS1xqherkyOuvtZRJM7VELB6e0xfHpHfDDkPoo7yo3+3jElmNLY7GlnKgzp/H2xDfRpWNbBNWrg0MHD5i0fXPnALbz3jBH/Dm2NBbmUGlUqoqQ3KlYzat5QiotvJrdu2c35nwdjrHjJmDj5m1o3LgJJo4fi5joaJP1xRwZ1pTTpEpZ/HbmAUYui8KENedhJ5Vg4asN4Wj/7CM6snVlvNqyEr7ZcwMjfj2D+NQsLBreEM4Oz+71MqV7LXR6qRzCfr+MMSui4Oxgh59eaYAi3u4SGY+YcmxpLLaWk5GRjjp16mD6jFkma9OSObb03jBH3Dm2NBbmUGlVqoqQk3qeD7J65XL0HzgQAwaFonqNGpgaNgO+fr74beN6k/XFHBnWlDNp3QXsvPAIt2LT8O/jVHyy4yr8yjoiwO/ZEathLSph6d93cOhaLP6LTcOs7VfgaC9FSKAPAMBFLkO/Rv744c+bOHU7EdcfpWLG1suoWd4FLap7mnU8YsqxpbHYWk7bdh0w6d330aVrN5O1ackcW3pvmCPuHFsaC3Osh0QiEe3DGpWaIiRLqULU3eLPB8nOysLVK5fRqnVbneWtWrfBhfPnTNIXc2RYe46rXHO6UnJGNgCgQllHlHOVI/JWwrNclYCou0loUElzqeW6fm6wl0lxIs82calZ+O9JKoIq6n85Zmt+3SyRwRwCbO+9YY54c2xpLMyh0kx0RUjPnj2RnJysff7ll18iKSlJ+zw+Ph4BAQFFtqFQKJCSkqLzOH0rFgqlGt4uctQo51LovolJiVCpVPDy0i1UvLy8ERcXa9ygLJBh7TmTu9XEuXtJ+C82TdOWiwMAID41S2e7hNQseOes83JxQJZSjaeZSp1t4tOytfvrw5pfN0tkMIcA23tvmCPeHFsaC3OoNBNdEbJv3z4oFArt82+++QYJCc++2VYqlbh+/XqRbYSHh8Pd3V3n8eOaHQA0d0nX57DV89sIgmDyw13myLDGnOkhtVHLxwVhv1/WJxSCUOwmxW5T8H7W9bpZOoM5BNjee8Mc8ebY0liYYx0sPeWK07FKmPDcX4vPP9dHWFgYkpOTdR72FesBAFpUK/rSvB5lPSCTyRAXF6ezPCEhHl5eprm3iDkyrDVnao9aaF/bG+NWncOTp8+K0dwjIM8f0fAsY4/4tCztNg52Urg66l552tPZHglpukdQzDUeS+fY0lhsMceW2Np7wxzx5tjSWJhDpZnoihBTkMvlcHNz03lceqiZ4lXc/UHsHRxQN6AeIiOO6yyPjIhAUMNGJumfOTKsMWdaj9oIfqk8xq8+h+ikTJ11D5MyEftUgZZ5TjC3k0rQpEpZXLyveW+vxqQgW6XW2cbbxQE1yrvgwoNk6MvaXjdLZzCHANt7b5gj3hxbGgtzqDQT3c0KCzqsZIrDTNkqAf7uTqjkmf/eE88bMXI0ZkyfioDAQAQFNcLvmzYiJiYGoUOGvnA/zJlhTTnTQ2ojpL4P3t94CekKFbzKaI54pCqUUCjVAIB1J+9jTNsquBefjnsJGRjTtgoys9XY88/jnG1V2HYuGu93rYnkjGwkZ2Tj/a41cfNJKk7mOVndHOMRU44tjcXWctLT0nDv3j3t84cPHuDa1atwd3eHn7+/1eXY0nvDHHHn2NJYmGNFrHPWk2iJrggRBAGjRo2CXK65wVxmZibefPNNlCmjuSld3vNFDKXv+SA9QnoiOSkRSxYtRGzsE9SsVRsLFi+Bv38Fo7MtkWFNOYObVQQA/Dqysc7yT7Zfwc4LjwAAKyPuwdFehuk968DNyQ7/PEzBxDXnkZ6l0m7//b6bUKkFfD0wEHJ7KU7fTsQn2y9CbeCsPmt53cSSwRzjXL78D94Y/Zr2+XdzwgEAfV/uj8+/+trqcmzpvWGOuHNsaSzModJKIhhz0kUJGj16tF7bLV++3KB2q07fhW8GBuHlRhWN6RaVsNZfHSrxjIiPgks8g4iIiIznKLqvx58p//pvlu5CoZ4sHWzpLhhMdG+1ocWFvroG+KBFMeeDEBEREREVxFqvQiVWoitCSsq8YU0t3QUiIiIiIoKNXh2LiIiIiIjEq9QcCSEiIiIiMhanY5kWj4QQEREREZFZsQghIiIiIiKz4nQsIiIiIqJicDqWafFICBERERERmRWLECIiIiIiMitOxyIiIiIiKganY5kWixAShYiPgks8w2/U2hLPAICYFa+aJYeIiIjIWnE6FhERERERmRWPhBARERERFYezsUyKR0KIiIiIiMisWIQQEREREZFZcToWEREREVExeHUs0+KRECIiIiIiMisWIUREREREZFacjkVEREREVAxOxzItHgkhIiIiIiKzYhFCRERERERmxSKkEBvXr0VIt2A0a1QfQ0MH4GzUGavMYI6u9/vUw8HPeuDe/wbjxoKBWPNee9T0cy10+x/GNEfimlfxZvc62mVlyzjgm9ea4tS3ffBw6RBc+rEfvh7RBG5O9mYfj5gymMMcc2UwhznmymCO+HPMSSKRiPZhjViEFGDvnt2Y83U4xo6bgI2bt6Fx4yaYOH4sYqKjrSqDOfm1rlsev+6/gW6f7sOAbw7CTibFlmmd4SyX5du2Z5OKaFLDC9EJ6TrL/Tyc4FvWCbPWnUWbsF2YuOQEOjfwx89jW5p9PGLJYA5zzJXBHOaYK4M54s8h6yYRBEGwdCfMIVOp/7avDg1F3YAAzJw1W7usX58QdArugnff/8Ak/TFHBnN0+Y1am2+Zl6scNxcNQq/P9yPi+pNn23o4Yf/sHhj0zSFsnNIRi/Zew+J91wtt++XmlfHLhNao8PpGPFg2zCzjEVMGc5hjrgzmMMdcGcyxTI6jiC+ZVOmt7ZbuQqHuL3jZ0l0wGI+EPCc7KwtXr1xGq9ZtdZa3at0GF86fs5oM5ujHzVkzhSoxTaFdJpEAi99sjXm7ruDaw2S923makQ2VWv+anp815thSji2NhTnizrGlsTDHykhE/LBCoqs3b926hWrVqr3Q/DaFQgGFQqGzTJDJIZfLi903MSkRKpUKXl5eOsu9vLwRFxdrdJ/MncEc/Xz5ahOcuP4EVx88Kzbe610PSrWAX4o48pGXh4sDPuxXHysO3TQom5815thSji2NhTnizrGlsTCHSjPRHQmpVasWYmOffUiHDBmCx48fG9RGeHg43N3ddR7ffhNuUBvPF0GCIJj8xB9zZDCncN+ObIZ6lcrijQXHtMuCqnpifPc6eOuXE3q14epkh41TOuH6w2R8s/WiwX0A+Fljjm3l2NJYmCPuHFsaC3OoNBLdkZDnT1HZvXs3wsMNKyDCwsIwefJk3XZlxR8FAQCPsh6QyWSIi4vTWZ6QEA8vL2+D+mHJDOYU7ZvXmiKkcQX0/GI/ohMytMtb1SmHcm6OuPRTP+0yO5kUX7zaGBN6vISg95/NB3VxtMPmD4ORlpmN4T8ehVJl2OlV/Kwxx5ZybGkszBF3ji2NhTnWhUWUaYnuSIgpyOVyuLm56Tz0mYoFAPYODqgbUA+REcd1lkdGRCCoYSOT9M8cGcwp3JzXmqJ300ro+9VB3ItN01m38fhttP1oF9rP2K19RCekY96uqxg455B2O1cnO/w+LRhZKjWGzT0KRbbaYuOxdAZzmGOuDOYwx1wZzBF/Dlk/0R0JKeh6x+auPEeMHI0Z06ciIDAQQUGN8PumjYiJiUHokKFWlcGc/L4b1QyDWlXFsB+OIjUzG+XdHQEAKenZyMxWITE1C4mpWTr7KFVqPE7KwM2YpwA0R0B+n9YZzg4yjF/0F1yd7OGac4+QuBTdc5FKejxiyWAOc8yVwRzmmCuDOeLPIesmuiJEEASMGjVKe+QiMzMTb775JsqUKaOz3ZYtW0qsDz1CeiI5KRFLFi1EbOwT1KxVGwsWL4G/fwWrymBOfq93qQ0A2DWzq87yib+cwPq/b+nVRlA1TzSrqTmkfG6u7iXxGry3Ta82cvGzxhxbyrGlsTBH3Dm2NBbmWA9OxzIt0d0nZPTo0Xptt3z5coPaNeQ+IWSbCrpPSEmIWfGqWXKIiIhsjZjvE1LlnZ2W7kKh7v7cx9JdMJjo3mpDiwsiIiIiIrIuoitCiIiIiIjEhtOxTMsmr45FRERERETixSKEiIiIiIjMitOxiIiIiIiKwelYpsUjIUREREREZFYsQoiIiIiIyKw4HYuIiIiIqDicjWVSPBJCRERERERmxSKEiIiIiIjMitOxqNSIWfGqWXLqTN5plpzrc/uYJYeIiIh4dSxT45EQIiIiIiIyKxYhRERERERkVpyORURERERUDE7HMi0eCSEiIiIiIrNiEUJERERERGbF6VhERERERMXgbCzT4pEQIiIiIiIyKxYhRERERERkVpyORURERERUDF4dy7R4JISIiIiIiMyKRUghNq5fi5BuwWjWqD6Ghg7A2agzVpnBHMvkTOxaEzs+aIfLc0IQ9WU3LHmjGaqXL6NdbyeVYHrfutg3vQOufhuCU593xdzhDVHeTa7TziutK2PD263wz5weuPtzH7g5GXfw0hpeM+bYRo4tjYU54s6xpbEwh0ojFiEF2LtnN+Z8HY6x4yZg4+ZtaNy4CSaOH4uY6GirymCO5XJa1PTCqr9vo9/cvzF8QSTspBKsntgSTg4yAICTgwyBFd3x874b6PXtXxi/9DSqlXfB0nHNddpxcpDh6NVYLPjzpsXGwhzmiCmDOcwxVwZzxJ9jbhKJeB/WSCIIgmDpTuTKyMjAwYMH0bt3bwBAWFgYFAqFdr1MJsPnn38OR0dHg9vOVOq/7atDQ1E3IAAzZ83WLuvXJwSdgrvg3fc/MDjbUhnMsUxOnck78y3zdHHAua+6I/Sn4zj1X0KB+zWo7I6dU9qj1ScHEJ2YobOuZU0vbHynNepP24OUDM2H+frcPiU+FkMwhzm2NBbmiDvHlsbCHF2OIj5bufbUvZbuQqFuzOlh6S4YTFRHQlatWoVffvlF+3z+/PmIiIjAuXPncO7cOaxZswaLFi0q0T5kZ2Xh6pXLaNW6rc7yVq3b4ML5c1aTwRxx5bjm/FZNSs8uYht7qNUCUjIK38ZQ1vyaMce6cmxpLMwRd44tjYU5VJqJqt5cu3Yt3n//fZ1l69atQ/Xq1QEAa9aswYIFC/Jt8zyFQqFzBAUABJkccrm8kD2eSUxKhEqlgpeXl85yLy9vxMXF6jMMUWQwR1w5H/evh1P/xeNGzNMC18vtpJjety62Rz1EqiGH7Yphza8Zc6wrx5bGwhxx59jSWJhjXXh1LNMS1ZGQGzduoHbt2trnjo6OkEqfdbF58+a4cuVKse2Eh4fD3d1d5/HtN+EG9eX5D5ogCCb/8JkjgzmWz/k8NBAv+bvh7ZVnC1xvJ5Vg3qgmkEokmLnpklF9LY61vWbMsd4cWxoLc8SdY0tjYQ6VRqI6EpKcnAw7u2ddio3VrZjVanW+IxwFCQsLw+TJk3WWCbLij4IAgEdZD8hkMsTFxeksT0iIh5eXt15tiCGDOeLImT0wEF0CfTH4p+N4lJSZb72dVIIFo5ugkpcTXpl3wqRHQQDrfM2YY505tjQW5og7x5bGwhwqzUR1JKRixYr4559/Cl1/8eJFVKxYsdh25HI53NzcdB76TMUCAHsHB9QNqIfIiOM6yyMjIhDUsJFebYghgzmWz/lsUCB6BPnilfkncD8hI9/63AKkWrkyeHVBZJHnixjL2l4z5lhvji2NhTnizrGlsTDHulj6Cli2dnUsUR0J6dmzJ2bNmoVevXrluwJWRkYGZs+ejV69epV4P0aMHI0Z06ciIDAQQUGN8PumjYiJiUHokKFWlcEcy+V8EVoffZtUwNhfTyMtU4lyrpoiOCUzG4psNWRSCRa93hSBFd0x5pdTkEkk2m2S0rOQrdJctK6cqxzl3OSoWk5zj5E6fm5IUyjxMDF/UVNSY2EOc8SUwRzmmCuDOeLPIesmqiLko48+wm+//YY6depg0qRJqF27NiQSCa5du4b58+dDqVTio48+KvF+9AjpieSkRCxZtBCxsU9Qs1ZtLFi8BP7+FawqgzmWyxnRrioA4Ld3Wuss/2DNOWw+9QB+ZR3Rrb4vAGDv9A462wz5OQKRN+MBAK+2rYL3Q+po121+r422HXONhTnMEVMGc5hjrgzmiD+HrJuo7hMCALdv38aECROwf/9+5HZNIpGga9euWLhwofZKWYYy8VR7okIVdJ+QkqDvfUKIiIishZjvExLw0Z+W7kKhrnzVzdJdMJjo3upq1aph7969SEhIwM2bmrtE16xZE56enhbuGRERERERmYLoipBcnp6eaN68uaW7QUREREREJibaIoSIiIiISCys9SpUYiWqS/QSEREREZHtYxFCRERERERmxelYRERERETFkHA+lknxSAgREREREZkVixAiIiIiIjIrTsciIiIiIioGZ2OZFo+EEBERERGRWfFICJGJXZ/bxyw5zT87YJacU7O6mCWHiIiISg8WIURERERExeDVsUyL07GIiIiIiMisWIQQEREREZFZcToWEREREVExOB3LtHgkhIiIiIiIzIpFCBERERERmRWnYxERERERFYOzsUyLR0KIiIiIiMisWIQQEREREZFZcToWEREREVExeHUs0+KRkEJsXL8WId2C0axRfQwNHYCzUWesMoM54s2JOnMab098E106tkVQvTo4dPCAwW283q4q1o1vhhMzOuLI1Pb48ZUGqOrlnG+7CZ2q48CUdjj1cScsHd0ENcqVKbTNhSMa4uJnXdDppXIG98dW3hvmiDuDOcwxVwZzxJ9DhgkPD0ezZs3g6uqK8uXLo1+/frh+/brONoIg4NNPP4W/vz+cnJzQsWNHXL58WWcbhUKBt99+G97e3ihTpgz69u2LBw8eGNQXFiEF2LtnN+Z8HY6x4yZg4+ZtaNy4CSaOH4uY6GirymCOuHMyMtJRp04dTJ8xy+g2mlYtiw0nH2D4ktMYt/IsZFIJFo9sBCf7Z/9pj25bBSNaVUb4rmsY9sspxKUq8MvIxnB2kOVrb3iryhAE4/piS+8Nc8SbwRzmmCuDOeLPIcMdPXoUb731FiIjI7F//34olUp069YNaWlp2m3mzJmDuXPnYv78+Th9+jR8fX3RtWtXPH36VLvNe++9h61bt2LDhg04duwYUlNT0bt3b6hUKr37IhEEY//ksC6ZSv23fXVoKOoGBGDmrNnaZf36hKBTcBe8+/4HJumPOTKYI/6cXEH16uCHnxcguHMXvfdp/ln+IycezvY4Or0DRi89g6i7SQCAgx+2w5oT97D82F0AgL1MgsNT2+PH/Tex+cxD7b61fVwwf3hDvPLLKRye2h7vrruAw9dicWqWfn2ytfeGOeLMYA5zzJXBHMvkOIr4RIHGnx2ydBcKdXZWsFH7xcbGonz58jh69Cjat28PQRDg7++P9957D9OmTQOgOerh4+ODb775BuPHj0dycjLKlSuH1atXY8iQIQCA6OhoVKpUCbt370b37t31yhbdkZCUlBS9HiUlOysLV69cRqvWbXWWt2rdBhfOn7OaDOaIP6ckuOT89k7OyAYAVPBwQjlXOU7cTNBuk60SEHUnCQ0ruWuXOdpL8U1oIL7adR3xqVkG59rae8MccWYwhznmymCO+HNIl0KhyPe3skKhKHa/5ORkAICnpycA4Pbt23j06BG6deum3UYul6NDhw6IiIgAAERFRSE7O1tnG39/fwQGBmq30YfoipCyZcvCw8Oj0Efu+qIY+0YAQGJSIlQqFby8vHSWe3l5Iy4u1uhxmTuDOeLPKQkf9qiNs3cTcfOJ5rCqt4sDACA+TffzH5+mgJerXGe/C/eTceSaceOztfeGOeLMYA5zzJXBHPHnkK7w8HC4u7vrPMLDw4vcRxAETJ48GW3btkVgYCAA4NGjRwAAHx8fnW19fHy06x49egQHB4d8f4/n3UYfojvodfjwYe3PgiCgZ8+e+PXXX1GhQgW92wgPD8fs2bN1ls34+BPMnPWp3m08fwUEQRBMflUEc2QwR/w5pvJRrzqo5eOCUUvzn/z3/KRLCSTahR3reKN5dU8MXnTyhftga+8Nc8SZwRzmmCuDOeLPMScx9z8sLAyTJ0/WWSaXywvZWmPSpEm4ePEijh07lm+dMe+foe+x6IqQDh066DyXyWRo2bIlqlevrncbBb0RgqzoNyKXR1kPyGQyxMXF6SxPSIiHl5e33n2wdAZzxJ9jStN71kHHl8ph9NIzeJzy7KhHXM7UKm8XufZnAPAs46CddtW8uicqeTjheJjuf3tzhzbA2ZzzSopja+8Nc8SZwRzmmCuDOeLPIV1yubzYoiOvt99+Gzt27MBff/2FihUrapf7+voC0Bzt8PPz0y5/8uSJ9uiIr68vsrKykJiYqHM05MmTJ2jdurXefRDddCxTkMvlcHNz03no+8bYOzigbkA9REYc11keGRGBoIaNTNI/c2QwR/w5phLWqw46B5TDG8uj8DApU2fdw8QMxD5VoFVNT+0yO5kETaqWxfn7mnmgS/++g0ELIzF40UntAwC+3XMDs7bqXpKvMLb23jBHnBnMYY65Mpgj/hwyjiAImDRpErZs2YJDhw6hWrVqOuurVasGX19f7N+/X7ssKysLR48e1RYYTZo0gb29vc42MTEx+OeffwwqQkR3JEQMRowcjRnTpyIgMBBBQY3w+6aNiImJQeiQoVaVwRxx56SnpeHevXva5w8fPMC1q1fh7u4OP39/vdqY0bsOQur74t31F5CWpYJXzjkgqZlKKJRqAMCaE/fweruquBufjnvx6XijfTVkZqux+6Jm3mZ8alaBJ6PHJGfmK2qKYkvvDXPEm8Ec5pgrgznizzE3Ec/G0ttbb72FdevWYfv27XB1ddWew+Hu7g4nJydIJBK89957+Oqrr1CrVi3UqlULX331FZydnTFs2DDttq+//jo++OADeHl5wdPTE1OmTEH9+vXRpYv+V/m0iiLE3HPweoT0RHJSIpYsWojY2CeoWas2FixeAn9//c9LEUMGc8Sdc/nyP3hj9Gva59/N0ZxA1vfl/vj8q6/1amNI80oAgOVjmuosn7nlMnacj9GsO3YXjvYyzOj9Etwc7XDpYQreXHUW6Vn6X8tbH7b03jBHvBnMYY65Mpgj/hwy3KJFiwAAHTt21Fm+fPlyjBo1CgAwdepUZGRkYOLEiUhMTESLFi3w559/wtXVVbv9Dz/8ADs7OwwePBgZGRno3LkzVqxYAZks/z3ICiO6+4QMGDBA5/nOnTsRHByMMmV07/C8ZcsWg9o15D4hRNagoPuElAR97xNCRET0osR8n5CmXxwufiMLOTOzk6W7YDCTvtUnTpzAH3/8AScnJ4wZMwb+ek4pycvd3V3n+fDhw03VPSIiIiIio4j56ljWyKgiZMqUKZg/fz6io6O1NzfZvHkzhg4dCrVaMw99/vz5iIqKMujSuoDmcBAREREREdkuo66OdfjwYXTq1ElbgADAxx9/DHd3d6xatQpz5sxBfHw8vv/+e5N1lIiIiIiIbINRR0Lu3buHNm3aaJ//+++/uH79Oj755BPt9Km///4bu3fvxty5c03TUyIiIiIiC+FsLNMy6khIamoqXFxctM+PHTsGiUSCkJAQ7bKAgAA8ePDgxXtIREREREQ2xagixM/PD9evX9c+37t3L1xcXNCkSRPtspSUFIPu3EhERERERKWDUdOxOnTogPXr12PBggVwdHTEtm3b0LdvX51rA9+8eVPnNvBERERERNaKV8cyLaOOhMyYMQNOTk545513MHbsWNjb2+OTTz7Rro+NjcWRI0d0zhshIiIiIiICjDwSUrNmTVy5cgW///47AKB3796oWrWqdv3du3cxceJE7e3diYiIiIiIchl9s0I/Pz9MmjSpwHVNmzZF06ZNje4UEREREZGYcDaWaZn0jum3b9/GgQMH4OTkhP79+6NMmTKmbJ6I8jg1q4tZcupO2WWWnKvf9TJLDhEREVmeUeeEfPPNN6hVqxYSExO1y44cOYL69evjzTffxMiRI9GkSROd9URERERERICRRcj27dtRoUIFeHh4aJd9+OGHUKvVmD17NiZMmIAbN27gp59+MllHiYiIiIgsRSKRiPZhjYwqQm7duoV69eppn9+/fx9RUVF46623MHPmTMyfPx+dO3fWnrhORERERESUy6giJCkpCWXLltU+z71jep8+fbTLGjdujHv37r1wB4mIiIiIyLYYdWK6j48P7t69q32+f/9+yOVytGjRQrssMzPTag8PERERERHlxT9rTcuoIqRZs2bYvn07du3aBUdHR/z222/o2LEj5HK5dptbt27B39/fZB0lIiIiIiLbYNR0rI8++ghKpRJ9+/ZFt27dkJmZibCwMO36p0+f4vDhwzpHRoiIiIiIiAAjj4Q0btwYkZGRWL16NQBg0KBBaNmypXb9hQsX0LVrV94xnYiIiIhsAk8zMC2jb1YYFBSEoKCgAte1bdsWbdu2NbpTRERERERku4yajvW8tLQ0xMTEIC0tzRTNERERERGRDTO6CMnOzsZXX32F2rVrw83NDRUrVoSbmxtq1aqFr776CllZWabsp9ltXL8WId2C0axRfQwNHYCzUWesMoM5zDFFxoQuNbBtchtc+ro7Tn/eBb+83gTVy5fR2aZ7A1+sfLM5or7oits/9kLdCm752qns5YzFY5rgzBddcPHrbpg/shG8XRzMPh7miPezxhzmiCmDOeLPMSeJRLwPa2RUEZKRkYFOnTrh448/xp07d1C7dm20b98ederUwb179/Dxxx+jY8eOyMjIMHV/zWLvnt2Y83U4xo6bgI2bt6Fx4yaYOH4sYqKjrSqDOcwxVUaLGp5YfewuBvx4HK8tOgmZVIJVbzaHk4NMu42zgwxnbidgzh/XCmzDyUGGVROaQxCAVxecROhPJ2BvJ8WvY5sZ9AvUlt4bW8uxpbEwR9w5tjQW5lBpZVQRMmfOHEREROCVV17BrVu3cPXqVRw+fBhXrlzB7du38eqrryIyMhJz5swxdX/NYvXK5eg/cCAGDApF9Ro1MDVsBnz9fPHbxvVWlcEc5pgqY9Qvp/H7qQf491EqrkY/xdR1F1HB0xn1K7prt9l65iHm7buJYzfiCmyjaTUPVPR0xofrLuB6zFNcj3mKD9ddQFCVsmhdy8us42GOeD9rzGGOWDKYI/4csm5GFSEbNmxA06ZNsWbNGlSsWFFnnb+/P1atWoWmTZtiw4YNJumkOWVnZeHqlcto1Vr3xPpWrdvgwvlzVpPBHOaUZIark+aaFknp+k+7dLCTQhAEZCnV2mUKpRoqtYCm1T31asOW3htby7GlsTBH3Dm2NBbmWBeJRCLahzUyqgi5c+cOunTpUuQ2nTt3xp07d4xp/oUpFAqkpKToPBQKhV77JiYlQqVSwctL95tZLy9vxMXFmqR/5shgDnNKMmNmvwCc/i8BNx6l6r3PuTtJSM9SYVrfl+BoL4WTgwwf9a0LmVSC8m7y4huAbb03tpZjS2NhjrhzbGkszKHSzKgixNnZGbGxRX+QYmNj4ezsbFSnXlR4eDjc3d11Ht9+E25QG89XlYIgmLzSNEcGc5hj6ozPBtbDS/6ueGeVYd9oJaRlYdKKs+hcrzwuf9MDF8O7wdXRDpfuJ0OlLn7/vGzpvbG1HFsaC3PEnWNLY2EOlUZG3SekZcuW2LBhA9577z3Uq1cv3/orV65g48aN6NChwwt30BhhYWGYPHmyzjJBpt83rR5lPSCTyRAXpzuvPSEhHl5e3ibpnzkymMOcksj4dEA9dA70wZB5J/AoOdPg/f++HoeOXxyBRxl7KNUCnmYoceqzzngQn67X/rb03thaji2NhTnizrGlsTDHurCIMi2jjoTMmDEDCoUCzZo1w9tvv43Nmzfj77//xubNmzFp0iQ0a9YMCoUCYWFhBrc9YMAAvR5FkcvlcHNz03nI5foVIfYODqgbUA+REcd1lkdGRCCoYSODx2OpDOYwx9QZswfWQ/cGvnh1QSQeJLzYle8S07LxNEOJVrW84OUix4HLj/Xaz5beG1vLsaWxMEfcObY0FuZQaWbUkZDWrVtj/fr1eOONN7BgwQIsXLhQu04QBLi7u2PlypVo06aNwW27u7sXv1EJGzFyNGZMn4qAwEAEBTXC75s2IiYmBqFDhlpVBnOYY6qMzwYF4uUm/hj36xmkKlTwdtUU9U8zs6HI1sylcne2h7+HE3xyzu/IvY9IbIoCcU8152QNal4RNx+nIiE1C42remDWgAAsO3obt57of6NTW3pvbC3HlsbCHHHn2NJYmEOllVFFCAAMHDgQ3bt3x7Zt23Du3DmkpKTAzc0NjRo1wssvvwxXV1ej2l2+fLmxXTKZHiE9kZyUiCWLFiI29glq1qqNBYuXwN+/glVlMIc5psoY0bYKAGDD2610lk9ZdwG/n3oAAOgS6IPvhgVp180f2RgA8OPeG/hp778ANIXJ1N514O7sgIcJ6Viw/yaWHrlt9vEwR7yfNeYwRywZzBF/jrlxNpZpSQRBEEqi4fXr1+P06dOYO3duSTRvsEylpXtAZJ3qTtlllpyr3/UySw4REYmXo9Ffj5e8Dj8cL34jCzn6vuGzjyzNqHNC9PHnn3/ip59+KqnmiYiIiIjISom43iQiIiIiEgdeHcu0SuxICBERERERUUFYhBARERERkVlxOhYRERERUTE4G8u0eCSEiIiIiIjMSu8jIRMnTjSo4RMnThjcGSIiIiIisn16FyGLFy82uHFeRYCIiIiIbAH/rjUtvYuQw4cPl2Q/iIiIiIiolNC7COnQoUNJ9oOIiIiIiEoJXh2LiIp09bteZsmpO2WXWXLMNR4iIrItnI1lWrw6FhERERERmRWLECIiIiIiMitOxyIiIiIiKoaU87FMikdCiIiIiIjIrFiEEBERERGRWXE6FhERERFRMTgby7R4JISIiIiIiMxKryMhUqnUqFvVSyQSKJVKg/cjIiIiIiLbpVcR0r59+3xFSGJiIi5evAiZTIZKlSrBx8cHjx8/xv3796FSqdCgQQN4eHiUSKeJiIiIiMzJmC/kqXB6FSFHjhzRef7gwQO0adMGw4YNw1dffYXKlStr1927dw9hYWE4fvw4/vjjD5N2loiIiIiIrJ9R54RMmTIFfn5+WLNmjU4BAgCVK1fG2rVr4evriw8//NAknbSEjevXIqRbMJo1qo+hoQNwNuqMVWYwhznmynjRnAldamDb5Da49HV3nP68C355vQmqly+js033Br5Y+WZzRH3RFbd/7IW6FdzytVPZyxmLxzTBmS+64OLX3TB/ZCN4uziYfTylNceWxsIccefY0liYQ6WRUUXIgQMH0Llz5yK3CQ4OxoEDB4zqlKXt3bMbc74Ox9hxE7Bx8zY0btwEE8ePRUx0tFVlMIc55sowRU6LGp5YfewuBvx4HK8tOgmZVIJVbzaHk4NMu42zgwxnbidgzh/XCmzDyUGGVROaQxCAVxecROhPJ2BvJ8WvY5sZfFUTa3ndxJRjS2NhjrhzbGkszKHSyqgiJDMzEzExMUVuEx0djYyMDKM6ZWmrVy5H/4EDMWBQKKrXqIGpYTPg6+eL3zaut6oM5jDHXBmmyBn1y2n8fuoB/n2UiqvRTzF13UVU8HRG/Yru2m22nnmIeftu4tiNuALbaFrNAxU9nfHhugu4HvMU12Oe4sN1FxBUpSxa1/Iy63hKY44tjYU54s6xpbEwx3pIJeJ9WCOjipAmTZpgw4YNOHHiRIHrIyIisHHjRjRr1uyFOleYlJSUEmkXALKzsnD1ymW0at1WZ3mr1m1w4fw5q8lgDnPMlVFSOa5OmlPWktKz9N7HwU4KQRCQpVRrlymUaqjUAppW99S7HWt+3SyVY0tjYY64c2xpLMyh0syoIuTLL7+ESqVCu3bt0L9/f3z//fdYvXo1vv/+e/Tr1w/t27eHIAj44osvDG77u+++K3J9SkoKunXrZky39ZKYlAiVSgUvL91vTb28vBEXF2s1GcxhjrkySipnZr8AnP4vATcepeq9z7k7SUjPUmFa35fgaC+Fk4MMH/WtC5lUgvJucr3bsebXzVI5tjQW5og7x5bGwhwqzYy6Y3rbtm2xe/dujBs3Dtu3b8f27dshkUggCAIAoFq1aliyZAnatGljcNsff/wxvLy8MHr06HzrUlNT0b1792KPhCgUCigUCp1lgkwOuVz/P0KevwybIAgmvzSbOTKYwxxzZZgy57OB9fCSvytCfyr4aGthEtKyMGnFWXweGohR7apCLQjYeTYal+4nQ6Uufv/nWdvrJoYcWxoLc8SdY0tjYY51sPb+i41RRQgAdO7cGTdv3sSxY8dw4cIFJCcnw93dHUFBQWjbtq3Rb9Tq1asxYsQIeHh4oF+/ftrlqamp6NatGxISEvDXX38V2UZ4eDhmz56ts2zGx59g5qxPi833KOsBmUyGuDjdOecJCfHw8vLWexyWzmAOc8yVYeqcTwfUQ+dAHwyZdwKPkjMN7svf1+PQ8Ysj8ChjD6VawNMMJU591hkP4tP1bsMaXzdL59jSWJgj7hxbGgtzqDQzajpWLolEgnbt2mHSpEmYMWMGJk2ahHbt2r1QpTho0CDMmzcPw4YNw+HDhwFoCpAePXogLi4OR44cgY+PT5FthIWFITk5Wefx4bQwvfLtHRxQN6AeIiOO6yyPjIhAUMNGxg3KAhnMYY65MkyZM3tgPXRv4ItXF0TiQcKLXdgiMS0bTzOUaFXLC14uchy4/Fjvfa3tdRNDji2NhTnizrGlsTCHSjOjj4TkUiqVuHHjBpKTk+Hm5oY6derAzu7Fmn3jjTeQkJCAfv36Yfv27fj444/x6NEjHD16FH5+fsXuL5fnn3qVqdQ/f8TI0ZgxfSoCAgMRFNQIv2/aiJiYGIQOGWroUCyawRzmmCvDFDmfDQrEy038Me7XM0hVqODtqvlv+GlmNhTZmrlU7s728Pdwgk/O+R259xGJTVEg7qlmCuag5hVx83EqElKz0LiqB2YNCMCyo7dx60maWcdTGnNsaSzMEXeOLY2FOdaDs7FMy+hqITExEdOmTcO6det0LsXr5OSEYcOGITw8PN9JSYaYOnUqEhMT0blzZ1StWhVHjx5FhQoVjG7PED1CeiI5KRFLFi1EbOwT1KxVGwsWL4G/v+nyzZHBHOaYK8MUOSPaVgEAbHi7lc7yKesu4PdTDwAAXQJ98N2wIO26+SMbAwB+3HsDP+39F4CmMJnauw7cnR3wMCEdC/bfxNIjt80+ntKYY0tjYY64c2xpLMyh0koi5J5NboDExES0atUKN27cgJeXF5o2bQpfX188fvwYZ86cQVxcHGrVqoUTJ07A01P/y2ICwIABA3Se7969G0FBQfkKkC1bthjUriFHQojI/OpO2WWWnKvf9TJLDhERGc7xhefolJxev5yydBcKtWt8c0t3wWBGvdWff/45bty4gbCwMMyYMQPOzs7adRkZGfjqq6/w5Zdf4osvvsDcuXMNatvd3V3n+SuvvGJMF4mIiIiITEYCzscyJaOOhFSvXh3VqlXDwYMHC92mS5cuuHXrFm7duvVCHTQVHgkhEjceCSEiIjEfCen9y2lLd6FQf4wvmRuElySjro4VHR2Nli1bFrlNixYtEB0dbVSniIiIiIjIdhlVb7q7u+Pu3btFbnP37t18U6uIiIiIiKyRlLOxTMqoIyEdO3bEpk2bcODAgQLXHzx4EJs2bULHjh1fpG9ERERERGSDjDoS8sknn2DXrl3o3r07evbsiQ4dOsDHxwePHz/GkSNHsGfPHjg7O2PWrFmm7i8REREREVk5o4qQgIAA/Pnnnxg1ahR27dqFXbt2QSKRIPcc9xo1amDFihWoV6+eSTtLRERERGQJEt6t0KSMvgZB69atcf36dRw/fhznzp1DSkoK3Nzc0KhRI7Rp04ZvFBERERERFeiFLoQmkUjQtm1btG3b1lT9ISIiIiIiG/fCV2NWKpW4ceMGkpOT4ebmhjp16sDOTsQXeSYiIiIiMhAn+ZiWUVfHAoDExESMGzcOZcuWRf369dG2bVs0aNAAZcuWxbhx4xAfH2/KfhIRERERkY0w6o7piYmJaNWqFW7cuAEvLy80bdoUvr6+ePz4Mc6cOYO4uDjUqlULJ06cgKenZ0n022C8YzoRAYBHyDclnpG4Z1qJZxAR2SIx3zG9369nLN2FQm17o6mlu2Awo97qzz//HDdu3EBYWBhmzJgBZ2dn7bqMjAx89dVX+PLLL/HFF19g7ty5JussEREREZElSDkfy6SMmo61bds2dOrUCV9++aVOAQIATk5O+PzzzxEcHIxt27aZoo9ERERERGRDjCpCoqOj0bJlyyK3adGiBaKjo43qFBERERER2S6jpmO5u7vj7t27RW5z9+5duLu7G9UpIiIiIiIx4Wws0zLqSEjHjh2xadMmHDhwoMD1Bw8exKZNm9CxY8cX6RsREREREdkgo46EfPLJJ9i1axe6d++Onj17okOHDvDx8cHjx49x5MgR7NmzB87Ozpg1a5ap+0tERERERFbOqCIkICAAf/75J0aNGoVdu3Zh165dkEgkyL3ab40aNbBixQrUq1fPpJ0lIiIiIrIECedjmZTRV2Nu3bo1rl+/juPHj+PcuXNISUmBm5sbGjVqhDZt2vCNIiIiIiKiAr3QLWEkEgnatm2Ltm3bmqo/RERERERk44w6Mb002Lh+LUK6BaNZo/oYGjoAZ6NMf5dMc2QwhznmyrCWnClDW+LY/NfwZPt7uPvbJPz2aX/UquiZb7s6lb2w6bMBeLTtPTzZ/h6O/jwClcq5atf7eJTB0mm9cHvjW4jb8T4iFo5E/3Z1zD4eseXY0liYI+4cWxoLc6yDRCLehzUyugiJjY3FnDlzEBoaiq5duyI4ODjfo3Pnzqbsq9ns3bMbc74Ox9hxE7Bx8zY0btwEE8ePRYwJ73tijgzmMMdcGdaU065BJSzecRYd3lmD3tM3QiaT4o+vB8PZ0V67TTW/sjj4w6u4cS8B3T9Yh+bjlyN8zXFkZqu02yyd1hu1K3oidNYWNB23DNuP3cDqGX0RVKO8WccjphxbGgtzxJ1jS2NhDpVWEiH3bHIDXLx4EcHBwUhMTERRu0skEqhUqkLXm1OmUv9tXx0airoBAZg5a7Z2Wb8+IegU3AXvvv+BSfpjjgzmMMdcGdaU4xHyjc5zb3cn3N/8DrpMXovjlx4AAFZ91BfZKhVe/2ZXoe3E7ngf7/z8J9YfuKxd9uD3dzDjf0fw47shZhuPmHJsaSzMEXeOLY2FObocX+hEgZIVuuKspbtQqE2jGlu6CwYz6kjIBx98gISEBMyYMQO3b99GdnY21Gp1vodYChBDZGdl4eqVy2jVWvc8l1at2+DC+XNWk8Ec5pgrw9pz3MrIAQCJTzMBaA5r92hRHf8+SMSO8MG4+9sk/PXzCPRpXUtnv4h/HmBQh5fg4eoIiQQI7VgXcnsZ/rpwz6LjsVSOLY2FOeLOsaWxMMe6SCUS0T6skVFFyIkTJ9CvXz989tlnqFKlCmQyman7Vaj79+9jzJgxJdZ+YlIiVCoVvLy8dJZ7eXkjLi7WajKYwxxzZVh7zjdvBuP4pfu4cicOAFC+bBm4OssxZUgL7D99C33CfsOO4zew4ZP+aNugkna/EV9sh51Miugt7yJ59xTMe687hny6Fbdjkiw6Hkvl2NJYmCPuHFsaC3OoNDPqoJeDgwNq1Khh6r7oJSEhAStXrsSyZcsK3UahUEChUOgsE2RyyOVyvXOev8SwIAgmv+ywOTKYwxxzZVhjzg9vd0X9auXR+f212mVSqaadP07cxLwtmhMpL/73BC3qVcDY3g1x7OJ9AMCno9vDw8URIVM3ID45HX1a18baj19GlzxtmXs8YsixpbEwR9w5tjQW5lBpZNSRkODgYJw5I96rHISHh8Pd3V3n8e034Xrt61HWAzKZDHFxcTrLExLi4eXlbZL+mSODOcwxV4a15sx9qwt6t6yJ7h+ux8O4p9rlccnpyFaqcPWubsb1e/GoVN4NgObE9Qn9mmD893tw5NxdXLoVi6/WHMfZG48w/mX95+Va4+tmyQzmMMdcGcwRf44lSET8sEZGFSHffvstLl++jO+++87U/TGJsLAwJCcn6zw+nBam1772Dg6oG1APkRHHdZZHRkQgqGEjk/TPHBnMYY65Mqwx54dJXfBy29roMXUD7j5K1lmXrVQj6voj1K6ke9neWhU8ce9xCgDAWa45iKx+7sIcKrVg0Nxca3vdLJ3BHOaYK4M54s8h66fXdKyCzsGoV68epk2bhsWLFyMoKAju7u75tpFIJFi6dOmL99JAcnn+qVeGXB1rxMjRmDF9KgICAxEU1Ai/b9qImJgYhA4ZarI+miODOcwxV4Y15fz4dlcMCQ5A6CdbkJqeBR+PMgCA5DQFMrM0vyh+2HQSq2e8jGMXH+Dohbvo1qw6eraqie4frAMAXL+fgJsPEzD/3e4IW3IY8SkZ6NumFjo3rooBH2/Gq90bmm08YsqxpbEwR9w5tjQW5lBppVcRsmLFikLX3bp1C7du3SpwnTFFyIABA4pcn5SUZFB7xugR0hPJSYlYsmghYmOfoGat2liweAn8/StYVQZzmGOuDGvKGd9XM11q//fDdJaP/XYX1vz5DwBgx/F/8fZP+/DhKy3x/VudceNBAl6ZvRURlx8CAJQqNfrN2IwvXu+AzZ8PhIujPf6LTsIb3+7CvlMF/z4sqfGIKceWxsIccefY0liYYz14Totp6XWfkLt37xodUKVKFYO2Hz16tF7bLV++3KB2DTkSQkS26/n7hJSExD3TSjyDiMgWifk+Ia+sOm/pLhRq/WsNLd0Fg+n1VhtaSLwIQ4sLIiIiIiKyLiKuN4mIiIiIxEHK2VgmpVcR8tdffwEAmjdvDkdHR+1zfbRv3964nhERERERkU3Sqwjp2LEjJBIJrl69itq1a2uf60OlUr1QB4mIiIiIyLboVYTMmjULEokE3t7eOs+JiIiIiEoD/u1rWnoVIZ9++mmRz4mIiIiIiPRl1B3TiYiIiIiIjMWrYxERERERFYOzsUxLryIkODjYqMYlEgkOHjxo1L5ERERERGSb9CpCjhw5YlTjPIGHiIiIiIiep1cRolarS7ofRERERESixS/XTYvnhBBRqZK4Z1qJZ1Sb+HuJZwDA7YUDzZJDRERkaia5OlZCQgLu379viqaIiIiIiMjGGV2EJCcn491334WPjw/KlSuHatWqadedPHkSPXv2RFRUlEk6SURERERkSVKJeB/WyKgiJCEhAS1atMC8efNQqVIl1K1bF4IgaNc3aNAAx48fx9q1a03WUSIiIiIisg1GFSGffvopbty4gfXr1+PMmTMIDQ3VWe/k5IQOHTrg0KFDJukkERERERHZDqNOTN+xYwd69+6NIUOGFLpNlSpVEBERYXTHiIiIiIjEglfHMi2jjoTExMQgICCgyG0cHR2RlpZmVKeIiIiIiMh2GVWEeHl5FXs1rGvXrsHPz8+oThERERERke0yqghp3749duzYgYcPHxa4/sqVK9i7dy+6dOnyQp0jIiIiIhIDiYgf1sioImTGjBlQKpVo06YN1q1bh7i4OADA1atXsXTpUgQHB0Mul+PDDz80aWeJiIiIiMj6GVWE1K9fHxs3bkRSUhJGjBiBhQsXQhAEBAYGYuzYscjIyMBvv/2GWrVqmbq/ZrNx/VqEdAtGs0b1MTR0AM5GnbHKDOYwx1wZzNH1do862PNRJ/z7c19c+q4Xlk9shRo+LjrbfNCnLv7+rBv+m/cyrv7QBxvfb4dG1Tx0thnerhp+/6A9bvzUFzFLBsLNyd4i4xFTBnMMF3XmNN6e+Ca6dGyLoHp1cOjgAZO2/zx+1phjrhyyXkbfrLBv3764desWvvvuO4SGhqJLly7o378/vvnmG/z333/o2bOnKftpVnv37Macr8MxdtwEbNy8DY0bN8HE8WMREx1tVRnMYY65MpiTX6va3lh++BZ6hR/GkB+PQSaVYMN7beHkINNuc+txKj5afx6dZh/Ay3OO4H5cGja81w5eLg7abZwcZDh8+RF+3nPNouMRSwZzjJORkY46depg+oxZJmuzMPysMcdcOeYmlUhE+7BGEiHvXQb1dOvWLVSvXr3Y7bZu3Yr+/fsb1TFTy1Tqv+2rQ0NRNyAAM2fN1i7r1ycEnYK74N33PzBJf8yRwRzmmCuDObqqTfw93zIvFwf8M7cP+n97FJH/xhW4n4ujHf79+WWEzv0Lx67F6qxrVdsbW6Z0QJ13dyAlIxsAcHvhQLOMR0wZzHlxQfXq4IefFyC4c8mct8nPGnNeJMfRqJtHmMcbG/+xdBcK9euQQEt3wWBGHQnp1q0bnjx5UuQ2W7duxdChQw1ue8yYMXo9Skp2VhauXrmMVq3b6ixv1boNLpw/ZzUZzGGOuTKYox/XnGlUiWlZBa63l0kwvF01JKdn4cqDZKMyCsPPGnPMhZ815tjaZ5pKjlH15pMnT9CjRw8cPXoUrq6u+dZv374dQ4cORZUqVQxue8WKFahSpQoaNWoEIw7SvLDEpESoVCp4eXnpLPfy8kZcXGwhe4kvgznMMVcGc/Tz6eAGOPlvHK5Hp+gs71LfF4vHtoCTgwyPkzMx5IdjSEgtuFAxFj9rzDEXftaYY2uf6bysdNaTaBl1JGTbtm24evUqXn75ZWRl6f6f5Y4dOzB48GBUqlQJR44cMbjtN998E8nJybh16xY6deqEpUuXYuvWrfkeRVEoFEhJSdF5KBQKg/rx/F0xBUEw+Z0yzZHBHOaYK4M5hfvqlYYIqOCOCf87lW/d8eux6PL5AfT55ggOX36EJeNbwMtVbnSfi8LPGnPMhZ815tjaZ9qW/PXXX+jTpw/8/f0hkUiwbds2nfWjRo2CRCLRebRs2VJnG4VCgbfffhve3t4oU6YM+vbtiwcPHhjUD6OKkODgYKxevRp//fUXXnnlFe0Ri507dyI0NBSVKlXC0aNH4e/vb3DbCxcuRExMDKZNm4adO3eiUqVKGDx4MPbt26f3kZHw8HC4u7vrPL79JlyvfT3KekAmk2kvO5wrISEeXl7eBo/HUhnMYY65MphTtC+GBqFbkB8Gfv8XYpIy8q3PyFLhTmwazt5OwAerzkKpEjCsTdUX6X4+/Kwxx1z4WWOOrX2mbVFaWhqCgoIwf/78Qrfp0aMHYmJitI/du3frrH/vvfewdetWbNiwAceOHUNqaip69+4NlUqldz+MvjrWoEGDMG/ePGzduhVvvvkmdu7ciUGDBqFixYo4fPgwKlSoYGzTkMvleOWVV7B//35cuXIF9erVw8SJE1GlShWkpqYWu39YWBiSk5N1Hh9OC9Mr297BAXUD6iEy4rjO8siICAQ1bGTUeCyRwRzmmCuDOYX78pWG6NmoAkLn/o378el67SORAA72Rv9qLhA/a8wxF37WmGNrn+m8nj86IKaHIbOAQkJC8MUXX2DAgAGFjlUul8PX11f78PT01K5LTk7G0qVL8f3336NLly5o1KgR1qxZg0uXLuHAAf0v//1C1yCYMGECnjx5gtmzZ2Pp0qWoXLkyDh8+jEqVKr1IszpyX1xBEKBWq/XaRy6XQy7Xnc5gyNWxRowcjRnTpyIgMBBBQY3w+6aNiImJQegQw0+0t2QGc5hjrgzm5Bc+rCH6N6+E0QtPIDUzG+XcNL+TnmZkIzNbDScHGd7r+RL2XYjBk+RMeLg4YGSH6vDzcMLOM88OaZdzk6O8myOqldfcY6RuBTekZirxMEG/osZU4xFLBnOMk56Whnv37mmfP3zwANeuXoW7uzv8jJi1UBR+1phjrhx6Jjw8HLNnz9ZZ9sknn+DTTz81qr0jR46gfPnyKFu2LDp06IAvv/wS5cuXBwBERUUhOzsb3bp1027v7++PwMBAREREoHv37npl6FWE5P3F9bzRo0fj3LlzOHbsGFauXJlv+8qVK+vVkbwUCgW2bNmCZcuW4dixY+jduzfmz5+PHj16QCo17TeEBekR0hPJSYlYsmghYmOfoGat2liweAn8/Y0/umOJDOYwx1wZzMlvVMcaAIAtUzroLH93+Rn8duIu1GoBNX1dEdqqCjxdHJCYloXzdxLRb85R3Ih5qt3+tQ7VMaVPgPb5tqkdte2YczxiyWCOcS5f/gdvjH5N+/y7OZopyn1f7o/Pv/raZDkAP2vMMV8OPRMWFobJkyfrLHv+C3l9hYSEIDQ0FFWqVMHt27fx8ccfIzg4GFFRUZDL5Xj06BEcHBzg4aF7c10fHx88evRI7xy97hMilUqLPZmooBOOJBIJlEoDDkEAmDhxIjZs2IDKlStj9OjRGD58eL4rLBjDkCMhREQvoqD7hJQEQ+4TQkRkDcR8n5Dxmy9buguF+mVQPaP2k0gk2Lp1K/r161foNjExMahSpQo2bNiAAQMGYN26dRg9enS+6V5du3ZFjRo1sHjxYr2y9XqrX3vtNbNd0WDx4sWoXLkyqlWrhqNHj+Lo0aMFbrdlyxaz9IeIiIiIqLTy8/NDlSpV8O+//wIAfH19kZWVhcTERJ2jIU+ePEHr1q31blevImTFihWG9fYFmLPgISIiIiKiwsXHx+P+/fvw8/MDADRp0gT29vbYv38/Bg8eDEBztOSff/7BnDlz9G5XdAe9zFnwEBERERHpQ2ojX5Knpqbi5s2b2ue3b9/G+fPn4enpCU9PT3z66acYOHAg/Pz8cOfOHXz00Ufw9vZG//79AQDu7u54/fXX8cEHH8DLywuenp6YMmUK6tevjy5duujdD9EVIUREREREVDLOnDmDTp06aZ/nntA+cuRILFq0CJcuXcKqVauQlJQEPz8/dOrUCRs3boSrq6t2nx9++AF2dnYYPHgwMjIy0LlzZ6xYsQIymUzvfuh1YnpwcDAkEglWrlyJihUrIjg4WL/GJRIcPHhQ786UJJ6YTkTmwhPTiYiMI+YT0yf8fsXSXSjUooEBxW8kMnq91UeOHIFEIkF6err2uT54bgcRERER2QL+WWtaehUhz98kUN+bBhIRERERET2vxO78l5mZiZSUlJJqnoiIiIiIrFSJFSETJkyAp6dnSTVPRERERGQ2EolEtA9rVGJFCKC5izoREREREVFeJVqEEBERERERPU/EF0IjIrJO5rp0buNZf5ol5+xn3cySQ0QkZvzm3rT4ehIRERERkVmxCCEiIiIiIrPSezrWkydPDGo4IyPD4M4QEREREYmRtV6FSqz0LkJ8fX0NevEFQeCbRURERERE+ehdhLRv355FBRERERERvTC9i5AjR46UYDeIiIiIiMRLyu/iTYonphMRERERkVmxCCEiIiIiIrPizQqJiIiIiIrB6VimxSMhhdi4fi1CugWjWaP6GBo6AGejzlhlBnOYY64M5lgmZ2yHatg4sQVOfxKMvz/qiHnDG6Kqt3O+7d7qXANHprfH2dmdseKNpqhZvozO+kqeTvj51SAcm9ERp2YFY+4rDeDl4mDWsTCHOWLLYI74c8h6sQgpwN49uzHn63CMHTcBGzdvQ+PGTTBx/FjEREdbVQZzmGOuDOZYLqdpNQ+sj7yPVxadxBvLzkAmleDX0U3gZC/TbvN6+6oY2aYKvth5DYMXnkRcqgK/jmkCZwfNNk72MvxvdBMIAEb/egav/nIK9jIpFoxoBEMuimgtrxlzrD/HlsbCHCqtJIIgCJbuhDlkKvXf9tWhoagbEICZs2Zrl/XrE4JOwV3w7vsfmKQ/5shgDnPMlcEcy+Q0nvVnvmUeZexxfEYnjFhyGlF3EgEAR6d3wKqIu1j61x0AgL1Mgr8/6oi5+/7Fb6ceoHVNL/wyqjFafn4IaQoVAMDN0Q6Rs4Lx+tIzWDCyaYmPxRDMYY4tjYU5uhxFfKLABzuvW7oLhfq+Tx1Ld8FgPBLynOysLFy9chmtWrfVWd6qdRtcOH/OajKYwxxzZTBHXDmucs3/gydnZAMAKno4oZybHBH/xj/LVQk4czsRDSuXBQA42EkhCAKylGrtNgqlGiq1gMZVPSw2FuYwx1IZzBF/Dlk/FiHPSUxKhEqlgpeXl85yLy9vxMXFWk0Gc5hjrgzmiCtnaq86iLqTiJuPUwEA3q6a8zriUrN0totLzYJ3zjkfF+4nISNbhQ961IajvRRO9jJMCakNmVSCcq76nRdiza8Zc6wrx5bGwhwqzYw66PXZZ58Vu41UKoWbmxvq1KmDDh06wNHRUa+2pVJpsXdml0gkUCoLn1+lUCigUCh0lgkyOeRyuV59yM3Q2V8QTH7HeHNkMIc55spgjuVzZvZ9CXV8XTH8l1P51gnQnXkrkUC7JDEtG++vu4hZL9fF8FaVoRYE7L74CJcfpkClztdUkaztNWOO9ebY0liYYx14dSzTMqoI+fTTT3U+SHlPK3l+uUQiQdmyZfHDDz/gtddeK7btrVu3FrouIiIC8+bNQ3GnsYSHh2P27Nk6y2Z8/Almzvq02HyPsh6QyWSIi4vTWZ6QEA8vL+9i99eHOTKYwxxzZTBHHDkz+ryETi+Vx2v/O43HKc++hIl7qjkCUs5Frv0ZALzKOCA+z9GRiJvx6PH9MZR1todKLeBpphJ/hXXAw8QMs4+FOcyxdAZzxJ9D1s+o6ViHDx9G7969IZfLMW7cOKxcuRJ79+7FypUrMXbsWMjlcvTp0webN29GWFgYlEolxowZgwMHDhTb9ssvv5zvUadOHaxYsQLff/89QkNDcf160ScGhYWFITk5Wefx4bQwvcZm7+CAugH1EBlxXGd5ZEQEgho20qsNMWQwhznmymCO5XNm9HkJXQLKY8zSM/mKhgeJGYhNUaBVzWdTI+xlEjSt5oHz95LytZWUno2nmUq0qO4JzzIOOHT1iVnHwhzmiCGDOeLPIetn1JGQf//9F0ePHsX58+dRu3ZtnXUjRozAe++9hxYtWqBv37744osvMGzYMDRu3Bjff/89unTpondOdHQ0PvnkE6xcuRLdu3fH+fPnERgYWOx+cnn+qVeGXB1rxMjRmDF9KgICAxEU1Ai/b9qImJgYhA4Zqn8jIshgDnPMlcEcy+V83LcuegX5YtKa80hTKLXneTzNVEKRc6L5qoi7GNexGu7Gp+NufDrGdayGzGwV/jgfo22nf2N//BebhsS0LDSsXBZhvetg1fG7uBOXbraxMIc5YspgjvhzzM3KZ5OJjlFFyE8//YQhQ4bkK0ByvfTSSxgyZAh++OEHjBkzBgEBAejTpw8OHTqkV/vJycn46quvMG/ePDRs2BAHDx5Eu3btjOmqUXqE9ERyUiKWLFqI2NgnqFmrNhYsXgJ//wpWlcEc5pgrgzmWy3mlZSUAwKqxzXSWf7T5H2w7q7km/9K/7sDRXoZZfevCzckOFx8k443lZ5GepdJuX7VcGbzfvRbc/9/efYdFcTRgAH+PA44mRVAQu2Iv2BWNAooKxq6oiVE0Rk0s0Rhjjy1G1BhbbDGJJdEYjS3G2I0YFEnsFTuIBSIdpRxtvz/4vHgCcuCyt3e+P599Hm92b96ZuxOZm9ldSzM8SkzDt8fDsenUfUn7whzmyCmDOfLPIcNWrPuEWFpaYvz48QgMDCzwmKlTp2L58uVITc39Fm3KlClYunRpnhPGX7Zo0SIsXLgQLi4umD9/Pnr06FHU5uWrKDMhRESGIL/7hJSE83M7SZJDRCTn+4RM+kO+9wlZ9Lbh3SekWG91mTJlcOjQoQIHIYIg4NChQ1qXZ0tISICdnV2hdU+ZMgWWlpZwc3PDpk2bsGnTpnyP27VrV3GaTkRERERUZCZcjyWqYp2YPmDAAFy8eBG9e/fGtWvXtPZdvXoVvXv3xqVLl/DOO+9oyv/55x/UqVOn0LoHDx6Mfv36oXTp0rCzsytwIyIiIiIiw1SsmZA5c+bg7Nmz2LNnD3777TdYW1ujTJkyiImJQUpKCgRBQLt27TSXyY2OjkaVKlXQr1+/QuveuHFjcZpEREREREQGoliDEEtLSxw9ehQbNmzA5s2bcfnyZURGRsLW1haenp4YOHAghg4dChOT3IkWFxeXV97/g4iIiIhIzoq1fIgKVOzTf0xMTDBs2DAMGzZMzPYQEREREZGR46COiIiIiIgk9VoXQrt//z62bNmCixcvIikpCba2tmjUqBEGDhyIKlWqiNREIiIiIiL94sWxxFXsQcjKlSsxceJEZGZm4sVbjezcuRNz587FokWLMG7cOFEaSURERERExqNYy7H279+Pjz/+GPb29pg/fz5Onz6N8PBwhIaGYsGCBXBwcMCECRPwxx9/iN1eIiIiIiIycMWaCVm8eDEcHR1x/vx5uLq6asorV66MFi1aYODAgWjUqBGWLFmCt99+W7TGEhERERHpA29WKK5izYScP38e/fr10xqAvKh8+fLo168fzp0791qNIyIiIiIi41OsmZCMjAxYW1u/8hhra2tkZGQUq1FERFS483M7SZJTZ6I0S2vDFnPmnIjoTVGsmZCaNWvi999/R1ZWVr77s7KysG/fPtSsWfO1GkdEREREJAcKhXw3Q1SsQUhAQABu3ryJzp0751lydfbsWfj5+eHmzZsICAgQpZFERERERGQ8irUc6+OPP8bJkyexe/dutGjRApaWlnB2dsa///6LtLQ0CIKAHj168BK9RERERESUR7FmQpRKJXbu3IlNmzbBy8sLKpUKkZGRUKlU8Pb2xqZNm7B7926YmPCG7ERERERk+EwU8t0M0WvdMX3QoEEYNGhQvvsePnyI+Ph4NGzY8HUiiIiIiIjIyJTYVMXnn3+Oxo0bl1T1RERERERkoF5rJoSIiIiI6E3AmxWKiydtEBERERGRpDgIISIiIiIiSXEQUoBtW7fAr1N7NG/cAAP8e+P8ubMGmcEc5kiVwRzjzvnIpzr2TGiDKws648wXPvh2WFNUK2utdUznhi7Y9GELnJvXEeHL3kad8rZ56qnkaIW17zfF2Xk+uLygE1YGNIaTjbmkfWGOceQYU1+YYxj0fUNC3qzwDXDwwH4sWhCI4SM+wrYde9CkSVOMGjkcUY8fG1QGc5gjVQZzjD+nZfXS+OnkffRedgqD1/wNpYkCP37YApbmSs0xVuZKnA2Px6J9N/Ktw9JciR8/agFBAAau+hv+y0/DzNQE3w9vXqT/RA3lNWMOf64xp2RzyLApBEEQdDnwn3/+KVLFc+fOxYEDB5CdnV2shgFAbGwsFAoFHB0di13Hc+lZuh87cIA/6tStixkz52jKenbzg3d7H4z75NPXbotUGcxhjlQZzDHunDoT/8hTVtraHOe+7Ij+K07jn3vxWvvKl7bEyZnt0eWrYIQ9StaUt63lhA0jW6DR1MN4ps79oWxraYpLgZ3x3upQfD+iVYn3pSiYI98cY+oLc7RZyPiSSV8cvaPvJhTocx83fTehyHSeCWnVqhU8PDx03vbv31+sBiUmJmL06NFwcnKCs7MzypYtCycnJ4wZMwaJiYnFqrMoMjMyEHb9Gjxav6VV7tG6DS5dvGAwGcxhjlQZzHkzc0pZ5v6mkJiaofNzzE1NIAgCMrJyNGXqrBxk5whoVq20TnUY8mvGHP5cY454Ofqg7xsSvrE3Kxw8eDAUJbzoLD4+Hh4eHnj06BEGDhyIOnXqQBAEhIWFYePGjTh27BhCQkLg4OBQYm1ISExAdnZ2ntkXR0cnxMbGGEwGc5gjVQZz3sycGT3r4szdeNyKfqbzcy5EJCI1IxuTu9fGV/tuQKFQYEq32lCaKFDWVqVTHYb8mjGHP9eYI14OGT6dByEbN24swWbkmjt3LszNzXH37l04Ozvn2depUyfMnTsXS5cufWU9arUaarVaq0xQqqBS6fafHIA8Ay5BEEQfhEmRwRzmSJXBnDcnZ26feqjtWgr+y08X6XnxKRkYs/E8vvCvjyFtqyBHEPD7+ce48iAJ2TmFP/9FhvaaMYc/15hTMjlkuGR1YvqePXuwePHiPAMQAHBxccGiRYuwe/fuQusJDAyEnZ2d1vbVwkCd2uBg7wClUonY2Fit8vj4ODg6OunWERlkMIc5UmUw583Kmd27HjrUd8Y7K0MRnZRe5LYE34yF17wgNPv8CJrMOIIJWy7BxU6Fh3GpOj3fEF8z5vDnGnPEz9EHhYz/GCJZDUKioqJQr169AvfXr18f0dHRhdYzdepUJCUlaW2fTZ6qUxvMzM1Rp249hIac0ioPDQmBe6PGOtUhhwzmMEeqDOa8OTlz+tRD54YuGLgqFA/j016rTQkpmXialgWPGo5wtFHh6LV/dXqeob1mzBE/x5j6whx6k8nqGgROTk6IiIhAhQoV8t0fHh6u05WyVKq8S6+KcnWsQQFDMX3KJNStXx/u7o2x89dtiIqKgn//AbpXIoMM5jBHqgzmGH/O3L710aOpK0Z8fxbP1NlwKpX7M/ZpeibUmblrqeyszODqYAnn/5/f8fw+IjHJasQ+zV0i27dFBdz59xnin2WgSRUHzOxdF+tPhOPekxTJ+sIcw88xpr4wh95UshqE+Pr6Yvr06Thy5AjMzbVvXqVWq/H555/D19e35Nvh1wVJiQlYt2Y1YmKewK1GTaxauw6uruUNKoM5zJEqgznGnzPorcoAgF/GemiVT/z5Enb+8xAA4FPfGYvfddfsWxnQBACw7OAtLD94G0DuwGRS11qwszLHo/hUrDpyBz8EhUvaF+YYfo4x9YU5hsNQr0IlVzrfJ0QKDx8+RLNmzaBSqTB69GjUrl0bAHD9+nWsXr0aarUaZ8+eRcWKFYtcd1FmQoiI6D/53SekJIQtfluSHCKSLznfJ2TBn3f13YQCTWlfXd9NKDJZvdUVKlTA6dOnMWrUKEydOhXPx0cKhQIdO3bEypUrizUAISIiIiIi+ZDVIAQAqlatigMHDiAhIQG3b+dO37u5uaF0ad1uZEVEREREJDYuxxLXaw9Crl+/jhs3biAlJQWDBg0So00AAAcHB7Ro0UK0+oiIiIiISB6KfYneM2fOoFGjRmjQoAH8/f0xZMgQzb6//voLVlZW2Lt3rxhtJCIiIiIiI1KsQci1a9fQvn17hIeH45NPPoGfn5/W/rZt28LJyQm//vqrKI0kIiIiItInhUIh280QFWsQMmvWLADAuXPnsHjxYjRv3lxrv0KhgIeHB86cOfP6LSQiIiIiIqNSrEHIiRMn0KdPH7i5uRV4TKVKlRAVFVXshhERERERkXEq1onpT58+RdmyZV95THp6OrKzs4vVKCIiIiIiOeHVscRVrJmQihUr4urVq6885ty5c6he3fBunEJERERERCWrWIOQrl274vDhw/jzzz/z3b99+3aEhoaiZ8+er9M2IiIiIiIyQsVajjVt2jTs2LEDfn5+CAgI0Jz7sXr1apw+fRpbt25FlSpVMGHCBFEbS0RERESkDwZ6ESrZUgiCIBTniffu3cOgQYNw+vTpPPtatmypGYjIRXqWvltARESv0mreMUlyQmd0kCSHiIrO4rVvo11ylvx1T99NKNCEdtX03YQiK/ZbXa1aNZw6dQoXL15EaGgo4uPjYWtri5YtW+a5ZC8REREREdFzrz3ebNSoERo1aiRCU4iIiIiI5MmE67FEVawT04cPH47g4GCx20JERERERG+AYg1CfvjhB3h5eaFKlSqYMWMGbty4IXa7iIiIiIjISBVrEHLy5EmMGDECKSkpmD9/PurVq4cWLVrgm2++QUxMjNhtJCIiIiLSKxOFfDdDVKxBSOvWrbFmzRpERUVh9+7d6NWrF65evYpx48ahfPny6Nq1K7Zt24b09HSx20tERERERAbutU5MNzU1RY8ePdCjRw8kJydj+/bt2Lx5Mw4cOIADBw6gVKlSSExMFKmpRERERERkDIo1E5IfW1tbfPDBB9i/fz/mzZsHU1NTPH36VKzqiYiIiIj0RqGQ72aIRLkljCAIOHLkCDZv3ow9e/YgJSUFAODt7S1G9UREREREZEReaxBy7tw5bN68Gb/88guePHkCQRBQt25dDBo0CAMHDkSFChXEaicRERERERmJYi3H+vLLL1G3bl20aNECy5cvh0KhwPjx43H+/HlcvXoVkydPNvgByLatW+DXqT2aN26AAf69cf7cWYPMYA5zpMpgDnPEyHj/rcrYMrw5Tk31xJ+ftcXSAQ1R2dEqz3EfelXF4U/fQuh0L3w/pAmql7HW7LO1NMVkv5rYM6YVTk/3woFP2mCSX03YqJSS94c58v2sMcc4cqRkAoVsN0NUrEHI559/jsjISLz77rs4cOAAHj58iK+//tpo7px+8MB+LFoQiOEjPsK2HXvQpElTjBo5HFGPHxtUBnOYI1UGc5gjVkbTKg7YduYhBn9/Fh/+eAFKEwXWDGoEC7P//rsa0qYy3vOohAX7b2Lgd2cQ+ywDawY3hpV57iCjTCkVypRSYcnhO/Bf/Tdm7rmONm6OmNWjruT9YY58P2vMMfwcMmwKQRCEoj7pp59+Qu/evWFtbV34wTKRnqX7sQMH+KNO3bqYMXOOpqxnNz94t/fBuE8+FaU9UmQwhzlSZTCHOWJktJp3LE+Zg5UZjk9qh/c3nMP5+4kAgCOfvoUtoQ+w8dR9AICZUoE/P2uLZUfuYue5R/nW3bFuWXzZux485gfh1LT2kvRHV8yRZwZz9JNjIcrZyiVj1akIfTehQKPbVNF3E4qsWDMhgwYNMqgBSFFkZmQg7Po1eLR+S6vco3UbXLp4wWAymMMcqTKYw5ySzLD5/28kSWmZAIDyDhYoU0qF03fj/svOFnA2IhGNKtq9sp5n6ixk5+j2vZsxvTfGlmNMfWGOYdH3FbCM7epYol2iVyypqakYPXo0ypcvj7Jly+Ldd99FbGxskepQq9VITk7W2tRqtU7PTUhMQHZ2NhwdHbXKHR2dEBsrzt3gpchgDnOkymAOc0oy49PONXD+fiLuPsm96qKTjQoAEJ+SoXVcfEoGHG3M863DztIUw9tVKXCWJD/G9N4YW44x9YU59CbTaRBiYmICU1NT3Lp1S/NYqVQWupmaFn1ObdasWdi4cSPefvttvPPOOzhy5Ag++uijItURGBgIOzs7re2rhYFFqkPx0rBSEIQ8Za9LigzmMEeqDOYwR+yMqV1qoaazDabsvJpn38sLiRUA8pvjsFYp8c3ARrgXk4Jvg8KL3AZjem+MLceY+sIcehPpNEpo164dFAoFrKystB6XhF27duGHH37AgAEDAAADBw5EmzZtkJ2dDaVStyubTJ06FRMmTNAqE5QqnZ7rYO8ApVKZZ/YlPj4Ojo5OOtUhhwzmMEeqDOYwpyQyJvvVhGctJ7y/4RyeJP83kx37LPfvjjbmiH3232yIg7U54p9pz45YmSux+r1GSM3IxoRtV5Cl41KskugPc+T7WWOOYebogwnHUKLSaSYkKCgIx48f11x29/ljXbaievDgAdq2bat53KJFC5iamuJxEa6ooFKpYGtrq7WpVLoNQszMzVGnbj2EhpzSKg8NCYF7o8Y6t0HfGcxhjlQZzGGO2BlTutREhzplMGLTeTxOTNfa9yghHTFP1fCoXlpTZqpUoFkVe1x8kKQps1YpsWZQY2RmCxi/9RIysnL01h/myPezxhzDzSHDJ7trEGRnZ8PcXHtdr6mpKbKyinB5q9c0KGAopk+ZhLr168PdvTF2/roNUVFR8O8/wKAymMMcqTKYwxyxMqa9XQt+DZwxfutlpGRka87zeJaeBfX/BxJbQh9gWNsquB+Xhsj4VHzQtgrSMnNw4Eo0gNwZkDWDGsPCzATTf7kGa5UprP//PVTCS+eSlHR/mCPfzxpzDD+HDFuxBiHVqlXD+PHj8fHHHxd4zNq1a7Fo0SLcu3evSHULgoAhQ4ZozVykp6fjww8/1Loi165du4recB35+nVBUmIC1q1ZjZiYJ3CrUROr1q6Dq2t5g8pgDnOkymAOc8TK6Nc8d8b9h6FNtcpn7rmOvRejAAAbT92HhZkJpr1dC7aWprjyMBkf/XQBqRnZAIC6rqXQsELulbL2jWutVU+XZdrfzpZ0f5gj388acww/R2omPKdFVMW6T4iJiQlmz56NmTNnFnjMwoULMW3aNGRnZxep7qFDh+p03IYNG4pUb1HuE0JERNLL7z4hJSF0RgdJcoio6OR8n5B1off13YQCjWhVWd9NKLISe6uTkpJ0Pg/jRUUdXBARERERkWHReRDy119/aT2OiIjIUwbkntPx8OFD/PTTT6hZs+brt5CIiIiISM+4GktcOg9CvLy8NJflVSgU2LRpEzZt2pTvsc+vBT1//nxxWklEREREREZD50HIzJkzoVAoIAgC5s6dC09PT3h5eeU5TqlUonTp0vD29kadOnXEbCsRERERERkBnQchs2fP1vz9xIkTGDp0KAYPHlwSbSIiIiIikhVeHUtcxToxvTg3ISQiIiIiIgJ0vGM6ERERERGRWIo9CHnw4AFGjhyJ6tWrw9LSEkqlMs9mairjiz0TEREREelIoZDvZoiKNUq4d+8eWrZsiYSEBNSrVw9qtRqVK1eGhYUF7t69i6ysLLi7u8Pe3l7k5hIRERERkaEr1kzInDlzkJSUhGPHjuHSpUsAcu90HhYWhoiICHTr1g0pKSn49ddfRW0sEREREREZvmLNhBw9ehRdunSBp6enpkwQBACAq6srtm/fjgYNGmD69On49ttvxWkpEREZtdAZHSTJcWg+RpKchDMrJckhImnwRGpxFev1jI2NRe3atTWPTU1NkZqaqnmsUqnQsWNH7Nu37/VbSERERERERqVYgxAnJyekpKRoPY6IiNA6xtTUFImJia/TNiIiIiIiMkLFWo5Vo0YN3L17V/O4RYsWOHToEO7du4dq1aohJiYGO3bsQPXq1UVrKBERERGRvigM9TJUMlWsmRA/Pz8cP35cM9Mxfvx4PH36FA0bNkTz5s1Rs2ZNREdHY+zYsWK2lYiIiIiIjECxBiEfffQRgoKCoFQqAQBeXl745ZdfULlyZVy9ehXOzs5YsWIFhg8fLmpjiYiIiIjI8BVrOZatrS1atmypVebv7w9/f39RGkVEREREJCdcjCUuXm2MiIiIiIgkVayZkMjIyEKPMTExga2tLWxtbYsTQURERERERqpYg5AqVarofIWAsmXLolevXpg1axacnZ2LE0dEREREpFcmvDqWqIq1HGvw4MFo27YtBEGAg4MDvLy80L9/f3h5ecHBwQGCIKBdu3Z4++23YWFhgbVr16JZs2aIiooSu/1ERERERGRgijUI+eyzz3Dp0iXMnj0bDx48wLFjx/Dzzz/j2LFjePDgAWbNmoVLly5hwYIFuHv3Lr744gs8evQI8+bNE7v9JWbb1i3w69QezRs3wAD/3jh/7qxBZjCHOVJlMEe+OefOnsHYUR/Cx+stuNerhT+PHRW1/hcZwms28f1OOLn5Mzw5uRj3jwVi+5LhqFG5rNYxZUuXwro57+He4S8RF7IEv60cheqVymgdY25miiWT/fHgzwWIDfkavy4bifJl7SXvz5uaY0x9YQ69iYo1CJk0aRJatWqFmTNnwtLSUmufpaUlZs2ahZYtW2Ly5MkwMTHB9OnT0bx5c+zfv7/IWbGxsUhOTi5OM4vt4IH9WLQgEMNHfIRtO/agSZOmGDVyOKIePzaoDOYwR6oM5sg7Jy0tFbVq1cKU6TNFqzM/hvKatW3ihrXb/oLn4MXo+tFKKJVK7FszBlYW5ppjti8dgaoVnOA//lu0emcBIqPisX/tWK1jvvqsD7p7N8TgqRvQYehS2FiaY+eKD2FiUrQlG4byuskpx5j6whzDoZDxZoiKNQg5deoUmjZt+spjmjRpguDgYM3jli1b6rwcKzExEaNHj4aTkxOcnZ3h4OAAFxcXTJ06FampqcVpcpH8tGkDevXpg959/VGtenVMmjodLuVcsH3bVoPKYA5zpMpgjrxz3mrriTHjPoFPx06i1ZkfQ3nNeoxZjc2//42we9G4cusRRs7ejErlSqNx3YoAALdKZdGyYVV8/OUvOHc9ErfvP8G4wG2wtlShn1/u/322NhYY0tMDU5bsxvG/b+LSzYd4f8aPqO/mivYta0vanzcxx5j6whx6UxVrEJKTk4O7d+++8pi7d+9CEATNYzMzM1hYWBRad3x8PFq2bIlNmzahT58++Prrr7F48WJ0794d33zzDdq1a4f09HT8/fffWLFiRXGa/0qZGRkIu34NHq3f0ir3aN0Gly5eMJgM5jBHqgzmyD9HCob8mtna5P7flJCU+yWXyjz3mi3pGVmaY3JyBGRkZqF1o+oAgMZ1KsHczBRHT4dpjomKScK1u4/Ryr2qXvtj7DnG1Bfm0JusWFfHeuutt7Bz507s3r0bvXr1yrN/165d2LlzJzp27Kgpu3XrFlxdXQute+7cuTA3N8fdu3fzXE1r7ty56NSpEwYNGoTDhw8XOAhRq9VQq9VaZYJSBZVKVWh+QmICsrOz4ejoqFXu6OiE2NiYQp+vCykymMMcqTKYI/8cKRjya7bw0z44df4Ort/Nna2/GRGN+4/j8MXY7hgzbytS0jIwblB7lCtjBxcnOwCAi6Mt1BmZSHyaplXXk7incHbU/dL0hvy66SvHmPrCHMPCi2OJq1gzIQsXLoSFhQX69u2LZs2a4eOPP8YXX3yBjz/+GM2aNYO/vz8sLS2xYMECAEBcXByOHDmC9u3bF1r3nj17sHjx4nwv5+vi4oJFixZh586dmDBhAgICAvKtIzAwEHZ2dlrbVwsDi9THly9BLAiCzpclllMGc5gjVQZz5J8jBUN7zZZO6YcGNVwRMHWjpiwrKwfvTPwebpXLIuqvrxB/egnaNq2BgyevITsnp9B2Ca88ouDnvUjur5sccoypL8yhN1GxZkIaNGiA4OBgjBkzBqdOncL58+e19rdp0wbffPMNGjZsCACwt7fHv//+Cysrq0LrjoqKQr169QrcX79+fZiYmGDWrFkFHjN16lRMmDBBq0xQFj4LAgAO9g5QKpWIjY3VKo+Pj4Ojo5NOdcghgznMkSqDOfLPkYIhvmZLJvujq2cD+AxbhkdPErX2XQh7gFYDFsDWxgLmZqaITXiGv36ciHPXc2/WGx2XDJW5GexLWWrNhpQpbYPQS/f00p83JceY+sIcepMVayYEANzd3REcHIyIiAj89ttv+Omnn/Dbb78hIiICwcHBaNSokeZYpVIJOzs7mJmZFVqvk5MTIiIiCtwfHh6OsmXLFrgfAFQqleZu7c83XZZiAYCZuTnq1K2H0JBTWuWhISFwb9RYpzrkkMEc5kiVwRz550jB0F6zpZP90aO9O3xHrsD9x3EFHpf8LB2xCc9QvVIZNKlbCfuCLgMALoRFIiMzCx1a/XcSuouTLepVd0XopXDJ+/Mm5RhTX5hjWBQKhWw3Q1SsmZAXVapUCZUqVRKjLQAAX19fTJ8+HUeOHIG5ubnWPrVajc8//xy+vr6i5eVnUMBQTJ8yCXXr14e7e2Ps/HUboqKi4N9/gEFlMIc5UmUwR945qSkpiIyM1Dx+9PAhboSFwc7ODuV0OFdPV4bymi2b2g/9/ZrB/5N1eJaSDmfHUgCApGfpSFdnAgB6+zRGTMIzPIiOR/0arlj8WV/8HnQZx0JvAMgdnGzccxoLJvRGXFIKEpJSEfhJL1y98xh//n1D0v68iTnG1BfmkNT++usvfPXVVzh37hyioqKwe/du9OzZU7NfEATMmTMH69atQ0JCAlq2bIlVq1ZprVRSq9WYOHEitm7dirS0NHTo0AGrV69GhQoVdG7Haw1CMjIycPToUdy4cQMpKSn4/PPPAQDp6elITk6Gk5MTTEyKNtkyZ84cNGvWDDVq1MDo0aNRu3but0zXr1/H6tWroVar8eOPP75Oswvl69cFSYkJWLdmNWJinsCtRk2sWrsOrq7lDSqDOcyRKoM58s65du0qPhg6WPN48aLcc+S69+iFL+YvEC3HUF6zkf3aAQCOfD9eq3z4zJ+w+fe/AQAuZWyx8NPeKOtYCtGxydiy728ErjuodfykxTuRnZ2DzQuHwVJlhuP/3MSIcT8hJ6doZ4UYyusmpxxj6gtzSGopKSlwd3fH0KFD0adPnzz7Fy1ahCVLlmDjxo2oWbMm5s2bh44dO+LmzZsoVSr3S5vx48fj999/xy+//AJHR0d8+umn6Nq1K86dOwelUqlTOxTCi9fRLYK9e/dixIgRiImJ0ZxslJ2dDQD4559/4OHhgZ9++gnvvvtukesODw/HqFGjcPjwYc1lfhUKBTp27IiVK1fCzc2tyHWmZxV+DBERGT+H5mMkyUk4s1KSHCJjYvHaa3RKzrYLj/TdhAL1b1y8AZ5CodCaCREEAa6urhg/fjwmT54MIHfWw9nZGQsXLsTIkSORlJSEMmXK4KeffkL//v0BAI8fP0bFihWxf/9+dO7cWafsYt+ssG/fvlCpVFi+fHmegUaLFi3g5uaGnTt3Fqd6VK1aFQcOHEBsbCxCQ0MRGhqKmJgYHDx4sFgDECIiIiIiY6VWq5GcnKy1vXy7Cl2Eh4cjOjoanTr9d3NblUoFT09PhISEAADOnTuHzMxMrWNcXV1Rv359zTG6KNYgZN68ebC3t8fZs2cxZswY1KhRI88xTZs2xaVLl4pTvYaDgwNatGiBFi1aoHTp0q9VFxERERGRMcrv9hSBgUW7PQUAREdHA0CeW2U4Oztr9kVHR8Pc3BwODg4FHqOLYk16hYaGom/fvihTpkyBx1SsWBF79+4tTvVERERERLIi56tQ5Xd7Cl2vDJuf4tznpaj3ginWTIharYadnd0rj0lKSirySelERERERFQ0r3N7ihe5uLgAQJ4ZjSdPnmhmR1xcXJCRkYGEhIQCj9FFsUYJ1apVw9mzZ195zOnTpzVXtiIiIiIiInmrWrUqXFxccOTIEU1ZRkYGTpw4gdatWwPIPeXCzMxM65ioqChcvXpVc4wuijUI6dOnD4KDgwu8VO7ixYtx9epVzRnzRERERESGTCHjrSiePXuGixcv4uLFiwByT0a/ePEiIiMjoVAoMH78eMyfPx+7d+/G1atXMWTIEFhZWWkuRGVnZ4dhw4bh008/xbFjx3DhwgW89957aNCgAXx8fHRuR7HOCfnss8+wc+dODB06FJs3b0Z6ejoAYNKkSTh9+jRCQkLQqFEjjBkjzWUQiYiIiIiocGfPnoW3t7fm8fNzSQICArBx40ZMmjQJaWlpGDVqlOZmhYcPH9bcIwQAli5dClNTU/Tr109zs8KNGzfqfI8Q4DXuE5KQkIAxY8Zg+/btmvuDALknsvTr1w+rV6/Oc9a8PvE+IUREBPA+IURyJuf7hPx68bG+m1Ag/0au+m5CkRX7rXZwcMCWLVuwYsUKnDlzBvHx8bC1tUXz5s2LdFIKEREREZHcyfnqWIbotcebjo6O8PX1FaMtRERERET0BpDxpBcREZH4pFom5dCz5HMS9vDcSyIyTDoPQkaNGlXkyhUKBVatWlXk5xERERERyQnvficunQcha9eu1bnSF9fMcRBCREREREQv0nkQcvz4cZ2Oi4yMxNy5c3H37l2ewENERERERHnoPAjx9PR85f6EhATMnz8fq1atQnp6Ojw8PLBw4cLXbiARERERkb7xy3VxvfaJ6enp6Vi2bBkWLVqExMRE1K5dG/Pnz0fPnj1FaB4RERERERmbYp9jIwgCvv/+e9SoUQPTpk2DlZUV1q1bh6tXr3IAQkREREREBSrWTMiePXswbdo03Lx5E7a2tpg/fz7Gjx8PCwsLsdtHRERERKR3XIwlriINQk6ePInJkycjNDQU5ubm+OSTTzB9+nQ4ODiUVPuIiIiIiMjI6DwI6d69O/744w+YmJggICAAc+fORYUKFUqybUREREREZIR0HoTs27cPCoUClSpVQnR0NEaMGFHocxQKBf7444/XaiARERERkb7x4ljiKtKJ6YIgIDw8HAcPHtR5M1Tbtm6BX6f2aN64AQb498b5c2cNMoM5zJEq49zZMxg76kP4eL0F93q18Oexo6JnPGdM742x5RhTX143Z6J/U5xc4o8n20fg/ub3sX16F9Qob5/nuFoVHPDr528jettwPNk+AicW90XFMjaa/eamJlgysh0ebBmG2B0j8evnb6O8o7Xk/ZFbjjH1hTn0JtJ5EBIeHl7k7d69eyXZ9hJz8MB+LFoQiOEjPsK2HXvQpElTjBo5HFGPHxtUBnOYI1UGAKSlpaJWrVqYMn2mqPW+zJjeG2PLMaa+iJHTtr4r1v5xBZ4Td6Dr579BqTTBvi+6w0r13yKEqi62OLaoD249TEDnqbvRYuwvCPzlDNIzsjXHfDWiLbp7VMPgRYfQYdJO2FiYYeesrjAxKdrXsobyusklgznyzyHDphAEQdB3I6SQnqX7sQMH+KNO3bqYMXOOpqxnNz94t/fBuE8+FaU9UmQwhzlSZbzMvV4tLF2xCu07+IhetzG9N8aWY0x9ESPHoedKrcdOthZ48PMH8Jm8C6eu5f4y9uOkTsjMysGwJfnPHNpamePBlmEYtuQIdgTfAQCUK22N2xsC0HPOPvw6s7tk/ZFTjjH1hTnaLF77DnYl5/cr/+q7CQXq1sBZ300osmLfJ0SfHj16VGJ1Z2ZkIOz6NXi0fkur3KN1G1y6eMFgMpjDHKkypGRM742x5RhTX0oqx9ZaBQBIeJYOIHd9uW+zKrj9OBF753bH/c3v46+v+6Jbq6qa5zR2KwNzMyWOnn+gKYuKT8G1yHi0qu2i1/7oK8eY+sIcepMZ1CAkOjoaY8eOhZub2yuPU6vVSE5O1trUarVOGQmJCcjOzoajo6NWuaOjE2JjY4rddqkzmMMcqTKkZEzvjbHlGFNfSipn4Qdv4dS1x7h+Px4AUNbOCqWszDGxb1McOXcf3T7fi72n7+GXaV3wVn1XAICLgzXUmdlITNH+P+xJQiqcHaz02h995RhTX5hDbzLZDUISExMxcOBAlClTBq6urlixYgVycnIwc+ZMVKtWDaGhoVi/fv0r6wgMDISdnZ3W9tXCwCK1Q/HSJRAEQchT9rqkyGAOc6TKkJIxvTfGlmNMfREzZ+mH7dCgiiMCFh3SlD0/p2NfaDi++e0SLofHYvGO89h/JgLD/eoX0i6gOGupDe1103cGc+SfIyWFQr6bIZLdyrtp06bhr7/+QkBAAA4ePIhPPvkEBw8eRHp6Og4cOABPT89C65g6dSomTJigVSYoVTrlO9g7QKlUIjY2Vqs8Pj4Ojo5OundEzxnMYY5UGVIypvfG2HKMqS9i5ywZ2Q5dW1aFz5RdeBSXoimPTU5DZlY2wh7Eax1/80E8WtfNnQmJTkiBykwJe2uV1mxIGXsrhIZF66U/+s4xpr4wh95kspsJ+eOPP7BhwwYsXrwYe/fuhSAIqFmzJv7880+dBiAAoFKpYGtrq7WpVLoNQszMzVGnbj2EhpzSKg8NCYF7o8ZF7o++MpjDHKkypGRM742x5RhTX8TMWfphO/RoXQ2+0/fg/r9PtfZlZuXg3O0nqPnSZXtrlLdH5JPcYy/ciUFGZjY6NK6o2e/iYIV6lUoj9IbugxBDe930ncEc+eeQ4ZPdTMjjx49Rt25dAEC1atVgYWGBDz74QNI2DAoYiulTJqFu/fpwd2+Mnb9uQ1RUFPz7DzCoDOYwR6oMAEhNSUFkZKTm8aOHD3EjLAx2dnYo5+oqWo4xvTfGlmNMfREjZ9lHnujvWRP+8/7As9RMONvnnsORlKrWXIJ36a4L+GlSZ5y89hgnLj9Cp6aV0KVFVXSeuhsAkJyagY1HrmPBsDaIe5qOhKfpCBzWBlfvx+HPiw8KzC6J/sgpx5j6whzDoYCBrnuSKdkNQnJycmBmZqZ5rFQqYW1dvJsyFZevXxckJSZg3ZrViIl5ArcaNbFq7Tq4upY3qAzmMEeqDAC4du0qPhg6WPN48aLc87C69+iFL+YvEC3HmN4bY8sxpr6IkTPy7QYAgCMLemuVD196FJuP3QAA7D19D2NXB+Ez/6b4ekQ73HqUgHfmH0DI9SjN8ZO+O4nsbAGbJ/vC0lyJ45cfYsTSP5CTU7SzQgzldZNLBnPkn0OGTXb3CTExMYGfn59m+dTvv/+O9u3b5xmI7Nq1q0j1FuU+IURERK/r5fuElISEPWNKPINISnK+T8gfV5/ouwkFert+WX03ochk91YHBARoPX7vvff01BIiIiIiolyGehUquZLdIGTDhg36bgIREREREZUg2V0di4iIiIiIjJvsZkKIiIiIiOTGhFfHEhVnQoiIiIiISFIchBARERERkaS4HIuIiIiIqBC8Opa4OBNCRERERESS4iCEiIiIiIgkxeVYRERERESF4HIscXEQQkREVAIS9owp8QyHTl+WeAYAJByeLkkOEb05uByLiIiIiIgkxZkQIiIiIqJCKHizQlFxJoSIiIiIiCTFQQgREREREUmKy7GIiIiIiAphwtVYouJMCBERERERSYqDECIiIiIikhSXYxERERERFYJXxxIXZ0KIiIiIiEhSHIQQEREREZGkOAgpwLatW+DXqT2aN26AAf69cf7cWYPMYA5zpMpgDnOkymCOtonvtMbJ1UPxZN9E3N85Htvn9kWNiqW1jkn7c3q+2yf9W2mOef/txji05D38+/tEpP05HXbWKr30R04ZzJF/jpQUCvluhoiDkHwcPLAfixYEYviIj7Btxx40adIUo0YOR9TjxwaVwRzmSJXBHOZIlcGcvNq6V8La387Bc8xGdP3sZyiVJti36F1YWZhpjqnSZ5nWNmLR78jJEbD7rxuaY6wsTHHkzF189fMpvfZHLhnMkX8OGTaFIAiCvhshhfQs3Y8dOMAfderWxYyZczRlPbv5wbu9D8Z98qko7ZEigznMkSqDOcyRKoM52hw6fZmnzMnOCg92fwKf8T/i1OUH+T5v+9y+sLEyR5eJP+fZ19a9Eg4vHQSXbouRlKIGACQcni5Jf+SUwRz95FjI+JJJx2/G6bsJBfKu5ajvJhSZrGZC/v77bxw4cECr7Mcff0TVqlVRtmxZjBgxAmq1ukTbkJmRgbDr1+DR+i2tco/WbXDp4gWDyWAOc6TKYA5zpMpgjm5s/7+MKiE5Pd/9ZR2s4dvKDZv2XypW/a/CzxpzpMrRB4WM/xgiWQ1CZs+ejcuXL2seX7lyBcOGDYOPjw+mTJmC33//HYGBgYXWo1arkZycrLXpOnhJSExAdnY2HB21R5SOjk6IjY0pWof0mMEc5kiVwRzmSJXBHN0sHOWDU5cjcT0i/+e/16kBnqZmYE/wjXz3vw5+1pgjVQ4ZPlkNQi5evIgOHTpoHv/yyy9o2bIlvvvuO0yYMAErVqzA9u3bC60nMDAQdnZ2WttXCwsfvLxI8dJZPoIg5Cl7XVJkMIc5UmUwhzlSZTCnYEs/7owG1coiYN6eAo8Z7OeObceuQp2ZXeT6dcXPGnOkyiHDJauVdwkJCXB2dtY8PnHiBHx9fTWPmzdvjgcP8l/f+qKpU6diwoQJWmWCUrerfDjYO0CpVCI2NlarPD4+Do6OTjrVIYcM5jBHqgzmMEeqDOa82pKxndC1dU34jP8Rj2Kf5ntMmwYVUauSEwbN3V3sNr8KP2vMkSpHH0w4hhKVrGZCnJ2dER4eDgDIyMjA+fPn4eHhodn/9OlTmJmZFfR0DZVKBVtbW61NpdJtEGJmbo46deshNET76iChISFwb9S4CL3RbwZzmCNVBnOYI1UGcwq29OPO6NG2Nnw/3Yz70UkFHhfg545zN6Nw5d6TYrf5VfhZY45UOWT4ZDUT4uvriylTpmDhwoXYs2cPrKys0LZtW83+y5cvo3r16iXejkEBQzF9yiTUrV8f7u6NsfPXbYiKioJ//wEGlcEc5kiVwRzmSJXBnLyWjfNF/w714D/jVzxLzYCzgzUAIClFjfSM/y4NWcrKHL0962DK2mP51uPsYA3n0jaoXj73HiP1q5XF09QMPHhS8KCmJPojlwzmyD+HDJusBiHz5s1D79694enpCRsbG2zatAnm5uaa/evXr0enTp1KvB2+fl2QlJiAdWtWIybmCdxq1MSqtevg6lreoDKYwxypMpjDHKkymJPXyB5NAQBHlg3SKh++8HdsPvTfxV78vetBoVBg+5/X8q3ng+5NMCOgnebx0eWDNfVI2R+5ZDBH/jlSM9SrUMmVLO8TkpSUBBsbGyiVSq3y+Ph42NjYaA1MdFWU+4QQEREZgvzuE1ISinKfEKLXIef7hATfStB3EwrUtqaDvptQZLJ8q+3s7PItL126tMQtISIiIiIisclyEEJEREREJCe8wrC4ZHV1LCIiIiIiMn4chBARERERkaS4HIuIiIiIqBBcjSUuzoQQEREREZGkOAghIiIiIiJJcTkWEREREVEhTHh5LFFxJoSIiIiIiCTFQQgREREREUmKy7GIiIgMVMLh6ZLkVBu9S5Kce6t6S5JDVBxcjCUuzoQQEREREZGkOAghIiIiIiJJcTkWEREREVFhuB5LVJwJISIiIiIiSXEQQkREREREkuJyLCIiIiKiQii4HktUnAkhIiIiIiJJcRBCRERERESS4nIsIiIiIqJCKLgaS1ScCSEiIiIiIklxEFKAbVu3wK9TezRv3AAD/Hvj/LmzBpnBHPnmnDt7BmNHfQgfr7fgXq8W/jx2VNT6X2Qsrxlz5J9jTH1hjrYxvjWxf6o3bi3vhstfdcH6j1qhurONZr+piQLTe9fDsZkdcGdFd5xf6IflQ5rC2c5Cq57KTtb44cNWuLL4bdxc1g1rh7eAUymVpH1hjvHkkOHiICQfBw/sx6IFgRg+4iNs27EHTZo0xaiRwxH1+LFBZTBH3jlpaamoVasWpkyfKVqd+TGm14w58s4xpr4wJy+PmmWwMeguui4IwoDlp6A0UWDruLdgaa4EAFiaK9Ggoj2W/XEDnb/8Ex+sDUU151LYONpDU4eluRJbx7eBAAH+S4LRY9EJmJuaYNNojyItdTGU14w5JZsjNYWMN0OkEARB0Hcjnnv//fexfPlylCpVSvS607N0P3bgAH/UqVsXM2bO0ZT17OYH7/Y+GPfJp6K0R4oM5sg/5zn3erWwdMUqtO/gI3rdxvaaMUe+OcbUF+ZoqzZ6V56y0jbmuPp1V/RafAJ/347L93nulR1wYJo3mk85gEcJafCsUxabP26DOp/8jmf//4/ZzsoMYUu7of/SYGwa27bE+1IUzJE+x0LGZyufuZek7yYUqHk1O303ochkNROyadMmpKWl6bUNmRkZCLt+DR6t39Iq92jdBpcuXjCYDObIP0cKxvaaMUe+OcbUF+boxtbSDACQmJL5imNMkZMjICkt9xhzMxMIgoCMrBzNMerMbGTnCGjh5qRTriG/ZswxrP9DqWTJahAi1qSMWq1GcnKy1qZWq3V6bkJiArKzs+Ho6KhV7ujohNjYGFHaJ0UGc+SfIwVje82YI98cY+oLc3Qz278h/r4di5uPk/PdrzI1wbTe9bH7zAPNrMe5e/FIzcjG9N71YWmmhKW5Ep/3aQCliQJlXzp3RMq+MMfwcvRC32uujGw9lqwGIQCgEOH6Z4GBgbCzs9PavloY+FrtEARBlLZJncEc+edIwdheM+bIN8eY+sKcgs1/xx11ytti1Pdn8t1vaqLAmuEtYKJQYOrPFzXl8c8yMPLbv9GxoQtur+iOm8u6oZSlGS7fT0B2TtG+iDS014w5hvt/KJUM2a28q1mzZqEf0vj4+Ffunzp1KiZMmKBVJih1u/KGg70DlEolYmNjX8qMg6OjblPFcshgjvxzpGBsrxlz5JtjTH1hzqvNG+COTg3LodfivxCVmHcJtamJAt+OaImKjlbot/SkZhbkuRNhT9B6xmGUtjZHVo6A5LRMXFzUBQ/iHkreF+YYbg4ZPtnNhMyZMwdLly595VYYlUoFW1tbrU2l0m0QYmZujjp16yE05JRWeWhICNwbNS5Wn/SRwRz550jB2F4z5sg3x5j6wpyCfTnAHX6NXOG/NBgP4lLz7H8+AKla1hr9l51EQkpGgXXFp2QgOS0TbWqVgVMpFQ5fipK0L8wx7Bx9UMj4jyGS3UzIgAEDULZsWb22YVDAUEyfMgl169eHu3tj7Px1G6KiouDff4BBZTBH3jmpKSmIjIzUPH708CFuhIXBzs4O5VxdRcsxpteMOfLOMaa+MCev+e80Qq8WFTB0dSiepWehjG3ul3tP0zKRnpkDpYkC341siQaV7DF41WkoTRSaYxJTMpCZnbvcqn/ryrgdlYy4pxloWr005vZriHXH7uDuv88k6wtzjCOHDJusBiFyWSvo69cFSYkJWLdmNWJinsCtRk2sWrsOrq7lDSqDOfLOuXbtKj4YOljzePGi3POWuvfohS/mLxAtx5heM+bIO8eY+sKcvIZ4VQMA7JrYTqt8/Maz2H46EuUcLNG5Ue4XKEc/76B1TJ+v/8LpW7nLc6o722Bqz3qwtzbHg7gUrDhwE+uO3pG0L8wxjhwybLK6T4iJiQmio6NLZCakKPcJISIiov/kd5+QknBvVW9Jcki+5HyfkHMR+V8JTg6aVrHVdxOKTFZvdU5OTuEHERERERGRQZPdielERERERGTcZDUTQkREREQkR/I4c9l4cCaEiIiIiIgkxUEIERERERFJisuxiIiIiIgKw/VYouJMCBERERERSYqDECIiIiIikhSXYxERERERFULB9Vii4kwIERERERFJSiEIgqDvRkghPUvfLSAiIqJXqT52tyQ5d7/pJUkOFZ2FjNfoXLj/VN9NKFDjyqX03YQik/FbTUREREQkDwquxhIVl2MREREREZGkOAghIiIiIiJJcTkWEREREVEhuBpLXJwJISIiIiIiSXEQQkREREREkuJyLCIiIiKiwnA9lqg4E0JERERERJLiIISIiIiIiCTF5VhERERERIVQcD2WqDgTUoBtW7fAr1N7NG/cAAP8e+P8ubMGmcEc5kiVwRzmSJXBHOPOGdO5Jv6Y7IWbS7vi0qIu+GFkS1R3ttHsNzVRYFrPejg6oz1uL+uGc4G+WB7QFM52FppjKpS2wqM1vfLdujZxlawvzCn5HDJcHITk4+CB/Vi0IBDDR3yEbTv2oEmTphg1cjiiHj82qAzmMEeqDOYwR6oM5hh/TqsaTth04h66LTqBd5afhKnSBD+PbQNLcyUAwNJciQaV7LF8/034Bh7H8HV/o5qzDTZ81EpTx+OEVDSavF9r++r360hJz8Kf1/6VrC/MKdkcMmwKQRAEfTdCCulZuh87cIA/6tStixkz52jKenbzg3d7H4z75FNR2iNFBnOYI1UGc5gjVQZzjDun+tjdecpK25jjyldvo/fXf+HvO3H5Ps+9sj32T/FG82kH8TghLd9jDk3zxpXIREzcfAF3v+lV4n0pCub8x0LGJwpcefhM300oUIMKNoUfJDOcCXlJZkYGwq5fg0frt7TKPVq3waWLFwwmgznMkSqDOcyRKoM5b2aOraUZACAxNeOVx+TkCEhOy8x3f4NK9qhf0R6/hNzXOdeQX7M3IYcMn+wGISYmJlAqlXk2BwcHtGrVCrt27Sq0DrVajeTkZK1NrVbrlJ+QmIDs7Gw4OjpqlTs6OiE2NqZYfdJHBnOYI1UGc5gjVQZz3sycWX0b4O87sbj5+Gm++1WmJpjasx52n3mAZwUse3indWXcikrG2XvxOuca8mv2JuSQ4ZPdpNfu3XmnYgEgMTER//zzD9577z1s2rQJ/v7+BdYRGBiIOXPmaJVN/3wWZsycrXM7FArtKyAIgpCn7HVJkcEc5kiVwRzmSJXBnDcn58sB7qhT3ha9Fv+V735TEwVWD2sOE4UC0365lO8xFmYm6Nm8Apbvv1nkfMDwXrM3LUdKht16+ZHdIKRHjx4F7gsICEDdunWxePHiVw5Cpk6digkTJmiVCUqVTvkO9g5QKpWIjY3VKo+Pj4Ojo5NOdcghgznMkSqDOcyRKoM5b1bOF/0aolMDF/ReEoyoxPQ8+01NFFg7vAUqOVmj37KTBc6CvN24PCzNTfHr35FFyjfE1+xNyiHDJ7vlWIXp1KkTbt269cpjVCoVbG1ttTaVSrdBiJm5OerUrYfQkFNa5aEhIXBv1LjY7ZY6gznMkSqDOcyRKoM5b07OvP4N4dfYFf2WncSDuNQ8+58PQKqWtUH/5SeRkFLw+SID2lTGkctRiH9W8DH5MbTX7E3LIcMnu5mQwqSlpcHCwqLwA1/DoIChmD5lEurWrw9398bY+es2REVFwb//AIPKYA5zpMpgDnOkymCO8efMH+COns0r4P21oXimzkIZ29wvEZ+mZSI9MwdKEwXWjWiJBhXtELD6NJQmCs0xiSkZyMz+76KfVcpYo5WbEwatCtFLX5hTsjmS43osURncIOS7775D48YlO5L29euCpMQErFuzGjExT+BWoyZWrV0HV9fyBpXBHOZIlcEc5kiVwRzjzwnwrAYA2DmhnVb5J5vOYXtoJMrZW6KzezkAwJEZHbSO6bskGKdv/7cMaEDryohOTMOJsCd66QtzSjaHDJvs7hPy8rkczyUlJeHs2bO4e/cugoODizwQKcp9QoiIiEh6+d0npCToep8Qkp6c7xNy9ZF87xNSv7zh3SdEdm/1hQv5X0Pa1tYWvr6+GDVqFCpXrixxq4iIiIjoTabgeixRyW4Qcvz4cX03gYiIiIiISpDBXR2LiIiIiIiKbvbs2VAoFFqbi4uLZr8gCJg9ezZcXV1haWkJLy8vXLt2rUTawkEIEREREVEhFAr5bkVRr149REVFabYrV65o9i1atAhLlizBypUrcebMGbi4uKBjx454+vSpyK8mByFERERERG8MU1NTuLi4aLYyZcoAyJ0FWbZsGaZPn47evXujfv362LRpE1JTU/Hzzz+L3g4OQoiIiIiIDJharUZycrLWplar8z329u3bcHV1RdWqVTFgwADcu3cPABAeHo7o6Gh06tRJc6xKpYKnpydCQop3r51X4SCEiIiIiKgQChlvgYGBsLOz09oCAwPz9KFly5b48ccfcejQIXz33XeIjo5G69atERcXh+joaACAs7Oz1nOcnZ01+8Qku6tjERERERGR7qZOnZrnXnsqlSrPcX5+fpq/N2jQAB4eHqhevTo2bdqEVq1aAQAUL51kIghCnjIxcCaEiIiIiMiAqVQq2Nraam35DUJeZm1tjQYNGuD27duaq2S9POvx5MmTPLMjYuAghIiIiIioMPpec/WqrZjUajXCwsJQrlw5VK1aFS4uLjhy5Ihmf0ZGBk6cOIHWrVsXP6QAXI5FREREsnD3m16S5LRdGCRJTvBkL0lyiHQ1ceJEdOvWDZUqVcKTJ08wb948JCcnIyAgAAqFAuPHj8f8+fNRo0YN1KhRA/Pnz4eVlRXeffdd0dvCQQgRERER0Rvg4cOHeOeddxAbG4syZcqgVatWCA0NReXKlQEAkyZNQlpaGkaNGoWEhAS0bNkShw8fRqlSpURvi0IQBEH0WmUoPUvfLSAiIiI54EyIfFnI+OvxG1Gp+m5CgWqXs9J3E4qM54QQEREREZGkOAghIiIiIiJJyXjSi4iIiIhIHkrgVhlvNM6EEBERERGRpDgIISIiIiIiSXE5FhERERFRIbgaS1ycCSEiIiIiIklxEEJERERERJLiIKQA27ZugV+n9mjeuAEG+PfG+XNnDTKDOcyRKoM5zJEqgznMESOjcUU7LOlXH/s/9sCZ6V7wrOmktb+0tRlmda2N/R97IHhSW6wY0BAVHSw1+8vZWeDMdK98tw61y0jenzc1R1IKGW8GiIOQfBw8sB+LFgRi+IiPsG3HHjRp0hSjRg5H1OPHBpXBHOZIlcEc5kiVwRzmiJVhaa7ErX9T8NWh2/nu/6pvfbg6WGDir1fx3vdnEZWUjlUD3WFhlvur07/J6fBdFqK1fXsiHKkZ2Qi5Gy95f97EHDJsshqEXLx4Ud9NAAD8tGkDevXpg959/VGtenVMmjodLuVcsH3bVoPKYA5zpMpgDnOkymAOc8TKCLkbj7UnwnH8ZmyefZVKW6JhBTssPHAL16Oe4n58GhYevAVLMyU613MGAOQIQFxKhtbmVcsJR64/QVpmtuT9eRNzyLDJahDSpEkTNG3aFGvWrEFSUpJe2pCZkYGw69fg0fotrXKP1m1w6eIFg8lgDnOkymAOc6TKYA5zpMowU+b+eqTOytGU5QhAVk4OGlWwy/c5tV1sUMulFPZejCpSljG9N1Lm6INCxn8MkawGIadOnUKTJk0wZcoUlCtXDu+99x6OHz9e5HrUajWSk5O1NrVardNzExITkJ2dDUdHR61yR0cnxMbGFLkt+spgDnOkymAOc6TKYA5zpMqIiEvF48R0jPauhlIWpjA1USDAoxKcbFRwtDHP9zk9GpXDvZgUXH6UXKQsY3pvpMwhwyerQYiHhwe+++47REdHY82aNXj48CF8fHxQvXp1fPnll3j48KFO9QQGBsLOzk5r+2phYJHaolBojyoFQchT9rqkyGAOc6TKYA5zpMpgDnNKOiM7R8DknVdR2dEKf376FoInt0PTyvY4dScOOYKQ53iVqQk613PG3ktFmwV5kTG9N1LmkOGS5c0KLS0tERAQgICAANy9excbNmzAt99+i9mzZ6Njx47Yv3//K58/depUTJgwQatMUKp0ynawd4BSqURsrPYa0fj4ODg6OhXwrKKRIoM5zJEqgznMkSqDOcyRKgMAbkQ/w8Dvz8JapYSZ0gSJqZnYMKQJwqKe5jm2fe0ysDAzwR9X/i1yjjG9N1Lm6APHUOKS1UxIfqpXr44pU6Zg+vTpsLW1xaFDhwp9jkqlgq2trdamUuk2CDEzN0eduvUQGnJKqzw0JATujRoXqw/6yGAOc6TKYA5zpMpgDnOkynhRijobiamZqOhgiTrlSuHErbwnsvdoVA5/3YpDYmpmkes3pvdGyhwyfLKcCXnuxIkTWL9+PXbu3AmlUol+/fph2LBhJZ47KGAopk+ZhLr168PdvTF2/roNUVFR8O8/wKAymMMcqTKYwxypMpjDHLEyLM2UqFj6v/t+uNpboKazDZLSMvFvshodapdBQmom/k1OR/Wy1vi0Yw2cuBWLv8MTtOqp4GCJxpXsMP6XK3rtz5uYQ4ZNdoOQBw8eYOPGjdi4cSPCw8PRunVrfPPNN+jXrx+sra0laYOvXxckJSZg3ZrViIl5ArcaNbFq7Tq4upY3qAzmMEeqDOYwR6oM5jBHrIw65Urh20GNNI8ndHQDAOy7FI05+27AycYcn3SsjtLW5oh9loH9V6LxffD9PPV0d3dBzFM1Qu8V7d4gYvfnTcyRGldjiUshCPmcYaUnHTt2xPHjx1GmTBkMHjwY77//PmrVqiVK3elZolRDREREBq7twiBJcoIne0mSY0wsZPf1+H/uPknTdxMKVL2sZeEHyYys3mpLS0vs3LkTXbt2hVKp1HdziIiIiIioBMhqELJ37159N4GIiIiIKC+uxxKV7K+ORURERERExoWDECIiIiIikpSslmMREREREcmRguuxRMWZECIiIiIikhQHIUREREREJCkuxyIiIiIiKoSCq7FExZkQIiIiIiKSFAchREREREQkKYUgCIK+GyGF9Cx9t4CIiIjeJA79fpAkJ2H7MElypGAh4xMFImLT9d2EAlVxstB3E4qMMyFERERERCQpDkKIiIiIiEhSMp70IiIiIiKSCV4dS1ScCSEiIiIiIklxEEJERERERJLiciwiIiIiokIouB5LVJwJISIiIiIiSXEQQkREREREkuJyLCIiIiKiQii4GktUnAkhIiIiIiJJyWoQcufOHX03QWPb1i3w69QezRs3wAD/3jh/7qxBZjCHOVJlMIc5UmUwhznnzp7B2FEfwsfrLbjXq4U/jx0Vtf4XvU5fJvZuiJOLuuPJlkG4v+FdbJ/sgxqudlrHrBvTFmm7hmltJxZ00zrmmw/b4Npqf8RvDUDkhnexfYoPapbXrkeK/sgxhwyXrAYhNWvWRMWKFTF48GBs2LABERERemnHwQP7sWhBIIaP+AjbduxBkyZNMWrkcEQ9fmxQGcxhjlQZzGGOVBnMYQ4ApKWlolatWpgyfaZodebndfvStl45rD0QBs8pv6PrnINQKhXYN8sXVirt1fCHzj9Alfd/1mw95x3S2n/hbixGrAxGo493ovsXh6BQAPtm+sLEpGjrg4zpM0CGTyEIgqDvRjwXHByMEydOICgoCKdPn0Z6ejoqVaqE9u3bw9vbG97e3ihfvnyx6k7P0v3YgQP8UaduXcyYOUdT1rObH7zb+2DcJ58WK18fGcxhjlQZzGGOVBnMYc7L3OvVwtIVq9C+g4/odb9uXxz6/aD12MnWAg82DoTPjD9w6no0gNyZEHtrFfot1H02p35lB5xZ2ht1P9qO8H+fImH7MEn6o6vXybGQ8dnKD+LV+m5CgSqWVum7CUUmq5mQtm3bYsaMGTh69CgSExNx/PhxDB06FOHh4RgxYgQqVaqEWrVqlWgbMjMyEHb9Gjxav6VV7tG6DS5dvGAwGcxhjlQZzGGOVBnMYY6USqIvtlZmAICEZ9q/zLat74L7G97F5ZV9seqjt1DGzqLAOqxUphjcvibCo5PxMC5F52x+BkhuZDveNDMzQ7t27dC8eXN4eHjg0KFD+O6770r8vJGExARkZ2fD0dFRq9zR0QmxsTEGk8Ec5kiVwRzmSJXBHOZIqST6snBoS5y6Ho3rkQmassMXHmLX6QhExjxDlbI2mPlOUxyY0wWtJ+5BRlaO5rgRvnXw5aDmsLE0w42HiXh7zkFkvrBfH/3RZw4ZPtkNQtLT0xESEoLjx48jKCgIZ86cQdWqVeHp6Yk1a9bA09Oz0DrUajXUau1vGQSlCiqV7lNVipeuwyYIQp6y1yVFBnOYI1UGc5gjVQZzmCMlsfqydLgHGlQujQ7T92mV7zgVrvn79cgEnL8bi5tr+8OvaUX89vd9zb5f/rqDY5cewcXBCuN71Mfmie3Rfto+qDOz9dIfueRIycCbLzuyWo7l6emJ0qVLY9y4cYiPj8fYsWNx//59hIWFYe3atXj33Xd1OickMDAQdnZ2WttXCwN1aoODvQOUSiViY2O1yuPj4+Do6FSsfukjgznMkSqDOcyRKoM5zJGSmH1Z8kErdG1eCZ1n7sejuNRXHhudkIbImGdwe+kqWsmpmbgblYxT16Px7ld/olZ5O/RoWVnnNvAzQHIjq0FISEgInJyc4O3tjQ4dOqB9+/ZwdnYucj1Tp05FUlKS1vbZ5Kk6PdfM3Bx16tZDaMgprfLQkBC4N2pc5LboK4M5zJEqgznMkSqDOcyRklh9WfqBB3q0rALfWQdw/8mzQo8vbaNCBSdrRCW8erCiUChgbqbUuR38DJDcyGo5VmJiIoKDgxEUFISFCxfinXfeQc2aNeHp6QkvLy94enqiTJkyhdajUuVdelWUq2MNChiK6VMmoW79+nB3b4ydv25DVFQU/PsPKGqX9JrBHOZIlcEc5kiVwRzmAEBqSgoiIyM1jx89fIgbYWGws7NDOVdX0XJety/LRrRG/7bV4B94FM/SMuFsbwkASErNQHpGNqwtTDGjfxPsOR2BqIRUVC5rg7kDmyHuqRp7Q3OXYlVxLoW+bari2MVHiE1Oh2tpa3zaqyHSMrJw6PwDSfsjtxzpcT2WmGQ1CLG2toavry98fX0BAE+fPsXJkydx/PhxLFq0CAMHDkSNGjVw9erVEm2Hr18XJCUmYN2a1YiJeQK3GjWxau06uLoW7/LA+spgDnOkymAOc6TKYA5zAODatav4YOhgzePFi3KXXHfv0QtfzF8gWs7r9mWkbx0AwJF5b2uVD//mL2w+fhvZOQLqVXLAu15usLcyR3RiGk5ceYxBXx/Hs/RMAIA6Ixtt6rhgTNf6cLA2x5OkNJy8Hg3vqfsQk5QuaX/klkOGTVb3CXlZTk4Ozpw5g+PHj+P48eM4efIk0tPTkZ1dtJOwgKLNhBARERG9rpfvE1JSdL1PiCGQ831CHiZk6LsJBargYK7vJhSZrN7qnJwcnD17FkFBQTh+/DhOnTqFlJQUlC9fHt7e3li1ahW8vb313UwiIiIiesPw6ljiktUgxN7eHikpKShXrhy8vLywZMkSeHt7o3r16vpuGhERERERiURWg5CvvvoK3t7eqFmzpr6bQkREREREJURWg5CRI0fquwlERERERHlwNZa4ZHWfECIiIiIiMn4chBARERERkaRktRyLiIiIiEiOeHUscXEmhIiIiIiIJMVBCBERERERSYrLsYiIiIiICqHg9bFExZkQIiIiIiKSlEIQBEHfjZBCepa+W0BEREQkvqqjdpZ4RvjqPiWeAQAWMl6jE52Uqe8mFMjFzkzfTSgyGb/VREREREQywdVYouJyLCIiIiIikhQHIUREREREJCkuxyIiIiIiKgRXY4mLMyFERERERCQpDkKIiIiIiEhSXI5FRERERFQIBddjiYozIUREREREJCkOQoiIiIiISFIchBRg29Yt8OvUHs0bN8AA/944f+6sQWYwhzlSZcg1Jzs7G2p1OtTpRd927/wVgwf2R/cunTD6w+G4cO5ssep5k3JEy1CnIzs7W5TPwOtgjnxzjKkvr5sz1rcWDkzzxu0V3XFl8dvYMMoD1Z1ttI75tFsdBM/thLvf9EDY0m7Y9klbNK7qoHWMuakJ5g1wx7UlXXH3mx7YONoD5ewtJe+PXClk/McQKQRBEPTdCCmkZ+l+7MED+zF9yiRM/3wWGjVugh3bf8GunTuwe+8fKOfqKkp7pMhgDnOkypBjjiAIiH0SjadJicXKSUtLQ2JiAuzs7GBubo6UlFSkpaWiTJmyUCqVIvXGuHJKIqOUnT2cyrpA8cJibLl91pgjfY4x9UWMnNM3orHnzENcjIiHqdIEU3rWQ53ytmg36wjSMnIH871aVETsUzXux6TAwswEI3xqoFuzCmg9/SDinmUAABa82xgd3V0wfuM5JDxTY5Z/Q9hbm6PzvGO4u6qPJP2xkPHZyjFPi/DLpMTKlJLxC1cADkLyMXCAP+rUrYsZM+doynp284N3ex+M++RTUdojRQZzmCNVhhxzYv6NwtOkRJQpWxZWVlZav8Tq4sGDSFioVChT1llTdj8iHNY2NnByKvP6HTHCHDEzBEFAamoqYp48QSk7e5RxLqfZJ7fPGnOkzzGmvoiRU3XUTq3HjjbmuLqkG3p9dQKht2PzfY6NhSlur+gB/yV/4eSNGJSyNMXVr7th7Poz2Hv2IQDA2c4C5xZ2wXsrTmHDmLck6Q8HIcVjiIMQWS3HmjlzJrKyCn6DIyMj0bFjxxJtQ2ZGBsKuX4NHa+1/bB6t2+DSxQsGk8Ec5kiVIcec7OxszQDE0dERlpaWsLCw0HlTmZsjMyMDdvYOWuV2dnbIyswsUl1vSo7YGZaWlnB0dESZsmXxNClRszRLbp815kifY0x9KamcUpZmAICElIx895spFXivbVUkpWbg+sMkAEDDSg4wNzXBiev/ao77NykdNx4loVn10jpnS/W66YVCxpsBktUgZOPGjWjevDmuXLmSZ9+6detQv359mJqW7EgvITEB2dnZcHR01Cp3dHRCbGyMwWQwhzlSZcgxJysrEwBgZWVVrJys///Ca2qqvYTI1NT0lV+UvMk5JZXx/D18/p7K7bPGHOlzjKkvJZUzu19D/H07FjcfJ2uV+zRwwZ0VPRCxqhdG+NRA/6UnEf//pVhl7SygzsxGUmqm1nNin6pR1s5Cr/0h4ySrQcjVq1fRoEEDNG/eHIGBgcjJyUFkZCR8fHwwadIkLFmyBAcOHCi0HrVajeTkZK1NrVYXqS0vL90QBKHIyznkkMEc5kiVIascIf/jipGknfOatb0ZOeJmaN5DoYDy5znG/plmjl4yDDFn/juNULe8HT767p88+07djIHPF0fRbWEQjl+LxrqRLeFYSlVoncVZuC/V60aGS1aDEFtbW/z444/Ytm0bli9fjiZNmqBBgwYwNTXFlStX8MEHH+hUT2BgIOzs7LS2rxYG6vRcB3sHKJVKxMZqr6GMj4+Do6NTkfukrwzmMEeqDGPMMf3/SdQvf4OfnZUl6mysMeVI1Rdj+6wxR54Zhpozb4A7OrmXQ5+v/0JUYlqe/WkZ2YiIScH58Hh8+uN5ZGULeLdNFQDAk6R0qMyUsLMy03qOUykVYpLT9dIfudH3iisjW40lr0HIcy1btkSDBg1w+fJl5OTkYNKkSahYsaLOz586dSqSkpK0ts8mT9XpuWbm5qhTtx5CQ05plYeGhMC9UeMi9UOfGcxhjlQZxpijMDGBhaUlUp490yp/9iwFlsVc4qWvnPbeXpgwYcJr5bT39sIn48e/8hipXjNj+6wxR54Zhpjz5TuN0KVxefgvCcaDuFSdnqNQAOZmub8KXo5MQEZWDtrVKavZX9bOArXL2+Hs3Xid2yHV60aGT3an0m/duhVjxoxBo0aNEBYWhh9++AF+fn748MMPsWDBAlhaFn69apVKBZVKe3qxKFfHGhQwFNOnTELd+vXh7t4YO3/dhqioKPj3H1DU7ug1gznMkSrDGHMcHZ3w6OFDWFhawMrKCgnxCcjMzERpB91P0CxKToUK5V953OCAAGzYsLHI9e/YuQtmZmbIyckp8f5I9ZoZ22eNOfLMMKScwHcboVeLihi6+jSepWeijG3u70BP0zKRnpkDS3MlxnepjUOXovAkKR0ONuYI8KyGcg6W+P3/V8J6mpaFrScjMMu/IRJSMpCYkoGZfRsi7FES/gr791XxoveH3gyyGoT07dsXhw4dwvz58zF27FgAwKJFi9CrVy8MGTIEBw4cwKZNm+Dh4VGi7fD164KkxASsW7MaMTFP4FajJlatXQdX11f/kiC3DOYwR6oMY8yxs7NDdnYWYmNikJWVBZVKhcqVK8PM3LxEcoKDg5GdnQ1zc3MEBwfjyy+/RNiNm5rjXv4CJjMzE2ZmZi9Xl0fp0v8NAEq6P1K9Zsb2WWOOPDMMKWeIV3UAwK6Jnlrl4zacxfbT95GTI8DNpRT8PSqjtI05ElIycDEiAT0XncCtqKea42dtv4SsnBx8O6IlLM2VCA6LwfiVIcgp4jkhUr1uUuMpLeKS1X1C2rRpg02bNsHNzS3PvvT0dEyePBlr1qxBRkb+l5x7laLMhBCRYVOnp+NRZDiqVK0KC4vcq7oIgoC0zILvwF1SLM2URT4Zc+PGjZjwyXjEJyQCACIiIlC9WlVs/WUb1q5ZjdDQUKxavQbdu3fH2LFjcDI4GPHx8ahevTqmTJ2Gd955R1NXe28vuLs3wtJlywAA1apWwfDhI3Dnzh3s2PErHBwcMG36DIwYMaLA9rxcR0JCAsaPH4d9v/8OtVqNdp6eWL58BWrUqAEAuH//PsaOHYNTJ08iIyMDVapUwcJFX6FLly5ISEjA2LFjcOTwYTx79gwVKlTAlKnTMHTo0Hyz09PTEREejvKVqkJlofsVeojeJC/fJ6QkhK/W/WaFr0PO9wmJS5HvL5OO1jJ+4QogqxYHBwfDxCT/01QsLCywfPly9OkjzT8CIjIuaZnZqD/rkOS5V+d0hpW5OD9qp06ZjK8Wf40f1m+ASqVCeno6mjZpikmTJsPW1hb7//gDAYMHoVq1amjZsmWB9SxZ8jXmzv0CU6dNw84dOzB61Edo164dateurVM7hg4dgju3b2PPb3tha2uLKVMmo+vbXXD12nWYmZlhzJjRyMjIQNCJv2BtbY3r16/DxsYGADDz888Rdv06/th/AE5OTrhz5w7S0vKeQEtERMZNVoOQggYgL2rXrp0ELSEikp9x48ajd+/eWmWfTpyo+fuYsWNx8NBB7Pj111cOQvy6dMFHo0YBACZNnoxly5YiKChIp0HI7du38fvevQg+eQqtW7cGAGzevAWVK1XEnj174O/vjweRkejduw8aNGgAAKhWrZrm+ZEPItGoUWM0a9YMAFClShXdOk9EpGcKg70OlTzJahBCRFRSLM2UuDqns15yxdL0/7+4P5ednY2FCxZg+/ZtePToEdRqNdRqNaytrV9ZT8MGDTV/VygUcHFxQcyTJzq1ISwsDKamplqDHEdHR9SqVQs3wsIAAGPGfozRoz7CkSOH0aGDD3r36YOGDXMzP/zwI/j37YMLF86jY8dO6NGzp2YwQ0REbw5ZXqKXiEhsCoUCVuamkm9i3pzr5cHFkq+/xrJlSzHxs0k4euxPnL9wEZ06dy70vLmXT2hXKBTIycnRqQ0FnUb44o3IPvjgA9y5ew8D3xuEK1evoEXzZlj5zTcAAD8/P4RH3MfH48bjcdRjdPTpgM9emM0hIqI3AwchREQGKvhkMLp374H33nsP7u7uqFatGu7cvl2imXXr1kVWVhb+/vtvTVlcXBxu3bqF2nXqaMoqVqyIDz/8EDt37sKECZ/i+++/0+wrU6YMhgwZgp9+2owlS5fhu+/WlWibiYjEoFDIdzNEXI5FRGSg3Kq7YdeunQgJCYGDgwOWLlmC6OhorcGA2GrUqIHuPXpg5IjhWLP2W5QqVQpTp05B+fLl0aNHDwDAJ+PHw9fPDzVr1kRCQgKOH/9T06ZZM2eiSdOmqFevHtRqNf74Yx/qlGB7iYhInjgTQkRkoGZ8/jkaN2kCP9/OaO/tBRcXF/To2bPEc9ev34AmTZuie7euaNPaA4IgYN8f+zXLvLKzszF2zGjUq1sHXfx8UbNWLaxatRoAYG5ujunTpqKRe0N4ebaDUqnEz1t/KfE2ExGRvMjqPiElifcJIXpz5HefEDJMvE8IUeF4nxBpJKRKf68pXTlYiXcRFKlwJoSIiIiIiCTFQQgREREREUlKxpNeRERERETyYKhXoZIrDkKIiIiIDJgU52s0m32kxDMA4Oq8jpLkkP5xORYREREREUmKMyFERERERIVQgOuxxMSZECIiIiIikhQHIUREREREJCkuxyIiIiIiKgSvjiUuzoQQEREAQGmiwJ49e/TdDCIiegNwEEJEJANKE8Urt6FDhxS77mpVq2D5smWitZWIiOh1cTkWEZEMPHocpfn79m3bMGvWTITduKkps7S01EeziIjo/7gaS1ycCSGiN4MgABkp0m+CoFPzXFxcNJutnR0UCoVW2V9//YXmzZrCytICbtWrYe6cOcjKytI8f87s2ahSuRIsLVSoUN4V4z7+GADQ3tsL9+/fx4QJn2hmVXR15coV+HRoD2srS5RxcsTIESPw7Nkzzf6goCC0atkCpWysUdrBHm3faoP79+8DAC5duoQO7b1hZ1sK9na2aN6sKc6ePatzNhERGTfOhBRg29Yt2LjhB8TGxKC6Ww1MmjINTZo2M7gM5jBHqgzZ52SmwiTQVfS2FCZn6mPA3PqVx8THxyEuNhZZWVlQqVTIUKu19h86dAiDB72HZctXoG3btrh79y4+HDkCADBz1izs2LEDy5Ytxc9bf0G9evUQHR2NS5cuAQB27NyFxo3cMXz4CPTp2wcJ8fEIu34NKpUKLi7lYGWdf9tSU1PRxc8XLVu1wt//nMGTJ08wYvgHGDt2DDZs2IisrCz07tUTH3wwHFt+3oqMjAz8888/SEpKxO1b6ejn3xf16tVDUNAJ2JQqhYsXL8LMzEyEVzSXrD9rzOHPNSPN+aBdFfjULYuqZayRnpmDi5GJWHr4NiJiU7WOG9W+Gvo2qwBbS1NceZiEeb/fwN0nKQAAV3sLHJ7YNt/6J2y99HqdI4Mim5mQhw8f6rsJGgcP7MeiBYEYPuIjbNuxB02aNMWokcMR9fixQWUwhzlSZRhjjlSSkpIQHRUNpzJlUK16dVhZWSMuLlbrmMD5X2Ly5CkICAhAtWrV0LFjR8yZ+wXWrfsWAPAgMhIuLi7w8fFBpUqV0KJFCwwfPhwAULp0aSiVSpiamULIEVCnbl1Nzv3795GZkZFvu7Zs2YK0tDRs2vQj6tevj/bt22PFNyux+aef8O+//yI5ORlJSUl4u2tXVK9eHXXq1EHPnj1hqjSFU5ky+Pfff+Hl5Q0LCwtUqVwZ/v7+cHd3F+U1M7bPGnPkmcGcvJpVccDWvx/g3W//wYiN52BqosC6IU1gafbfr5Pvt62Cwa0rY/6+Gxiw5m/EPs3Ad0OawspcCQCITkqH54ITWtvKY3eRqs5C8O04UfsrOoWMNwOkEAQd1wqUMHt7e3zzzTcYNGhQidSfnlX4Mc8NHOCPOnXrYsbMOZqynt384N3eB+M++VSU9kiRwRzmSJUhtxx1ejoeRYajStWqsLCwyD1IEIDM1PyqzFd4eDgsLVRwKfff7Mndu3dQqlQplC3rrHuDzaxeeV3He/fuwtLCAuVcy2vKFi1cgMDAQCQkJgEAStlYIycnB0qlUnNMdnY20tPT8fRZCuLi4tD2rTYQBAGdO/vCr0sXdOvWDaamuZPd1apWwcD33sOojz7Syrlz+xZK2drC2dkFQO7J8Tt37UbPnj3x6YQJuHjxAo79eVxzfFJSEko72ON40Am0a9cO778/FL9s3Qqfjh3h08EHLVq2QOVKlVDOtTzmzJ6N+fO/RPPmzeHl7Y2hQ99H9erVdX/d/i89PR0R4eEoX6kqVP9/L+X0WWOOfnKMqS+GktNs9pE8ZQ5WZgie5oWA78/gXEQiAOD45Hb4KSQS64MjAABmSgVOTPHE0sO38euZR/nW/euolgiLeoqZu6/j6ryOxeucBJ6qc/TdhAKVUslmXkFnsmnx/PnzMXr0aPTp0wdxcfobCWdmZCDs+jV4tH5Lq9yjdRtcunjBYDKYwxypMgwmR6HIXRalwyaYWiItC7BycNYqt7Yvg9RM6FwPzK1fOQARcnKQnpYGa5tSWuUqlQovfj+Uk5OD2bPn4PyFi5rt0uUruHnrNiwsLFCxYkWE3biJb1augqWlJcaMHgUvz3bIzMzU1JGVmZknx8bGBmmp+Q/MBEGAooC2Py9fv34DToWcRmuP1ti2bRs6+vjgetgNAMCs2bNx5eo1dOrUCcF//YX69epi9+7dBb8/OjKIzxpz+HPtDcmxscj9oiMpNfdnTQUHS5QppULInf9+j8vMFnA2IgGNKtnnW0dd11Ko42qLXWfzH6CQ8ZLNIGTUqFG4dOkSEhISUK9ePezdu1cv7UhITEB2djYcHR21yh0dnRAbG2MwGcxhjlQZxpiTlZ0NADA1VWqVm5qaap0MXlI5L854AECTJk1w8+ZNuLm55dlMTHJ/jFtaWqJ79+5YvmIF/jwehNOnT+PKlSsAADMzM+Tk5BSpP3Xq1sXFixeRkpKiKTt16hRMTExQs2ZNTVnjxo0xZepUBJ04ATc3N+zauUOzr2bNmhg1ahR++OEH9OrdGxs3bijqS5SHsX3WmCPPDOboZpJfLZyLSMCd/5/v4WRjDgCIe6a9zDPuWYZm38t6Ny2Pu0+e4eKDpGK1QUoKGf8xRLI6Mb1q1ar4888/sXLlSvTp0wd16tTRLCd47vz584XWo1aroX7pxE5BqYJKpdK5LS9/A/iqbwWLS4oM5jBHqgxjzHl5oW3JrV19dc6Mz2eie7euqFixIvr6+8PExASXL1/G1StX8MW8edi4cSOys7PRsmVLWFlZYfNPP8HS0hKVK1cGAFSuXAVnz57F48dRsLe3h5OTU6H9GThwIObMnoUhQwIwa9ZsxMTEYNzHY/HeoEFwdnZGeHg4vlu3Dt26d4erqyuuXbuG+/fvY+jQ95GWloZJn32GPn37wta2FG7fuoWzZ86gd+8+4r1iRvZZY448M5hTsOlda6Omiw0Gf3cmz76XV/orFPlfKFBlaoIuDV3wbVB4kfPJ8MlqEAIA9+/fx86dO1G6dGn06NEjzyBEF4GBgZgzZ45W2fTPZ2HGzNmFPtfB3gFKpRKxsdonhsbHx8HR0anIbdFXBnOYI1WGMeaY/n8m4uVZguysrGL9TCpqTs7/Z0ie69y5M/b+vg/zvpiLr75aBDMzM9SuXRvvD/sAQO45dYsWLsDETycgOzsbDRo0wG97f9d84zlnzhy8//5QNGxQH2q1Gtk5QqH9sbKywoGDh/DJ+HFo2aI5rKys0Lt3H3y9ZIlm/42bN/Bj302Ii4tDuXLl8M4772BwQACUSiXi4uMwJGAw/v33Xzg4OKBvX3/MfunncnEY22eNOfLMYM6rTX27FrzrlEHA92fwb/J/X/rG/n8GxKmUSvN3AChtbY64lLwXwehU3xmWZkrsvWCYFxah1yOb5VgA8N1336FBgwZwcHDA1atX8cUXX2DWrFlamy6mTp2KpKQkre2zyVN1eq6ZuTnq1K2H0JBTWuWhISFwb9S4yH3SVwZzmCNVhjHmKExMYGFpiZQX7okBAM+epcDSyqrEc95+uytu3rqlVda5c2cEnzyFZympSEhMwunQvzVXwOrZsydCTociITEJyU+f4VTIaXTo0EHzXI/WrfHH/v24e/eeZgCSX3+ycwT07NlT87hBgwY4euxPpKSmISY2Dt+uWwcbGxsAgLOzM3bt2o2Hjx4jLV2Ne+ER+HTiRKSlpsLc3Bw//7wVEfcjceXKVVy4eBErvvnmv4sEvAZj+6wxR54ZzCnYtK614FOvLN5ffw6PEtK19j1MSEPMUzU8qpfWlJkqFWhWxQEXIxPz1NW7qSuO34hBQmpmnn1ypFDIdzNEspkJ8fX1xT///IOVK1di8ODBr1WXSpV36VVRro41KGAopk+ZhLr168PdvTF2/roNUVFR8O8/4LXaJXUGc5gjVYYx5jg6OuHRw4ewsLSAlZUVEuITkJmZidIOpQt/8huaI1VfjO2zxhx5ZjAnrxndaqNLQxd8vOUSUtRZcPz/eR7P0rOgzsq9ctRPIZEY7lkVkXGpuB+XiuGeVZGemYM/LkVr1VWxtCWaVnbARz+Jd/I9GRbZDEKys7Nx+fJlVKhQQd9Nga9fFyQlJmDdmtWIiXkCtxo1sWrtOri+cGlLQ8hgDnOkyjDGHDs7O2RnZyE2JkZzE8HKlSvDzDz/kyuZI11fjO2zxhx5ZjAnrwEtKwIANn6gfXPD6Tuv4rcLUQCA9cERsDAzwYzudWBrYYrLD5MxYuM5pGZoLzPt3bQ8njxVa11Ji94ssrlPSEkrykwIERm2fO8TQgYpv/uEEJH08rtPSEmQ831CUjPk+yuzlbnhrcmS1TkhRERERERk/DgIISLj8/8vhN6QiV6jpnkPDe9LPiIiegXZnBNCRCQWU1MzAEBqaiosLS313Bp6Han/v6P78/eUiEhv+GWIqDgIISKjo1QqUcrOHjFPngDIvadFydzMkEqKIAhITU1FzJMnKGVnn+cu8kREZNg4CCEio+RU1gUANAMRMkyl7Ow17yURERkPDkKIyCgpFAqUcS6H0k5lkZWVCfD0EMOiyF2CxRkQIpILBddjiYqDECIyakqlkr/IEhERyQyvjkVERERE9AZZvXo1qv7/XlpNmzZFcHCw5G3gIISIiIiIqBAKhXy3oti2bRvGjx+P6dOn48KFC2jbti38/PwQGRlZMi9cATgIISIiIiJ6QyxZsgTDhg3DBx98gDp16mDZsmWoWLEi1qxZI2k7OAghIiIiIjJgarUaycnJWptarc5zXEZGBs6dO4dOnTpplXfq1AkhISFSNTeXQPlKT08XZs2aJaSnpzPnDc0xpr4wR945xtQX5sg7x5j6whx550jVF8o1a9YsAbnXgdRss2bNynPco0ePBADCqVOntMq//PJLoWbNmhK1NpdCEAReuDIfycnJsLOzQ1JSEmxtbZnzBuYYU1+YI+8cY+oLc+SdY0x9YY68c6TqC+VSq9V5Zj5UKhVUKpVW2ePHj1G+fHmEhITAw8NDU/7ll1/ip59+wo0bNyRpL8BL9BIRERERGbT8Bhz5cXJyglKpRHR0tFb5kydP4OzsXFLNyxfPCSEiIiIiegOYm5ujadOmOHLkiFb5kSNH0Lp1a0nbwpkQIiIiIqI3xIQJEzBo0CA0a9YMHh4eWLduHSIjI/Hhhx9K2g4OQgqgUqkwa9Ysnaa2mGOcOcbUF+bIO8eY+sIceecYU1+YI+8cqfpCRde/f3/ExcVh7ty5iIqKQv369bF//35UrlxZ0nbwxHQiIiIiIpIUzwkhIiIiIiJJcRBCRERERESS4iCEiIiIiIgkxUEIERERERFJioOQAoSEhECpVMLX17dE6h8yZAgUCoVmc3R0hK+vLy5fvix6VnR0NMaOHYtq1apBpVKhYsWK6NatG44dOyZK/S/2xczMDM7OzujYsSPWr1+PnJwcUTLyy3pxE/t9Kijnzp07ouZER0dj3LhxcHNzg4WFBZydnfHWW29h7dq1SE1Nfe36hwwZgp49e+YpDwoKgkKhQGJi4mtn6JpZkvXv2LEDFhYWWLRoUYnmiOn5Zyy/SyKOGjUKCoUCQ4YMES1nwYIFWuV79uyBQqF47fpf9ODBAwwbNgyurq4wNzdH5cqVMW7cOMTFxYma8/LPnGrVqmHixIlISUkRNUcKL/bF1NQUlSpVwkcffYSEhATRs548eYKRI0eiUqVKUKlUcHFxQefOnXH69GlR6s/vZ+aLmxifZwDw8vLC+PHj85SL+Znu1q0bfHx88t13+vRpKBQKnD9/vtj1r127FqVKlUJWVpam7NmzZzAzM0Pbtm21jg0ODoZCocCtW7eKnZednY3WrVujT58+WuVJSUmoWLEiZsyYUey6XyQIAnx8fNC5c+c8+1avXg07OztERkaKkkWGj4OQAqxfvx5jx47FyZMnS+wfjK+vL6KiohAVFYVjx47B1NQUXbt2FTUjIiICTZs2xZ9//olFixbhypUrOHjwILy9vTF69GjRcp73JSIiAgcOHIC3tzfGjRuHrl27av2QFTPrxW3r1q2iZhSUU7VqVdHqv3fvHho3bozDhw9j/vz5uHDhAo4ePYpPPvkEv//+O44ePSpaljH7/vvvMXDgQKxcuRKTJk3Sd3OKpGLFivjll1+QlpamKUtPT8fWrVtRqVIl0XIsLCywcOHCEvnF9rl79+6hWbNmuHXrFrZu3Yo7d+5g7dq1OHbsGDw8PBAfHy9q3vN/n/fu3cO8efOwevVqTJw4UdQMqQZVL/78/P777/H7779j1KhRomYAQJ8+fXDp0iVs2rQJt27dwt69e+Hl5SXae/Piz8ply5bB1tZWq2z58uWi5Ehh2LBh+PPPP3H//v08+9avX49GjRqhSZMmxa7f29sbz549w9mzZzVlwcHBcHFxwZkzZ7S+hAoKCoKrqytq1qxZ7DylUolNmzbh4MGD2LJli6Z87NixKF26NGbOnFnsul+kUCiwYcMG/P333/j222815eHh4Zg8eTKWL18u6s82Mmy8T0g+UlJSsH37dpw5cwbR0dHYuHGjaP9AX/T8mygAcHFxweTJk9GuXTvExMSgTJkyomQ8/0b1n3/+gbW1taa8Xr16eP/990XJALT7Ur58eTRp0gStWrVChw4dsHHjRnzwwQclklWSSjpn1KhRMDU1xdmzZ7XemwYNGqBPnz7g1bMLt2jRIsycORM///xznm/4DEGTJk1w79497Nq1CwMHDgQA7Nq1CxUrVkS1atVEy/Hx8cGdO3cQGBgo6mzRi0aPHg1zc3McPnwYlpaWAIBKlSqhcePGqF69OqZPn441a9aIlvfiv893330Xx48fx549e0TLuHfvHjw8PFCzZk1s3boVVatWxbVr1/DZZ5/hwIEDCA0NRenSpUXJerEvFSpUQP/+/bFx40ZR6n4uMTERJ0+eRFBQEDw9PQEAlStXRosWLUTLePHnpZ2dHRQKhSQ/q0tC165dUbZsWWzcuBGzZs3SlKempmLbtm2YP3/+a9Vfq1YtuLq6IigoCK1atQKQO9jo0aMHjh8/jpCQEM1MTFBQELy9vV8rDwBq1KiBwMBAjB07Ft7e3jhz5gx++eUX/PPPPzA3N3/t+p+rWLEili9fjjFjxqBTp06oUqUKhg0bhg4dOog2G0bGgTMh+di2bRtq1aqFWrVq4b333sOGDRtK/BfCZ8+eYcuWLXBzc4Ojo6ModcbHx+PgwYMYPXq01i+5z9nb24uSU5D27dvD3d0du3btKtEcQxQXF4fDhw8X+N4AEH2pjLGZMmUKvvjiC+zbt88gByDPDR06FBs2bNA8Xr9+vahfEAC534LOnz8f33zzDR4+fChq3UDuz5pDhw5h1KhRmgHIcy4uLhg4cCC2bdtWoj9HLS0tkZmZKVp9Lw6qPD09UalSJfj5+eHo0aN49OgRpk+fLlrWi+7du4eDBw/CzMxM1HptbGxgY2ODPXv2QK1Wi1q3MTI1NcXgwYOxceNGrc/tr7/+ioyMDM2XBq/Dy8sLx48f1zw+fvw4vLy84OnpqSnPyMjA6dOnRRmEALkzH+7u7hg8eDBGjBiBmTNnolGjRqLU/aKAgAB06NABQ4cOxcqVK3H16lWsW7dO9BwybByE5OOHH37Ae++9ByB3mvzZs2einT/xon379mn+YyhVqhT27t2Lbdu2wcREnLflzp07EAQBtWvXFqW+4qhduzYiIiJErfPF1+359sUXX4iakV+Ov7+/aHU/f29q1aqlVe7k5KTJmzx5sihZ+b1efn5+otStLwcOHMDChQvx22+/Fbhu21AMGjQIJ0+eREREBO7fv49Tp05pfv6IqVevXmjUqJHWt7piuX37NgRBQJ06dfLdX6dOHSQkJCAmJkb0bAD4559/8PPPP6NDhw6i1Cf1oOr5v1FLS0tUr14d169fF+3f/3OmpqbYuHEjNm3aBHt7e7Rp0wbTpk0rkfMQjcX777+PiIgIBAUFacrWr1+P3r17w8HB4bXr9/LywqlTp5CVlYWnT5/iwoULaNeuHTw9PTWZoaGhSMtkdwsAAA1wSURBVEtLE20QolAosGbNGhw7dgzOzs6YMmWKKPXmZ926dbh+/TrGjx+Pb7/9FmXLli2xLDJMHIS85ObNm/jnn38wYMAAALk/uPv374/169eLnuXt7Y2LFy/i4sWL+Pvvv9GpUyf4+fnluwa1OJ7/B6nPb9QFQRA9/8XX7fkm5vktBeWsWLFC9IyXX5t//vkHFy9eRL169UT7tjK/1+v7778XpW59adiwIapUqYKZM2fi6dOn+m7Oa3FycsLbb7+NTZs2YcOGDXj77bfh5ORUIlkLFy7Epk2bcP369RKpvyAl8bPo+S/uFhYW8PDwQLt27fDNN9+IUrfUg6rn/0b//vtvjB07Fp07d8bYsWNFqftFffr0wePHj7F371507twZQUFBaNKkiehLv4xF7dq10bp1a83//3fv3kVwcLBoM5Xe3t5ISUnBmTNnEBwcjJo1a6Js2bLw9PTEmTNnkJKSgqCgIFSqVEnU5Znr16+HlZUVwsPDS2Rm9LmyZctixIgRqFOnDnr16lViOWS4OAh5yQ8//ICsrCyUL18epqamMDU1xZo1a7Br1y7RT+q0traGm5sb3Nzc0KJFC/zwww9ISUnBd999J0r9NWrUgEKhQFhYmCj1FUdYWJioJ3MD2q/b802stdmvyilXrpxodbu5uUGhUODGjRta5dWqVYObm1ueb19fR36vV/ny5UWrXx/Kly+PEydOICoqCr6+vgY/EHn//fc131KLvRTrRe3atUPnzp0xbdo0Uet9/nkuaHBz48YNODg4iDq4ev6L+82bN5Geno5du3ZJ9k3r80GVWOvon/8bbdiwIVasWAG1Wo05c+aIUvfLLCws0LFjR8ycORMhISEYMmRIicyOlSRbW1skJSXlKU9MTIStra2oWcOGDcPOnTuRnJyMDRs2oHLlyqLNuLm5uaFChQo4fvw4jh8/rjlXx8XFBVWrVsWpU6dw/PhxtG/fXpQ8IPfKXkuXLsVvv/0GDw8PDBs2rESXST7/PYooPxyEvCArKws//vgjvv76a61vjS9duoTKlStrXVGiJCgUCpiYmGhdKed1lC5dGp07d8aqVavyvXRlSVye9UV//vknrly5YtDr9UuKo6MjOnbsiJUrVxrkZUXloFKlSjhx4gSePHmCTp06ITk5Wd9NKjZfX19kZGQgIyMj30tbimnBggX4/fffERISIlqdzz/Pq1evzvPzKzo6Glu2bEH//v1FnQl5/ot75cqVRT9/QpdBVZkyZUrsvLpZs2Zh8eLFePz4cYnU/6K6desa3M+g2rVra11V6rkzZ87kWeL6uvr16welUomff/4ZmzZtwtChQ0X9HHt7eyMoKAhBQUHw8vLSlHt6euLQoUMIDQ0VbSlWWloaAgICMHLkSPj4+OD777/HmTNntK5iRSQlDkJesG/fPiQkJGDYsGGoX7++1ta3b1/88MMPouap1WpER0cjOjoaYWFhGDt2LJ49e4Zu3bqJlrF69WpkZ2ejRYsW2LlzJ27fvo2wsDCsWLECHh4eouU878ujR49w/vx5zJ8/Hz169EDXrl0xePBg0XJezHpxi42NFTVDCqtXr0ZWVhaaNWuGbdu2ISwsDDdv3sTmzZtx48YNKJVKfTdR9ipUqICgoCDExcWhU6dO+X47+jqSkpLyLGUriUt2K5VKhIWFISwsrMTf9wYNGmDgwIGiLV16buXKlVCr1ejcuTP++usvPHjwAAcPHkTHjh1Rvnx5fPnll6LmlSRdBlUleZUfLy8v1KtX77WvwPSiuLg4tG/fHps3b8bly5cRHh6OX3/9FYsWLUKPHj1Ey5HCqFGjcPfuXYwePRqXLl3CrVu3sGrVKvzwww/47LPPRM2ysbFB//79MW3aNDx+/Fj0993b2xsnT57ExYsXNTMhQO4g5LvvvkN6erpog5ApU6YgJycHCxcuBJD7Rc7XX3+Nzz77TPRzN4l0IpBG165dhS5duuS779y5cwIA4dy5c6JkBQQECAA0W6lSpYTmzZsLO3bsEKX+Fz1+/FgYPXq0ULlyZcHc3FwoX7680L17d+H48eOi1P9iX0xNTYUyZcoIPj4+wvr164Xs7GxRMvLLenGrVauW6Dk9evQQtc78PH78WBgzZoxQtWpVwczMTLCxsRFatGghfPXVV0JKSspr119QP44fPy4AEBISEl47Q9fMkqz/8ePHQq1atYTmzZuL1qeCPmsBAQGi1f+q16lHjx6iZOWXExERIahUKkHs/wIiIiKEIUOGCC4uLoKZmZlQsWJFYezYsUJsbKyoOVL8+7x165bg5OQktG3bVjhx4oQQGRkpHDhwQKhfv77QqFEj4enTp6LkFNSXLVu2CObm5kJkZKQoOenp6cKUKVOEJk2aCHZ2doKVlZVQq1YtYcaMGUJqaqooGS/asGGDYGdnJ3q9z509e1bo3LmzULZsWcHW1lZo1qyZsHXr1hLJCgkJEQAInTp1Er3u8PBwAYBQu3ZtrfIHDx4IAITq1auLkhMUFCQolUohODg4z75OnToJ7du3F3JyckTJetGsWbMEd3d30esl46AQBN6MgIiI6GURERGYPXs2Dh48iCdPnkAQBPTu3Rs//fQTrKys9N08IiKDxkEIERGRDmbNmoUlS5bg8OHDoi5nJSJ6E3EQQkREpKMNGzYgKSkJH3/8sWj3dCIiehNxEEJERERERJLi1zhERERERCQpDkKIiIiIiEhSHIQQEREREZGkOAghIiIiIiJJcRBCRERERESS4iCEiIhQpUoVVKlSRd/NICKiNwQHIUREIomIiIBCoXjl1qhRI303k4iISO9M9d0AIiJjU716dbz33nv57nNxcZG4NURERPLDQQgRkcjc3Nwwe/ZsfTeDiIhItrgci4hITxQKBby8vPDgwQP0798fjo6OsLa2hpeXF0JCQvJ9TlxcHD755BNUrVoVKpUKZcuWRf/+/XH9+vV8j8/IyMDy5cvRokULlCpVCjY2Nqhbty4mTJiAhISEPMenpKRgwoQJKF++PFQqFRo2bIgdO3aI2m8iIiKFIAiCvhtBRGQMIiIiULVqVXTu3BkHDx4s9HiFQoGGDRsiISEB5cqVQ/v27fHo0SNs27YNAHDo0CF4eXlpjo+Li0OrVq1w584deHl5oVWrVoiIiMCOHTugUqlw5MgReHh4aI5PT09H586d8ddff6FGjRrw9fWFSqXC7du3cfjwYYSEhGjOUalSpQoyMzNRpUoVxMfHw8fHB6mpqfjll1+QlpaGgwcPolOnTqK+XkRE9ObiIISISCTPByGvOiekVatW8PX1BZA7CAGAQYMGYdOmTZrHJ06cgLe3N6pXr46bN2/CxCR30nrYsGFYv349pk6divnz52vqPHToEHx9fVGjRg3cuHFDc/ykSZPw1VdfYdCgQdiwYQOUSqXmOUlJSVAqlbCxsQGQOwi5f/8+evToge3bt8Pc3BwAcOzYMfj4+Og8sCIiItIFByFERCJ5Pgh5lXHjxmHZsmUAcgchSqUS4eHhqFixotZxXbt2xR9//IHg4GC89dZbyMjIgL29PaysrBAZGQkrKyut4319fXHo0CHN8dnZ2ShdujQUCgXCw8Ph4ODwynY9H4Tcu3cvTx+qVKmCp0+fIi4uTsdXgoiI6NV4TggRkcg6d+4MQRDy3Z4PQJ6rXLlyngEIALRt2xYAcPHiRQDAjRs3kJaWhhYtWuQZgADQLNt68fjk5GQ0b9680AHIc/b29vkOoipUqIDExESd6iAiItIFByFERHpUtmzZfMudnZ0B5C6bAoDk5GSt8pc9v/Tv8+OfDxrKly+vc1vs7OzyLTc1NUVOTo7O9RARERWGgxAiIj168uRJvuX//vsvgP8GBra2tlrlBR3//Dh7e3sAwKNHj0RrKxERkVg4CCEi0qP79+/jwYMHecqDg4MBQHP1qtq1a8PCwgJnzpxBampqnuNPnDihdXytWrVga2uLM2fO5HspXiIiIn3iIISISI+ys7Mxffp0vHiNkBMnTmD//v1wc3ND69atAQDm5uZ45513EBsbi8DAQK06jh49igMHDsDNzQ1t2rQBkLuEauTIkUhKSsK4ceOQnZ2t9ZykpCQ8e/ashHtHRESUP14di4hIJLpcoheA5m7q+d0n5PHjx/jll18A5L1PSExMDFq1aoV79+6hffv2aNmypeY+IWZmZjh06BDeeustzfHp6eno1KkTgoODUaNGDfj5+UGlUuHevXs4ePAgTp48qXWfkOd9eJmXlxdOnDgB/ndBRERi4SCEiEgkulyiF4Dml3mFQgFPT0/8+OOPmDhxIo4ePYr09HQ0b94c8+fP18xqvCg2NhZffPEFfvvtNzx+/Bh2dnbw8vLCrFmzUL9+/TzHq9VqrFy5Eps3b8bNmzehVCpRqVIl+Pn5YcaMGZpzRzgIISIiKXEQQkSkJ88HIUFBQfpuChERkaR4TggREREREUmKgxAiIiIiIpIUByFERERERCQpU303gIjoTcVT8oiI6E3FmRAiIiIiIpIUByFERERERCQpDkKIiIiIiEhSHIQQEREREZGkOAghIiIiIiJJcRBCRERERESS4iCEiIiIiIgkxUEIERERERFJ6n+fz0ABNjVmPwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn_clf.avg_trn_loss_tracker, label='Train loss')\n",
    "plt.plot(nn_clf.avg_vld_loss_tracker, label='Test loss')\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.ylabel(\"Negative Log Likelihood Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e9c0c6ac",
   "metadata": {
    "id": "e9c0c6ac",
    "outputId": "45fd41f9-74fa-4a21-cf84-dff6916e4595"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test NLL loss: 2.5778864423290395\n"
     ]
    }
   ],
   "source": [
    "tst_nll = mean_nll(y_tst, nn_clf.predict_proba(X_tst))\n",
    "print(f\"Test NLL loss: {tst_nll}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5352bd3a",
   "metadata": {
    "id": "5352bd3a",
    "outputId": "73093130-75e5-4e1e-96be-45cb1fbbf015"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6],\n",
       "       [ 2],\n",
       "       [10],\n",
       "       ...,\n",
       "       [ 2],\n",
       "       [ 4],\n",
       "       [ 2]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_tst= nn_clf.predict(X_tst)\n",
    "class_labels[y_hat_tst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f71cd201",
   "metadata": {
    "id": "f71cd201",
    "outputId": "47b71840-34ed-483a-df0a-f58827f4d267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ratio: 4232/7172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.5900725041829337)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y=y_tst, y_hat=y_hat_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "bdbccd01",
   "metadata": {
    "id": "bdbccd01",
    "outputId": "3a7dcd88-fe44-4988-c01b-c59e9c8ecce5"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    y_hat=class_labels[y_hat_tst],\n",
    "    y=class_labels[np.argmax(y_tst, axis=1)],\n",
    "    class_names=class_names,\n",
    "    figsize=(10,10)\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "216px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
